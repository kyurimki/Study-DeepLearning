{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. 오차역전파법\n",
    "## 5.4 단순한 계층 구현하기\n",
    "- 사과 쇼핑의 예를 파이썬으로 구현하려고 한다.\n",
    "- 계산 그래프의 곱셈 노드 = MulLayer, 덧셈 노드 = AddLayer\n",
    "\n",
    "### 5.4.1 곱셈 계층\n",
    "- 모든 계층은 forward()와 backward() 공통의 메서드(인터페이스)를 가짐\n",
    "    - forward()는 순전파, backward()는 역전파를 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 계층 구현\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):  # x와 y를 바꾼다.\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__init__()`: 인스턴스 변수인 x와 y(순전파일 때 입력 값을 유지하기 위해서 사용) 초기화\n",
    "- `forward()`: x와 y를 인수로 받아 두 값을 곱해 반환\n",
    "- `backward()`: 상류에서 넘어온 미분(`dout`)에 순전파 때의 값을 서로 바꿔 곱한 후 하류로 흘림\n",
    "- 사과 2개 구입하는 예제는 다음과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 각 변수에 대한 미분-역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `backward()`의 호출 순서는 `forward()` 때와 반대\n",
    "- `backward()`가 받는 인수 = 순전파의 출력에 대한 미분\n",
    "    - ex. `mul_apple_layer`라는 곱셈 계층\n",
    "        - 순전파 때는 `apple_price` 출력\n",
    "        - 역전파 때는 `apple_priece`의 미분 값인 `dapple_price`를 인수로 받음\n",
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화가 필요 없어 `__init__()`에서 `pass` 수행\n",
    "- `forward()`: 입력받은 두 인수 x, y를 더해서 반환\n",
    "- `backward()`: 상류에서 내려온 미분(`dout`)을 그대로 하류로 흘림\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115655310-01641b80-a36e-11eb-9002-8ed307fdb48b.png)\n",
    "\n",
    "- 덧셈 계층과 곱셈 계층으로 사과 2개와 귤 3개를 사는 경우를 나타내면 위와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기\n",
    "- 계산 그래프를 신경망에 적용하기\n",
    "- 신경망을 구성하는 게층 각각을 클래스 하나로 구현\n",
    "### 5.5.1 ReLU 계층\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115656834-d929ec00-a370-11eb-89cf-47189656b027.png)\n",
    "\n",
    "- ReLU의 수식과 x에 대한 y의 미분은 다음과 같다.\n",
    "- 순전파 때 x가 0 이상이면, 역전파는 상류의 값을 그대로 하류로 흘리고, x가 0 이하면 역전파 때 하류로 신호를 보내지 않는다(= 0을 보낸다).\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115657122-548b9d80-a371-11eb-9b1f-fd1d2b71f722.png)\n",
    "\n",
    "- ReLU 계층은 계산 그래프로 다음과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `mask`\n",
    "    - True/False로 구성된 넘파이 배열\n",
    "    - 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외에는 False로 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순전파 때 입력 값이 0 이하면 역전파 때의 값은 0이 되어야 한다.\n",
    "- -> 역전파 때는 순전파 때 만든 mask를 써서 mask의 원소가 True인 곳에는 상류에서 전파된 dout = 0으로 설정\n",
    "- ReLU 계층은 전기 회로의 스위치에 비유할 수 있다. 순전파 때 전류가 흐르고 있으면 스위치를 ON으로 하고, 흐르지 않으면 OFF로 한다. 역전파 때 스위치가 ON이면 전류가 그대로 흐르고, OFF면 흐르지 않는다.\n",
    "\n",
    "### 5.5.2 Sigmoid 계층\n",
    "- 시그모이드 함수: *y = 1 / (1 + exp(-x))*\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115658859-9407b900-a374-11eb-8d73-c2ff546514f5.png)\n",
    "\n",
    "- [×]와 [+] 노드 외에 [exp]와 [/] 노드가 새롭게 등장한다.\n",
    "- 시그모이드 식의 계산은 국소적 계산의 전파로 이루어진다.\n",
    "#### 1단계\n",
    "- [/] 노드(*y = 1/x*)을 미분하면 다음 식이 된다.: *∂y/∂x = -(1/x^2) = -y^2*\n",
    "- 역전파 때는 상류에서 흘러온 값에 *-y^2*을 곱해서 하류로 전달한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115661125-f7471a80-a377-11eb-83df-dbb287db9b96.png)\n",
    "\n",
    "#### 2단계\n",
    "- [+] 노드는 상류의 값을 하류로 내보낸다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115661699-d3380900-a378-11eb-9c5a-7af29bccad37.png)\n",
    "\n",
    "#### 3단계\n",
    "- [exp] 노드는 *y = exp(x)* 연산을 수행하고, 미분은 다음과 같다.: *∂y/∂x = exp(x)*\n",
    "- -> 계산 그래프에서 상류의 값에 순전파 때의 출력(= *exp(-x)*)을 곱해 하류로 전파한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115668754-74778d00-a382-11eb-874c-089ebf46a11b.png)\n",
    "\n",
    "#### 4단계\n",
    "- [×] 노드는 순전파 때의 값(= -1)을 서로 바꿔 곱한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115668997-c02a3680-a382-11eb-9313-e043a255b2fb.png)\n",
    "\n",
    "- 역전파의 최종 출력인 *(∂L/∂y)y^2exp(-x)* 값이 하류 노드로 전파된다.\n",
    "- 계산 그래프의 중간 과정을 묶어 하나의 [sigmoid] 노드 하나로 대체할 수 있다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115671127-24e69080-a385-11eb-945d-e4b8129a0a9c.png)\n",
    "\n",
    "- 간소화 버전은 역전파 과정의 중간 계산들을 생략할 수 있어 더 효율적인 계산이라고 할 수 있다.\n",
    "- 노드를 그룹화하여 Sigmoid 계층의 세세한 내용을 노출하지 않고 입출력에만 집중할 수 있다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115670143-0af87e00-a384-11eb-8697-020d6e4bfc48.png)\n",
    "\n",
    "- *(∂L/∂y)y^2exp(-x)*는 위와 같이 나타낼 수 있다.\n",
    "- => Sigmoid 계층의 역전파는 순전파의 출력 *y*만으로 계산할 수 있다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115670591-8b1ee380-a384-11eb-86f3-6e367b3ebb3e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기\n",
    "### 5.6.1 Affine 계층\n",
    "- 신경망의 순전파에서 가중치 신호의 총합을 계산하기 위해 행렬의 곱(`np.dot()`) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[0.98584264 0.98786044 1.20507235]\n"
     ]
    }
   ],
   "source": [
    "# Affine 계층 예제\n",
    "X = np.random.rand(2)  # 입력\n",
    "W = np.random.rand(2, 3)  # 가중치\n",
    "B = np.random.rand(3)  # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B  # 뉴런의 가중치 합\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **어파인 변환 affine transformation**: 신경망의 순전파 때 수행하는 행렬의 곱\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115672771-d89c5000-a386-11eb-9f00-19075b18ef1f.png)\n",
    "\n",
    "- 지금까지 계산 그래프는 노드 사이에 스칼라값이 흘렀지만, 이 예에서는 행렬이 흐른다.\n",
    "- 행렬의 역전파를 전개하면 다음의 식을 얻을 수 있음\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115679291-624f1c00-a38d-11eb-8f1b-7d892b69b370.png)\n",
    "\n",
    "- W^T: 전치행렬\n",
    "    - W의 (i, j) 위치의 원소를 (j, i) 위치로 바꾼 것\n",
    "    - if) W의 형상이 (2, 3)이었다면, 전치행렬 W^T의 형상은 (3, 2)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115679488-99bdc880-a38d-11eb-99cd-51085849a793.png)\n",
    "\n",
    "- 행렬을 이용한 계산 그래프의 역전파는 다음과 같다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115680048-2c5e6780-a38e-11eb-910e-c651b30f1534.png)\n",
    "\n",
    "\n",
    "- *X*와 *∂L/∂X*, *W*와 *∂L/∂W*은 같은 형상이다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115680877-f077d200-a38e-11eb-9eff-3955f2a176fc.png)\n",
    "\n",
    "- 행렬의 곱에서 대응하는 차원의 원소 수를 일치시켜야 하기 때문에 행렬의 형상에 주의해야 한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115681116-2e74f600-a38f-11eb-9a44-1146a80fcf45.png)\n",
    "\n",
    "### 5.6.2 배치(batch)용 Affine 계층\n",
    "- 데이터 N개를 묶어 순전파 하는 경우 계산 그래프는 다음과 같다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115681435-7d229000-a38f-11eb-9111-49327d2bbfcc.png)\n",
    "\n",
    "- X의 형상이 (N, 2)가 된 것이 기존과 다른 부분이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(X_dot_W)\n",
    "\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순전파의 편향 덧셈은 각각의 데이터에 더해진다. -> 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터가 2개(N = 2)라고 가정한다. 편형의 역전파는 그 두 데이터에 대한 미분을 데이터마다 더해서 구한다.(∵ `np.sum()`에서 `axis=0`의 총합을 구함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115691052-a4ca2600-a398-11eb-9433-cb0436b82a9d.png)\n",
    "\n",
    "- 입력 이미지가 Affine 계층과 ReLU 계층을 통과하며 변환되고, 마지막 Softmax 계층에 의해서 10개의 입력이 정규화(출력의 합이 1이 되도록 변형)된다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115692299-d1cb0880-a399-11eb-96cb-62d4efc24923.png)\n",
    "\n",
    "- Softmax-with-Loss 계층의 계산 그래프는 위와 같다. 이를 간소화하면 다음과 같다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/61455647/115693057-82390c80-a39a-11eb-98f3-782baaf1e763.png)\n",
    "\n",
    "- 3클래스 분류를 가정하고 이전 계층에서 3개의 입력(점수)을 받는다.\n",
    "- Softmax 계층: 입력 (a1, a2, a3) -(정규화)-> 출력 (y1, y2, y3)\n",
    "- Cross Entropy Error 계층: 입력-Softmax의 출력 (y1, y2, y3) + 정답 레이블 (t1, t2, t3) -> 출력 손실 L\n",
    "- **역전파의 결과**: Softmax 계층의 역전파 결과 = (y1-t1, y2-t2, y3-t3) = **Softmax 계층의 출력 - 정답 레이블의 오차**\n",
    "    - ex. 정답 레이블이 (0, 1, 0)일 때 Softmax 계층이 (0.3, 0.2, 0.5) 출력 -> 역전파 (0.3, -0.8, 0.5)의 오차 전파\n",
    "    - ex. 정답 레이블이 (0, 1, 0)일 때 Softmax 계층이 (0.01, 0.99, 0) 출력 -> 역전파 (0.01, -0.01, 0)의 오차 전파 -> 작은 오차 -> 학습하는 정도도 작음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블(원-핫 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기\n",
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "- 신경망 학습의 순서\n",
    "    1. 전제: 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n",
    "    2. 1단계-미니배치: 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 학습의 목표이다.\n",
    "    3. 2단계-기울기 산출: 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "    4. 3단계-매개변수 갱신: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "    5. 4단계-반복: 1~3단계를 반복한다.\n",
    "- 오차역전파법인 등장하는 단계는 '기울기 산출'\n",
    "    - 4장에서 기울기를 구하기 위해서 수치 미분 사용\n",
    "    - 수치 미분은 구현하기 쉽지만 계산이 오래 걸리기 때문에 오차역전파법을 이용해 기울기를 효율적이고 빠르게 구할 수 있다.\n",
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "- 2층 신경망을 TwoLayerNet 클래스로 구현하고자 한다.\n",
    "    - Instance variables\n",
    "        - `params`\n",
    "            - 딕셔너리 변수로, 신경망의 매개변수 보관\n",
    "            - `params['W1']`: 1번째 층의 가중치, `params['b1']`: 1번째 층의 편향\n",
    "            - `params['W2']`: 2번째 층의 가중치, `params['b2']`: 2번째 층의 편향\n",
    "        - `layers`\n",
    "            - 순서가 있는 딕셔너리 변수로, 신경망의 계층 보관\n",
    "            - `layers['Affine1']`, `layers['Relu1']`, `layers['Affine2']`와 같이 각 계층을 순서대로 유지\n",
    "        - `lastLayer`\n",
    "            - 신경망의 마지막 계층\n",
    "            - 이 예에서는 SoftmaxWithLoss 계층\n",
    "    - methods\n",
    "        - `__init__(self, input_size, hidden_size, output_size, weight_init_std)`\n",
    "            - 초기화 수행\n",
    "            - 인수: 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일\n",
    "        - `predict(self, x)`\n",
    "            - 예측(추론) 수행\n",
    "            - 인수 `x`: 이미지 데이터\n",
    "        - `loss(self, x, t)`\n",
    "            - 손실 함수의 값\n",
    "            - 인수 `x`: 이미지 데이터, `t`: 정답 레이블\n",
    "        - `accuracy(self, x, t)`: 정확도\n",
    "        - `numerical_gradient(self, x, t)`: 가중치 매개변수의 기울기를 수치 미분 방식으로 구한다.\n",
    "        - `gradient(self, x, t)`: 가중치 매개변수의 기울기를 오차역전파법으로 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self. lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망의 계층을 `OrderedDict`에 보관한다.\n",
    "    - 순서가 있는 딕셔너리\n",
    "    - = 딕셔너리에 추가한 순서를 기억한다.\n",
    "    - -> 순전파 때 추가한 순서대로 각 계층의 `forward()`를 호출하기만 하면 됨.\n",
    "    - -> 역전파 때 계층을 반대 순서로 호출하면 됨.\n",
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "- 기울기를 구하는 방법\n",
    "    - 수치 미분\n",
    "    - 해석적 방법 -> 오차역전파법: 매개변수가 많아도 효율적으로 계산할 수 있음.\n",
    "- 수치 미분은 구현이 쉽고, 오차역전파법은 구현이 복잡하기 때문에 수치 미분의 결과와 오차역전파법의 결과를 비기ㅛ해 오차역전파법을 제대로 구현했는지 검증이 필요하다.\n",
    "- -> **기울기 확인**: 두 방식으로 구한 기울기가 일치하는지를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 5.318604853360182e-10\n",
      "b1: 3.778228039993861e-09\n",
      "W2: 5.190212139522985e-09\n",
      "b2: 1.405525142508801e-07\n"
     ]
    }
   ],
   "source": [
    "# 기울기 확인\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치 차이의 절대값을 구한 후, 그 절대값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \": \" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다\n",
    "- -> 오차역전파법으로 구한 기울기가 올바르다.\n",
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10841666666666666 0.1078\n",
      "0.90375 0.9084\n",
      "0.9224166666666667 0.9241\n",
      "0.9366833333333333 0.937\n",
      "0.9453333333333334 0.9451\n",
      "0.9515666666666667 0.9515\n",
      "0.9565166666666667 0.9526\n",
      "0.9602333333333334 0.9583\n",
      "0.9599166666666666 0.9576\n",
      "0.9672166666666666 0.963\n",
      "0.9701333333333333 0.9642\n",
      "0.9704 0.9637\n",
      "0.9726333333333333 0.9661\n",
      "0.9744 0.9685\n",
      "0.9747333333333333 0.9688\n",
      "0.9769333333333333 0.9674\n",
      "0.9778 0.969\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다.\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
