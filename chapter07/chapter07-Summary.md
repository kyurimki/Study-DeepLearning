# Chapter 7. 합성곱 신경망(CNN)
- **합성곱 신경망 convolutional neural network, CNN**: 이미지 인식 분야에서 딥러닝을 활용할 때 기초가 됨
## 7.1 전체 구조
- 신경망
	- 인접하는 계층의 모든 뉴런과 결합 -> **완전연결 fully-connected 전결합**
	- -> 완전히 연결된 계층 = **Affine 계층**
	- 완전연결 신경망은 Affine 계층 뒤에 활성화 함수의 ReLU/Sigmoid 계층으로 이어진다.
	- 그림처럼 층이 5개일 때 4개는 Affine-ReLU 층이고, 마지막 층은 Affine-소프트맥스 계층으로 구성된다.

![image](https://user-images.githubusercontent.com/61455647/116354744-d11bf180-a833-11eb-8a59-65cd56b76ada.png)

![image](https://user-images.githubusercontent.com/61455647/116355415-c9108180-a834-11eb-98ca-76f6fd689a32.png)
- CNN
	- 합성곱 계층(Conv)과 풀링 계층(Pooling)이 추가된다.
	- -> Affine-ReLU 연결이 Conv-ReLu(-Pooling) 흐름으로 연결된다.
	- 출력에 가까운 층에서는 Affine-ReLU 구성을 사용할 수 있다.
	- 마지막 출력 계층에서는 Affine-Softmax 조합이 그대로 사용된다.

## 7.2 합성곱 계층
### 7.2.1 완전연결 계층의 문제점
- 데이터의 형상이 무시된다.
	- ex. 입력 데이터가 이미지일 때, 이미지는 가로, 세로, 채널(색상)로 구성된 3차원 데이터를 1차원 데이터로 평탄화가 필요하다
	- 이미지는 3차원 형상이기 때문에 공간적인 정보가 담겨 있다.
		- ex. 공간적으로 가까운 픽셀은 값이 비슷하다.
		- ex. RGB의 각 채널은 서로 밀접하게 관련되어 있거나 거리가 먼 픽셀은 연관이 없다.
	- 완전연결 계층은 형상을 무시하고 모든 입력 데이터를 동등한 차원의 뉴런으로 취급해 형상에 담긴 정보를 살릴 수 없다.
- 합성곱 계층은 형상을 유지한다.
	- 이미지를 3차원 데이터로 입력 받아, 다음 계층에도 3차원 데이터로 전달한다.
	- CNN은 형상을 가진 데이터를 제대로 이해할 수 있다.
- **특징 맵 feature map**: CNN에서 합성곱 계층의 입출력 데이터
	- **입력 특징 맵 input feature map**: 합성곱 계층의 입력 데이터
	- **출력 특징 맵 output feature map**: 합성곱 계층의 출력 데이터

### 7.2.2 합성곱 연산
- 합성곱 연산 = 필터 연산

![image](https://user-images.githubusercontent.com/61455647/116359144-68377800-a839-11eb-912f-aa2f40e9cad2.png)

- 합성곱 연산은 입력 데이터에 필터(=커널)를 적용한다.
	- 위의 예에서 데이터와 필터는 세로/가로 방향의 형상을 가졌다. (높이(height), 너비(width))로 표기
	- 입력 (4,4), 필터 (3, 3), 출력 (2, 2)

![image](https://user-images.githubusercontent.com/61455647/116505578-3c2dfc80-a8f6-11eb-8744-8e4ae0f5732e.png)

- 합성곱 연산의 계산
	1. 필터의 윈도우(window, 예제에서 회색 3*3 부분)를 일정 간격으로 이동해가며 입력 데이터에 적용
	2. **단일 곱셈-누산 fused multiply-add, FMA**: 입력과 필터에서 대응하는 원소끼리 곱해 총합을 구함
	3. 결과를 출력의 해당 장소에 저장
	4. 과정을 모든 장소에서 수행해 완성
- 완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수 = 가중치
	- 편향은 필터를 적용한 후 모든 원소에 더해진다.
	- 편향은 항상 하나(1*1)만 존재한다.
- CNN에 편향까지 포함하면 다음과 같은 흐름이 된다.

![image](https://user-images.githubusercontent.com/61455647/116506076-69c77580-a8f7-11eb-96c8-19a6907caa27.png)

### 7.2.3 패딩
- **패딩 padding**
	- 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(ex. 0)으로 채움
	- 합성곱 연산에서 자주 이용하는 기법

![image](https://user-images.githubusercontent.com/61455647/116506249-db072880-a8f7-11eb-8f93-17cb010d917b.png)

- (4, 4) 크기의 입력 데이터에 폭이 1인 패딩을 적용하면 위와 같다.
	- 패딩으로 크기가 (4, 4)인 입력 데이터가 (6, 6)이 된다.
	- 이 때 (3, 3) 크기의 필터를 걸면 (4, 4) 크기의 출력 데이터가 생성된다.
	- 패딩을 2로 설정하면 입력 데이터 크기는 (8, 8), 3으로 설정하면 (10, 10)이 된다.
- 패딩은 출력 크기를 조정할 목적으로 사용
	- 합성곱 연산을 되풀이하는 심층 신경망에서 크기가 작아지면 어느 시점에서 출력의 크기가 1이 되면, 더 이상 합성곱 연산을 적용할 수 없게 된다.
	- ∴ 패딩을 통해 입력 데이터의 공간적 크기를 고정해 다음 계층에 전달할 수 있음

### 7.2.4 스트라이드
- **스트라이드 stride**: 필터를 적용하는 위치의 간격
	- 지금까지 예시에서는 스트라이드가 1이었음
	- 스트라이드를 2로 하면 필터를 적용하는 윈도우가 2칸씩 이동

![image](https://user-images.githubusercontent.com/61455647/116506789-f7f02b80-a8f8-11eb-98d7-9f80aed4ce6c.png)

- 예시처럼 크기가 (7, 7)인 입력 데이터에 스트라이드가 2인 필터를 적용하면 출력의 크기는 (3, 3)이다.
- 스트라이드를 키우면 출력의 크기는 작아지고, 패딩을 크게 하면 출력의 크기는 커진다.

![image](https://user-images.githubusercontent.com/61455647/116507272-e8bdad80-a8f9-11eb-9fd2-62d23916ad57.png)

- 입력 크기 (H, W), 필터 크기 (FH, FW), 출력 크기(OH, OW), 패딩 P, 스트라이드 S일 때 출력 크기는 다음과 같이 나타낼 수 있다.
	- OH, OW의 결과 값은 정수로 나눠 떨어지는 값이어야 한다.

### 7.2.5 3차원 데이터의 합성곱 연산
- 이미지는 세로/가로, 채널로 이루어진 3차원 데이터
- 2차원과 비교했을 때, 채널 방향으로 특징 맵이 늘었음.
	- 특징 맵이 여러 개라면 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 결과를 더해서 하나의 출력을 얻는다.

![image](https://user-images.githubusercontent.com/61455647/116508864-37b91200-a8fd-11eb-80ac-e30581fd69e1.png)

- 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다.
- 필터 자체의 크기는 원하는 값으로 설정할 수 있지만, 모든 채널의 필터가 같은 크기여야 한다.

### 7.2.6 블록으로 생각하기
- 3차원의 합성곱 연산은 데이터와 필터를 직육면체 블록이라고 생각하면 쉽다.
- 3차원 데이터 -> 다차원 배열: 데이터=(채널(channel), 높이(height), 너비(width)) = (C, H, W), 필터=(C, FH, FW)

![image](https://user-images.githubusercontent.com/61455647/116510171-73ed7200-a8ff-11eb-9f09-0f14292aae7f.png)

- 위의 예는 출력 데이터가 한 장의 특징 맵 = 채널이 1개인 특징 맵
- 합성곱 연산 출력으로 다수의 채널을 내보내려면 필터(가중치)를 다수 사용해야 함

![image](https://user-images.githubusercontent.com/61455647/116510407-d181be80-a8ff-11eb-9bd2-692d1614e05e.png)

- 위와 같이 필터 FN개를 적용하면, 출력 맵도 FN개가 생성된다. FN개의 맵을 모으면 형상이 (FN, OH, OW)인 블록 완성
- 합성곱 연산에서 필터의 수도 고려해야 함 -> 필터의 가중치 데이터 = (출력 채널 수, 입력 채널 수, 높이, 너비)의 4차원 데이터

![image](https://user-images.githubusercontent.com/61455647/116511104-e579f000-a900-11eb-97a9-3b39fa0bc559.png)

- 합성곱 연산에서 편향이 추가되면 다음과 같은 과정으로 나타낼 수 있다.
	- 편향은 채널 하나에 값 하나씩으로 구성
	- 편향의 형상=(FN, 1, 1), 필터의 출력 결과의 형상=(FN, OH, OW)
	- 편향과 필터의 출력 결과를 더할 때 브로드캐스트가 적용된다.

### 7.2.7 배치 처리
- 합성곱 연산에서 배치 처리를 위해 데이터 차원을 하나 늘려 4차원 데이터 (데이터 수, 채널 수, 높이, 너비) 순으로 저장

![image](https://user-images.githubusercontent.com/61455647/116512603-44d8ff80-a903-11eb-8d16-e191f8e8246d.png)

- 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이루어진다.

## 7.3 풀링 계층
- **풀링 pooling**: 세로/가로 방향의 공간을 줄이는 연산

![image](https://user-images.githubusercontent.com/61455647/116513018-ebbd9b80-a903-11eb-9b73-bdafdbed7dec.png)

- 2*2 최대 풀링(= 2*2 크기 영역에서 가장 큰 원소 하나 꺼냄)을 스트라이드 2로 처리하는 순서는 위와 같다.
	- **최대 풀링 max pooling**
		- 대상 영역에서 최대값을 취하는 연산
		- 이미지 인식에서 주로 사용됨
	- 보통 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정
	- **\* 평균 풀링 average pooling**: 대상 영역의 평균을 계산

### 7.3.1 풀링 계층의 특징
- **학습해야 할 매개변수가 없다**: 풀링은 대상 영역에서 최대값이나 평균을 취하기 때문에 학습할 것이 없다.
- **채널 수가 변하지 않는다**: 풀링 연산은 채널마다 독립적으로 계산하기 때문에 입력 데이터의 채널 수 그대로 출력 데이터로 내보낸다.

![image](https://user-images.githubusercontent.com/61455647/116515611-7a7fe780-a907-11eb-9d45-bb935a1e7711.png)

- **입력의 변화에 영향을 적게 받는다(강건하다)**
	- 입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않는다.
	- 다음과 같이 입력 데이터가 가로로 1원소만큼 어긋나도 출력은 같다.(데이터에 따라서는 다를 수 있다.)

![image](https://user-images.githubusercontent.com/61455647/116516814-08a89d80-a909-11eb-98e7-2404e7335759.png)

## 7.4 합성곱/풀링 계층 구현하기-7.5 CNN 구현하기
