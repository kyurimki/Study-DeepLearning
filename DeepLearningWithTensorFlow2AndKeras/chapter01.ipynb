{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "chapter01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-meRTND-paSy"
      },
      "source": [
        "# 01Ïû•. ÌÖêÏÑúÌîåÎ°ú 2.0ÏúºÎ°ú Ïã†Í≤ΩÎßù Íµ¨ÌòÑ\n",
        "## ÌÖêÏÑúÌîåÎ°ú(TensorFlow, TF)ÎûÄ?\n",
        "- Íµ¨Í∏Ä Î∏åÎ†àÏù∏ ÌåÄÏóêÏÑú Ïã¨Ï∏µ Ïã†Í≤ΩÎßù(deep neural networks)ÏùÑ ÏúÑÌï¥ Í∞úÎ∞úÌïú Ïò§ÌîàÏÜåÏä§ SW ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
        "- Ï£ºÏöî ÌäπÏßï\n",
        "\t- Python, C++, Java, R, GoÎ°ú ÏûëÏóÖ Í∞ÄÎä•\n",
        "\t- KerasÎäî TensorFlowÏôÄ ÌÜµÌï©Îêú Í≥†Í∏â Ïã†Í≤ΩÎßù API, SW Íµ¨ÏÑ± ÏöîÏÜåÏùò ÏÉÅÌò∏ÏûëÏö© Î∞©Ïãù ÏßÄÏ†ï\n",
        "\t- Î™®Îç∏ Î∞∞ÏπòÏôÄ ÏÉùÏÇ∞ Í≥ºÏ†ïÏóêÏÑú ÏâΩÍ≤å ÏÇ¨Ïö© Í∞ÄÎä•\n",
        "\t- TensorFlow 2.0ÏóêÎäî Ï†ïÏ†Å Í∑∏ÎûòÌîÑÏóê Í∏∞Î∞òÏùÑ Îëî Í∑∏ÎûòÌîÑ Ïó∞ÏÇ∞Í≥º Ï¶âÏãú Ïó∞ÏÇ∞(eager computation) ÏßÄÏõê ÎèÑÏûÖ\n",
        "\t- Í∞ïÎ†•Ìïú Ïª§ÎÆ§ÎãàÌã∞Ïùò ÏßÄÏõê: Github, Google TrendsÏóêÏÑú Ïù∏Í∏∞Î•º ÌôïÏù∏Ìï† Ïàò ÏûàÏùå\n",
        "    \n",
        "## ÏºÄÎùºÏä§(Keras)ÎûÄ?\n",
        "- Îî•Îü¨Îãù Î™®Îç∏ÏùÑ ÎßåÎì§Í≥† ÌõàÎ†®ÌïòÍ≥†Ïûê Í∏∞Ï¥à Íµ¨ÏÑ± ÏöîÏÜåÎ•º Íµ¨ÏÑ±ÌïòÎäî API\n",
        "- Ïó¨Îü¨ Îî•Îü¨Îãù ÏóîÏßÑ(GoogleÏùò TensorFlow, MSÏùò CNTK, AmazonÏùò MxNet, Theano etc.)\n",
        "- TensorFlow 2.0Î∂ÄÌÑ∞ ÏºÄÎùºÏä§Í∞Ä ÌëúÏ§Ä ÌïòÏù¥Î†àÎ≤® APIÎ°ú Ï±ÑÌÉù -> ÏΩîÎî© Îã®ÏàúÌôî, Ï¢Ä Îçî ÏßÅÍ¥ÄÏ†ÅÏù∏ ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç\n",
        "\n",
        "## TensorFlow 2.0Ïùò Í∞ÄÏû• Ï§ëÏöîÌïú Î≥ÄÌôî\n",
        "- KerasÎäî TensorFlowÏùò ÏùºÎ∂ÄÏù¥Í∏∞ ÎïåÎ¨∏Ïóê Î∂ÑÎ¶¨Ïóê ÏùòÎØ∏Í∞Ä ÏóÜÏùå\n",
        "- keras vs. tf.keras\n",
        "\t- tf.kerasÎäî Tensorflow ÎÇ¥Î∂ÄÏóê ÏºÄÎùºÏä§ Íµ¨ÌòÑÌïú Í≤É\n",
        "\t- Îã§Î•∏ TensorFlow APIÏôÄ Îçî ÎÇòÏùÄ ÌÜµÌï©ÏùÑ ÏõêÌïòÎ©¥ tf.keras ÏÇ¨Ïö©\n",
        "- TensorFlow 1.0 ÏÑ§ÏπòÌïòÍ∏∞\n",
        "    - CPUÎßå ÏûàÏùÑ Í≤ΩÏö∞: `pip install tensorflow`\n",
        "    - GPUÎèÑ ÏûàÎäî Í≤ΩÏö∞: `pip install tensorflow-gpu`\n",
        "- TensorFlow 2.0 ÏÑ§ÏπòÌïòÍ∏∞\n",
        "    - CPUÎßå ÏûàÏùÑ Í≤ΩÏö∞: `pip install tensorflow==2.0.0-alpha0`\n",
        "    - GPUÎèÑ ÏûàÎäî Í≤ΩÏö∞: `pip install tensorflow-gpu==2.0.0-alpha0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYJYXiKqpjhX"
      },
      "source": [
        "# TensorFlow 1.0ÏóêÏÑú Ïã†Í≤ΩÎßùÏùÑ ÏΩîÎî©ÌïòÎäî Ï†ÑÌÜµÏ†ÅÏù∏ Î∞©Î≤ï(11 lines)\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution() # To avoid RuntimeError: tf.placeholder() is not compatible with eager execution\n",
        "\n",
        "in_a = tf.placeholder(dtype=tf.float32, shape=(2))\n",
        "\n",
        "def model(x):\n",
        "    with tf.variable_scope(\"matmul\"):\n",
        "        W = tf.get_variable(\"W\", initializer=tf.ones(shape=(2, 2)))\n",
        "        b = tf.get_variable(\"b\", initializer=tf.zeros(shape=(2)))\n",
        "        return x * W + b\n",
        "\n",
        "out_a = model(in_a)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    outs = sess.run([out_a], feed_dict={in_a: [1, 0]})"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kJ8l5TPpaS6",
        "outputId": "eb83bfbf-5457-4438-d92e-6fcf796445a5"
      },
      "source": [
        "# TensorFlow 2.0ÏúºÎ°ú ÏΩîÎî©(8 lines)\n",
        "import tensorflow as tf\n",
        "W = tf.Variable(tf.ones(shape=(2, 2)), name=\"W\")\n",
        "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
        "\n",
        "@tf.function\n",
        "def model(x):\n",
        "    return W * x + b\n",
        "\n",
        "out_a = model([1, 0])\n",
        "\n",
        "print(out_a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"StatefulPartitionedCall:0\", shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj8p0GzRpaS8"
      },
      "source": [
        "## Ïã†Í≤ΩÎßù ÏÜåÍ∞ú: Ïù∏Í≥µ Ïã†Í≤ΩÎßù(Artificial Neural Networks, nets, ANN)\n",
        "- Ìè¨Ïú†Î•òÏùò Ï§ëÏ∂î Ïã†Í≤ΩÍ≥Ñ Ïó∞Íµ¨ÏóêÏÑú ÏòÅÍ∞ê Î∞õÏùÄ Î®∏Ïã†Îü¨Îãù Î™®Îç∏Ïùò ÏùºÏ¢Ö\n",
        "- Í∞Å Ïã†Í≤ΩÎßùÏùÄ ÏÑúÎ°ú Ïó∞Í≤∞Îêú ÎßéÏùÄ Îâ¥Îü∞ÏúºÎ°ú Íµ¨ÏÑ±\n",
        "- Ìïú Í≥ÑÏ∏µ(layer)Ïùò Îâ¥Îü∞ÏùÄ ÌäπÏ†ï ÏÉÅÌÉúÍ∞Ä ÎêòÎ©¥ Îã§Î•∏ Í≥ÑÏ∏µÏúºÎ°ú Î©îÏãúÏßÄÎ•º ÍµêÌôò(Î∞úÌôî, fire)ÌïòÍ≥†, Ïã†Í≤ΩÎßùÏùÄ Í≥ÑÏÇ∞ ÏàòÌñâ\n",
        "- Î≥ÄÏ≤ú Í≥ºÏ†ï\n",
        "    - 1950ÎÖÑÎåÄ ÌõÑÎ∞ò: Îã®Ïàú Í≥ÑÏÇ∞ÏùÑ ÏúÑÌïú Îëê Í≥ÑÏ∏µÏùò ÌçºÏÖâÌä∏Î°†\n",
        "    - 1960ÎÖÑÎåÄ: Îã§Í≥ÑÏ∏µ ÌõàÎ†®ÏùÑ ÏúÑÌïú Ïó≠Ï†ÑÌåå(backpropagation) ÏïåÍ≥†Î¶¨Ï¶ò\n",
        "    - 1980ÎÖÑÎåÄ Îã§Î•∏ Îã®Ïàú Í∏∞Î≤ïÎì§Ïù¥ Îçî Ìö®Í≥ºÏ†ÅÏù∏ Î∞©Î≤ïÏù¥ Îê† ÎïåÍπåÏßÄ ÌïôÎ¨∏ Ïó∞Íµ¨Ïùò Ï£ºÏöî Ï£ºÏ†ú\n",
        "    - 2000ÎÖÑÎåÄ Ï§ëÎ∞ò: G. HintonÏù¥ Ï†úÏïàÌïú ÌöçÍ∏∞Ï†ÅÏù∏ Îπ†Î•∏ ÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÍ≥º 2011ÎÖÑÍ≤Ω ÎåÄÎüâ Ïó∞ÏÇ∞Ïù¥ Í∞ÄÎä•Ìïú GPUÏùò Îì±Ïû•Í≥º ÌõàÎ†®Ïóê Ïì∞Ïùº ÎåÄÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞Ïùò ÏàòÏßë Í∞ÄÎä• -> deep learningÏùò Î∞úÌåê\n",
        "- Ï†êÏßÑÏ†Å Ï∂îÏÉÅÌôîÎ•º ÌÜµÌïú ÌïôÏäµÏùÄ Ïù∏Í∞ÑÏùò ÎëêÎáåÏóêÏÑú ÏàòÎ∞±Îßå ÎÖÑ ÎèôÏïà ÏßÑÌôîÌï¥ Ïò® Î™®Îç∏Í≥º ÎãÆÏïòÎã§.\n",
        "    - Ïù∏Í∞ÑÏùò ÏãúÍ∞Å ÏãúÏä§ÌÖúÏùÄ Ïó¨Îü¨ Í≥ÑÏ∏µÏúºÎ°ú Íµ¨ÏÑ±\n",
        "    - ÎààÏùÄ ÏãúÍ∞Å ÌîºÏßà(Visual Cortex)(V1) ÏòÅÏó≠Í≥º Ïó∞Í≤∞\n",
        "        - Í∏∞Î≥∏Ï†ÅÏù∏ ÌäπÏßïÍ≥º ÏãúÍ∞ÅÏ†Å Î∞©Ìñ•, Í≥µÍ∞Ñ Ï£ºÌååÏàò, ÏÉâÏÉÅÏùò ÏûëÏùÄ Î≥ÄÌôî Íµ¨Î∂Ñ\n",
        "        - ÏïΩ 1Ïñµ 4Ï≤úÎßå Í∞úÏùò Îâ¥Îü∞ÏúºÎ°ú Íµ¨ÏÑ±, ÏàòÏã≠Ïñµ Í∞úÏùò Ïó∞Í≤∞Î°ú Íµ¨ÏÑ±\n",
        "        - ÌòïÌÉú, ÏñºÍµ¥, ÎèôÎ¨º Îì±Í≥º Í∞ôÏù¥ Ï¢Ä Îçî Î≥µÏû°Ìïú Í∞úÎÖêÏùÑ Ïù∏ÏãùÌïòÍ≥† Îçî Î≥µÏû°Ìïú Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨Î•º ÌïòÎäî Îã§Î•∏ ÏòÅÏó≠(V2, V3, V4, V5, V6)ÏôÄ Ïó∞Í≤∞\n",
        "    - Îî•Îü¨ÎãùÏùÄ Ïù∏Í∞ÑÏùò ÏãúÍ∞Å ÏãúÏä§ÌÖú Ï°∞ÏßÅÏóêÏÑú ÏòÅÍ∞êÏùÑ ÏñªÏùå\n",
        "\n",
        "## ÌçºÏÖâÌä∏Î°†(Perceptron)\n",
        "- ÏûÖÎ†• ÌäπÏßï(feature) ÎòêÎäî Í∞ÑÎã®Ìïú ÌäπÏßïÏù∏ nÍ∞úÏùò ÌÅ¨Í∏∞Î•º Í∞ñÎäî ÏûÖÎ†• Î≤°ÌÑ∞(x1, x2, ..., xn)Ïù¥ Ï£ºÏñ¥ÏßÄÎ©¥ 1(True) ÎòêÎäî 0(False)Î•º Ï∂úÎ†•ÌïòÎäî Í∞ÑÎã®Ìïú ÏïåÍ≥†Î¶¨Ï¶ò\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117759330-91570000-b25e-11eb-9dc7-a836525c3d0c.png)\n",
        "\n",
        "- w: Í∞ÄÏ§ëÏπò Î≤°ÌÑ∞, wx: dot product, b: Ìé∏Ìñ•(bias)\n",
        "- wx + b: wÏôÄ bÏóê Ìï†ÎãπÎêú Í∞íÏóê Îî∞Îùº ÏúÑÏπòÎ•º Î≥ÄÍ≤ΩÌïòÎäî Ï¥àÌèâÎ©¥(hyperplane) Í≤ΩÍ≥Ñ Ï†ïÏùò\n",
        "    - Ï¥àÌèâÎ©¥: ÎëòÎü¨Ïãº Í≥µÍ∞Ñ(ambient space)Î≥¥Îã§ Ìïú Ï∞®ÏõêÏù¥ ÎÇÆÏùÄ Î∂ÄÍ≥µÍ∞Ñ(subspace)\n",
        "    - ÏûÖÎ†•Ïù¥ 3Í∞úÏùò ÌäπÏßï(Îπ®Í∞ï, ÎÖπÏÉâ, ÌååÎûÄÏÉâÏùò Ïñë)Ïùº Îïå ÌçºÏÖâÌä∏Î°†ÏùÄ ÏÉâÏÉÅÏù¥ Ìù∞ÏÉâÏù∏ÏßÄ ÏïÑÎãåÏßÄÎ•º Í≤∞Ï†ï\n",
        "![image](https://user-images.githubusercontent.com/61455647/117759499-daa74f80-b25e-11eb-83a2-d65e3c8b50fa.png)\n",
        "\n",
        "- ÌçºÏÖâÌä∏Î°†ÏùÄ 'ÏïÑÎßàÎèÑ'ÎùºÎäî Í≤∞Í≥ºÎ•º ÌëúÌòÑÌï† Ïàò ÏóÜÎã§\n",
        "\n",
        "### TensorFlow 2.0 ÏΩîÎìú Ï≤´ Î≤àÏß∏ ÏòàÏ†ú\n",
        "- tf.kerasÎ°ú Î™®Îç∏ÏùÑ ÏûëÏÑ±ÌïòÎäî Î∞©Î≤ï: Sequential API, Functional API, Model Subclassing\n",
        "- `Sequential()` Î™®Îç∏\n",
        "    - Ïã†Í≤ΩÎßù Í≥ÑÏ∏µÏùò ÏÑ†Ìòï ÌååÏù¥ÌîÑÎùºÏù∏(=stack)\n",
        "    - ÏïÑÎûòÏùò ÏΩîÎìúÎäî 784Í∞úÏùò ÏûÖÎ†• Î≥ÄÏàò(feature)Î•º Ï∑®ÌïòÎäî 10Í∞úÏùò Ïù∏Í≥µ Îâ¥Îü∞ÏùÑ Í∞ÄÏßÑ Îã®Ïùº Í≥ÑÏ∏µ Ï†ïÏùò\n",
        "    - ÎßùÏù¥ Î∞ÄÏßë(dense) = Í∞Å Í≥ÑÏ∏µÏùò Îâ¥Îü∞Ïù¥ Ïù¥Ï†Ñ Í≥ÑÏ∏µÏóê ÏúÑÌïú Î™®Îì† Îâ¥Îü∞Í≥º ÏôÑÏ†Ñ Ïó∞Í≤∞ÎêòÏñ¥ ÏûàÍ≥†, Í∑∏ Îã§Ïùå Í≥ÑÏ∏µÏóê ÏûàÎäî Î™®Îì† Îâ¥Îü∞Í≥ºÎèÑ ÏôÑÏ†Ñ Ïó∞Í≤∞ÎêòÏñ¥ ÏûàÎã§.\n",
        "    - Í∞Å Îâ¥Îü∞ÏùÄ `kernel_initializer` Îß§Í∞úÎ≥ÄÏàòÎ•º ÌÜµÌï¥ ÌäπÏ†ï Í∞ÄÏ§ëÏπòÎ°ú Ï¥àÍ∏∞ÌôîÌï† Ïàò ÏûàÎã§.\n",
        "        - `random_uniform`: Í∞ÄÏ§ëÏπòÎäî -0.05~0.05 ÏÇ¨Ïù¥ÏóêÏÑú Í∑†Îì±ÌïòÍ≤å ÎûúÎç§ Î∂ÑÌè¨\n",
        "        - `random_normal`: Í∞ÄÏ§ëÏπòÎäî Í∞ÄÏö∞Ïä§ Î∂ÑÌè¨Ïóê Îî∞Îùº ÌèâÍ∑†Ïù¥ 0Ïù¥Í≥† ÏûëÏùÄ ÌëúÏ§Ä Ìé∏Ï∞® 0.05Î°ú Ï¥àÍ∏∞Ìôî\n",
        "        - `zero`: Î™®Îì† Í∞ÄÏ§ëÏπòÎäî 0ÏúºÎ°ú Ï¥àÍ∏∞Ìôî"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYnEwNtypaS8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "NB_CLASSES = 10\n",
        "RESHAPED = 784\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(NB_CLASSES, input_shape=(RESHAPED,), kernel_initializer='zeros', name='dense_layer', activation='softmax'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5HGO7yVpaS9"
      },
      "source": [
        "## Îã§Ï∏µ ÌçºÏÖâÌä∏Î°†: Ïã†Í≤ΩÎßù Ï≤´ Î≤àÏß∏ ÏòàÏ†ú\n",
        "- PerceptronÏùÄ Îã®Ïùº ÏÑ†Ìòï Í≥ÑÏ∏µ Î™®Îç∏Ïùò Ïù¥Î¶Ñ, Îã®ÏàúÌïú ÏÑ†Ìòï Ìï®Ïàò\n",
        "- **Îã§Ï∏µ ÌçºÏÖâÌä∏Î°† MLP, Multi-Layer Perceptron**\n",
        "    - Ïó¨Îü¨ Í∞úÏùò Í≥ÑÏ∏µÏù¥ ÏûàÎäî Í≤ΩÏö∞, Ïó¨Îü¨ Í∞úÏùò Îã®Ïùº Í≥ÑÏ∏µÏù¥ ÏåìÏûÑ\n",
        "    - ÏûÖÎ†•Í≥º Ï∂úÎ†• Í≥ÑÏ∏µÏùÄ Ïô∏Î∂ÄÏóêÏÑú Î≥º Ïàò ÏûàÏßÄÎßå Ï§ëÍ∞ÑÏùò Îã§Î•∏ Î™®Îì† Í≥ÑÏ∏µÏùÄ Ïà®Í≤®Ï†∏ ÏûàÏùå -> **ÏùÄÎãâÏ∏µ hidden layers**\n",
        "    - Ï≤´ Î≤àÏß∏ ÏùÄÎãâÏ∏µÏùò Í∞Å ÎÖ∏ÎìúÎäî ÏûÖÎ†• Î∞õÍ≥† ÏÑ†Ìòï Ìï®ÏàòÏóê Ïó∞Í≥ÑÎêú Í∞íÏóê Îî∞Îùº Î∞úÌôî(fire) -> Ï≤´ Î≤àÏß∏ ÏùÄÎãâÏ∏µÏùò Ï∂úÎ†•ÏùÄ Îã§Î•∏ ÏÑ†Ìòï Ìï®ÏàòÍ∞Ä Ï†ÅÏö©Îêú Îëê Î≤àÏß∏ Í≥ÑÏ∏µÏúºÎ°ú Ï†ÑÎã¨ -> ÌïòÎÇòÏùò Îã®Ïùº Îâ¥Îü∞ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏµúÏ¢Ö Ï∂úÎ†• Í≥ÑÏ∏µÏúºÎ°ú Ï†ÑÎã¨\n",
        "    \n",
        "![image](https://user-images.githubusercontent.com/61455647/117767999-ac307100-b26c-11eb-9875-5f3ab2391de6.png)\n",
        "\n",
        "### ÌçºÏÖâÌä∏Î°† ÌõàÎ†®Ïùò Î¨∏Ï†úÏ†êÍ≥º Ìï¥Í≤∞Ï±Ö\n",
        "- Îã®Ïùº Îâ¥Îü∞Ïùº Îïå Í∞ÄÏ§ëÏπò wÏôÄ Ìé∏Ìñ• bÏùò Í∞íÏúºÎ°ú Í∞ÄÏû• Ï†ÅÌï©Ìïú Í≤É: Ïù¥ÏÉÅÏ†ÅÏúºÎ°ú ÏùºÎ†®Ïùò ÌõàÎ†® ÏòàÎ•º Ï†úÍ≥µÌïòÍ≥† Ïª¥Ìì®ÌÑ∞Í∞Ä Ï∂úÎ†•ÏóêÏÑú Î∞úÏÉùÌïòÎäî Ïò§Ï∞®Î•º ÏµúÏÜåÌôîÌïòÎäî Î∞©ÏãùÏúºÎ°ú Í∞ÄÏ§ëÏπòÏôÄ Ìé∏Ìñ• Ï°∞Ï†ï\n",
        "- ex. Í≥†ÏñëÏù¥ Ïù¥ÎØ∏ÏßÄÎ•º Ìè¨Ìï®Ìïú Í≤ÉÍ≥º Í∑∏Î†áÏßÄ ÏïäÏùÄ Î≥ÑÎèÑÏùò Ïù¥ÎØ∏ÏßÄ ÏßëÌï©Ïù¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï, Í∞Å Îâ¥Îü∞ÏùÄ Ïù¥ÎØ∏ÏßÄÏùò Îã®Ïùº ÌîΩÏÖÄ Í∞íÏóêÏÑú ÏûÖÎ†•ÏùÑ Î∞õÎäîÎã§Í≥† Í∞ÄÏ†ï\n",
        "- -> Ïª¥Ìì®ÌÑ∞Í∞Ä Ïù¥ÎØ∏ÏßÄÎ•º Ï≤òÎ¶¨ÌïòÎ©¥ÏÑú Í∞Å Îâ¥Îü∞Ïù¥ Í∞ÄÏ§ëÏπòÏôÄ Ìé∏Ìñ•ÏùÑ Ï°∞Ï†ïÌï¥ ÏûòÎ™ª Ïù∏ÏãùÎêòÎäî Ïù¥ÎØ∏ÏßÄÏùò ÎπÑÏú®Ïù¥ Ï†êÏ∞® Ï§ÑÏñ¥Îì§Í∏∞Î•º ÏõêÌï®\n",
        "- -> Ï∂úÎ†•Ïóê ÏïÑÏ£º ÏûëÏùÄ Î≥ÄÌôîÎßå ÏùºÏúºÌÇ§Î†§Î©¥ Í∞ÄÏ§ëÏπò(or Ìé∏Ìñ•)ÎèÑ ÏïΩÍ∞ÑÎßå Î≥ÄÍ≤ΩÌï¥Ïïº Ìï®, Ï∂úÎ†•Ïóê ÌÅ∞ Î≥ÄÌôîÍ∞Ä ÏÉùÍ∏¥Îã§Î©¥ Ï†êÏßÑÏ†ÅÏù∏ ÌïôÏäµ X\n",
        "- ÌçºÏÖâÌä∏Î°†ÏùÄ 0Í≥º 1Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê 'Ï°∞Í∏àÏî©'Ïùò ÏûëÎèôÏùÑ Î≥¥Ïù¥ÏßÄ ÏïäÏùå\n",
        "![image](https://user-images.githubusercontent.com/61455647/117768062-c9653f80-b26c-11eb-976f-6b672f59b7a6.png)\n",
        "\n",
        "- -> Î∂àÏó∞ÏÜç(discontinutiy) ÏóÜÏù¥ 0ÏóêÏÑú 1Î°ú Ï†êÏßÑÏ†ÅÏúºÎ°ú Î≥ÄÍ≤ΩÎêòÎäî Ìï®Ïàò ÌïÑÏöî <=> ÎØ∏Î∂Ñ Í∞ÄÎä•Ìïú Ïó∞ÏÜç Ìï®ÏàòÍ∞Ä ÌïÑÏöî\n",
        "\n",
        "### ÌôúÏÑ±Ìôî Ìï®Ïàò: ÏãúÍ∑∏Î™®Ïù¥Îìú(sigmoid)\n",
        "- ùúé(ùë•)= 1/(1+ùëí^(‚àíùë•))\n",
        "- ÏûÖÎ†•Ïù¥ (‚àí‚àû, ‚àû)ÏóêÏÑú Î≥ÄÌï† Îïå Ï∂úÎ†•ÏùÄ (0, 1)ÏóêÏÑú Î≥ÄÌôî\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117768191-fdd8fb80-b26c-11eb-876f-3dc97f54dc0e.png)\n",
        "\n",
        "- ÎπÑÏÑ†Ìòï Ìï®Ïàò ùúé(z = wùë• + b) Í≥ÑÏÇ∞Ïóê ÏãúÍ∑∏Î™®Ïù¥ÎìúÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§.\n",
        "    - z = wùë• + bÍ∞Ä Îß§Ïö∞ ÌÅ¨Í≥† ÏñëÏàò, ùëí^(‚àíz) -> 0Ïù¥ÎØÄÎ°ú ùúé(z) -> 1\n",
        "    - z = wùë• + bÍ∞Ä Îß§Ïö∞ ÌÅ¨Í≥† ÏùåÏàò, ùëí^(‚àíz) -> ‚àûÏù¥ÎØÄÎ°ú ùúé(z) -> 0\n",
        "- ÏãúÍ∑∏Î™®Ïù¥Îìú ÌôúÏÑ±Ìôî Ìï®ÏàòÎ•º ÏÇ¨Ïö©Ìïú Îâ¥Îü∞Ïùò Í≤ΩÏö∞ ÌçºÏÖâÌä∏Î°†Í≥º Ïú†ÏÇ¨Ìïú ÏûëÎèôÏùÑ ÌïòÏßÄÎßå, Í∑∏ Î≥ÄÌôîÎäî Ï†êÏßÑÏ†ÅÏù¥Í≥† Ï∂úÎ†•Í∞íÎèÑ ÏôÑÏ†ÑÌûà Ïú†Ìö®ÌïòÎã§.\n",
        "\n",
        "### ÌôúÏÑ±Ìôî Ìï®Ïàò: tanh\n",
        "- tanh(ùëß)=(ùëí^ùëß‚àíùëí^(‚àíùëß))/(ùëí^ùëß+ùëí^(‚àíùëß))\n",
        "- Ï∂úÎ†• Î≤îÏúÑ: [-1, 1]\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117768839-e77f6f80-b26d-11eb-9ec6-8bd8f253cce4.png)\n",
        "\n",
        "### ÌôúÏÑ±Ìôî Ìï®Ïàò: ReLU(Rectified Linear Unit)\n",
        "- ÏãúÍ∑∏Î™®Ïù¥ÎìúÏóêÏÑú Î∞úÍ≤¨Îêú ÏùºÎ∂Ä ÏµúÏ†ÅÌôî Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî Îç∞Ïóê ÎèÑÏõÄ\n",
        "- f(x) = max(0, x)\n",
        "    - ÏùåÏàò Í∞íÏóê ÎåÄÌï¥ÏÑúÎäî Ìï≠ÏÉÅ 0, ÏñëÏùò Í∞íÏóê ÎåÄÌï¥ ÏÑ†ÌòïÏúºÎ°ú Ï¶ùÍ∞Ä\n",
        "- ÏãúÍ∑∏Î™®Ïù¥ÎìúÏóê ÎπÑÌï¥ Íµ¨ÌòÑÏù¥ Îß§Ïö∞ Í∞ÑÎã®\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117769160-4b099d00-b26e-11eb-847e-19f163ca5c46.png)\n",
        "\n",
        "### Ï∂îÍ∞ÄÏ†ÅÏù∏ Îëê Í∞úÏùò ÌôúÏÑ±Ìôî Ìï®Ïàò: ELUÏôÄ LeakyReLU\n",
        "- ELU\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117770589-10a0ff80-b270-11eb-9974-0afcca168f98.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117769881-38dc2e80-b26f-11eb-9fec-5712939b3c60.png)\n",
        "\n",
        "- LeakyReLU\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117770387-cfa8eb00-b26f-11eb-9fa2-92d9e5b9d4af.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117770133-8b1d4f80-b26f-11eb-9798-88d712508eb3.png)\n",
        "\n",
        "- Îëê Ìï®Ïàò Î™®Îëê xÍ∞Ä ÏùåÏàòÏùº Îïå ÏûëÏùÄ Î≥ÄÌôîÎ•º ÏùºÏúºÏºú Í≤ΩÏö∞Ïóê Îî∞Îùº Ïú†Ïö©Ìï† Ïàò ÏûàÎã§.\n",
        "\n",
        "### ÌôúÏÑ±Ìôî Ìï®Ïàò(activation functions)\n",
        "- ('Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌïòÍ∞ï' Ï†àÏóêÏÑú) ÏãúÍ∑∏Î™®Ïù¥ÎìúÏôÄ ReLU Ìï®ÏàòÍ∞Ä Î≥¥Ïó¨Ï£ºÎäî Ï†ÑÌòïÏ†ÅÏù∏ Ï†êÏßÑÏ†ÅÏù∏ Î≥ÄÌôî ÌòïÌÉúÍ∞Ä Ïã†Í≤ΩÎßùÏóêÏÑú Ïò§Ï∞®Î•º Ï°∞Í∏àÏî© Ï§ÑÏù¥Î©∞ Ï†ÅÏùëÌï¥ ÎÇòÍ∞ÄÎäî ÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏùÑ Í∞úÎ∞úÌïòÎäî Í∏∞Î≥∏ Íµ¨ÏÑ± ÏöîÏÜåÏûÑÏùÑ Ïïå Ïàò ÏûàÏùÑ Í≤ÉÏù¥Îã§.\n",
        "- ÏûÖÎ†• Î≤°ÌÑ∞(x1, x2, ..., xm), Í∞ÄÏ§ëÏπò Î≤°ÌÑ∞(w1, w2, ..., wm), Ìé∏Ìñ• b, Ìï©Í≥Ñ Œ£Ïùº Îïå ÌôúÏÑ±Ìôî Ìï®Ïàò ùúéÎäî Îã§ÏùåÍ≥º Í∞ôÎã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117771097-b5bbd800-b270-11eb-8a9b-2fc62d26d68b.png)\n",
        "\n",
        "### Í∞ÑÎã®Ìûà ÎßêÌï¥: Í≤∞Íµ≠ Ïã†Í≤ΩÎßùÏù¥ÎûÄ?\n",
        "- Ïñ¥Îñ§ ÏûÖÎ†•ÏùÑ Ìï¥Îãπ Ï∂úÎ†•ÏúºÎ°ú Îß§ÌïëÌïòÎäî Ìï®ÏàòÎ•º Í≥ÑÏÇ∞ÌïòÎäî Î∞©Î≤ï\n",
        "- ÎπÑÏÑ†Ìòï ÌôúÏÑ±ÌôîÏôÄ Í≤∞Ìï©Ìï¥ Ïó¨Îü¨ Í≥ÑÏ∏µÏúºÎ°ú ÏåìÏùÑ Í≤ΩÏö∞ Í±∞Ïùò Î™®Îì† Í≤ÉÏùÑ ÌïôÏäµÌï† Ïàò ÏûàÎã§.\n",
        "- ÏµúÏ†ÅÌôîÌïòÎ†§Îäî Ï†ÅÏ†àÌïú Ï≤ôÎèÑ(ÏÜêÏã§Ìï®Ïàò(loss function)), ÌïôÏäµÌïòÍ∏∞Ïóê Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞, Ï∂©Î∂ÑÌïú Ïó∞ÏÇ∞ Îä•Î†• ÌïÑÏöî\n",
        "- **ÌïôÏäµ**: Î≥∏ÏßàÏ†ÅÏúºÎ°ú ÎØ∏ÎûòÏùò Í≤∞Í≥ºÎ•º ÏòàÏ∏°ÌïòÍ≥†Ïûê ÌôïÎ¶ΩÎêú Í¥ÄÏ∞∞ÏùÑ ÏùºÎ∞òÌôîÌïòÎ†§Îäî Í≤ÉÏùÑ Î™©ÌëúÎ°úÌïòÎäî Í≥ºÏ†ï\n",
        "\n",
        "## Ïã§Ï†ú ÏòàÏ†ú: ÌïÑÍ∏∞Ï≤¥ Ïà´Ïûê Ïù∏Ïãù\n",
        "- MNIST ÌïÑÍ∏∞Ï≤¥ Ïà´Ïûê Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÇ¨Ïö©\n",
        "- **ÏßÄÎèÑÌïôÏäµ Supervised Learning**\n",
        "    - Î®∏Ïã†Îü¨ÎãùÏóêÏÑú Ï†ïÎãµÏù¥ ÏûàÎäî Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö© -> Ïã†Í≤ΩÎßùÏùÑ Í∞úÏÑ†ÌïòÍ≥†Ïûê ÌõàÎ†® ÏòàÏãú ÏÇ¨Ïö©\n",
        "    - Î†àÏù¥Î∏îÏù¥ ÏóÜÎã§Í≥† Ïã†Í≤ΩÎßùÏù¥ ÏòàÏ∏°ÏùÑ ÏàòÌñâÌïú ÌõÑ Î†àÏù¥Î∏îÏùÑ ÌôïÏù∏Ìï¥ Ïã†Í≤ΩÎßùÏù¥ ÏñºÎßàÎÇò Ïà´ÏûêÎ•º Ïûò Ïù∏ÏãùÌïòÎäîÏßÄ ÌèâÍ∞ÄÌï† Ïàò ÏûàÎã§.\n",
        "\n",
        "### ÏõêÌï´ Ïù∏ÏΩîÎî©(OHE, One-Hot Encoding)\n",
        "- Ïã†Í≤ΩÎßù ÎÇ¥Î∂ÄÏóê ÏÇ¨Ïö©Îê† Ï†ïÎ≥¥Î•º Ïù∏ÏΩîÎî©ÌïòÎäî Í∞ÑÎã®Ìïú ÎèÑÍµ¨\n",
        "- Î≤îÏ£ºÌòï ÌäπÏßïÏùÑ Ïà´ÏûêÌòï Î≥ÄÏàòÎ°ú Î≥ÄÌôò\n",
        "- ex. [0-9]Ïùò Í∞í dÎ•º Í∞ñÎäî Î≤îÏ£ºÌòï ÌäπÏßï ÏàòÎäî 10Í∞úÏùò ÏúÑÏπòÎ•º Í∞ÄÏßÑ Ïù¥ÏßÑ Î≤°ÌÑ∞Î°ú Íµ¨ÏÑ±Ìï¥ dÎ≤àÏß∏ ÏúÑÏπòÎßå 1Î°ú ÌïòÍ≥† ÎÇòÎ®∏ÏßÄÎäî Ìï≠ÏÉÅ 0 Í∞íÏùÑ Í∞ñÎèÑÎ°ù Ïù∏ÏΩîÎî©\n",
        "- ÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏù¥ ÏàòÏπòÌòï Ìï®ÏàòÎ•º Ï≤òÎ¶¨ÌïòÎèÑÎ°ù ÌäπÌôîÎê† Í≤ΩÏö∞ ÏÇ¨Ïö©Îê®\n",
        "\n",
        "### TensorFlow 2.0ÏúºÎ°ú Îã®Ïàú Ïã†Í≤ΩÎßù Ï†ïÏùò\n",
        "- TensorFlow 2.0ÏùÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î°úÎìúÌïòÍ≥† Ïã†Í≤ΩÎßùÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÎäî ÌõàÎ†® ÏßëÌï© `X_train`ÏúºÎ°úÏùò Î∂ÑÌï†, Ïã†Í≤ΩÎßùÏùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞Ïóê ÏÇ¨Ïö©ÌïòÎäî ÌÖåÏä§Ìä∏ ÏßëÌï© `X_test`Î°ú Î∂ÑÌï†ÌïòÎäî Ï†ÅÏ†àÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï†úÍ≥µ\n",
        "- Îç∞Ïù¥ÌÑ∞Îäî Ïã†Í≤ΩÎßùÏùÑ ÌõàÎ†®Ìï† Îïå 32ÎπÑÌä∏ Ï†ïÎ∞ÄÎèÑÎ•º Í∞ñÎèÑÎ°ù `float32`Î°ú Î≥ÄÌôò, [0, 1] Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî\n",
        "- Ïã§Ï†ú Î†àÏù¥Î∏îÏùÑ Í∞ÅÍ∞Å `Y_train`Í≥º `Y_test`Ïóê Î°úÎìúÌïòÍ≥† ÏõêÌï´ Ïù∏ÏΩîÎî© ÏàòÌñâ\n",
        "- `EPOCH`: ÌõàÎ†®ÏùÑ ÏñºÎßàÎÇò ÏßÄÏÜçÌï† Í≤ÉÏù∏ÏßÄ\n",
        "- `BATCH_SIZE`: Ìïú Î≤àÏóê Ïã†Í≤ΩÎßùÏóê ÏûÖÎ†•ÌïòÎäî ÌëúÎ≥∏Ïùò Ïàò\n",
        "- `VALIDATION`: ÌõàÎ†® ÌîÑÎ°úÏÑ∏Ïä§Ïùò Ïú†Ìö®ÏÑ± ÌôïÏù∏ÌïòÍ±∞ÎÇò Ï¶ùÎ™ÖÏùÑ ÏúÑÌï¥ ÎÇ®Í≤®Îëî Îç∞Ïù¥ÌÑ∞Ïùò Ïñë"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaNom_pPpaS-",
        "outputId": "ae70480e-e70c-4ae8-d359-fa133c253366"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ïã†Í≤ΩÎßùÍ≥º ÌõàÎ†® Îß§Í∞úÎ≥ÄÏàò\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10  # Ï∂úÎ†• Í∞úÏàò = Ïà´ÏûêÏùò Í∞úÏàò\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # Í≤ÄÏ¶ùÏùÑ ÏúÑÌï¥ ÎÇ®Í≤®Îëî ÌõàÎ†® Îç∞Ïù¥ÌÑ∞\n",
        "\n",
        "# MNIST Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "# Í≤ÄÏ¶ù\n",
        "# ÌõàÎ†®Í≥º ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÅÍ∞Å 60000Í∞úÏôÄ 10000Í∞úÎ°ú ÎÇòÎà¥Îã§.\n",
        "# Î†àÏù¥Î∏îÏóê ÎåÄÌïú ÏõêÌï´ Ïù∏ÏΩîÎî©ÏùÄ ÏûêÎèôÏúºÎ°ú Ï†ÅÏö©ÎêúÎã§.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_trainÏùÄ 60000Í∞ú ÌñâÏúºÎ°ú 28*28 Í∞íÏùÑ Í∞ÄÏßÑÎã§. -> 60000 * 784 ÌòïÌÉúÎ°ú Î≥ÄÌôò\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# ÏûÖÎ†•ÏùÑ [0, 1] ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# Î†àÏù¥Î∏îÏùÑ ÏõêÌï´ Ïù∏ÏΩîÎî©\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnMA9SbqpaS_"
      },
      "source": [
        "- ÏûÖÎ†• Í≥ÑÏ∏µÏóê Ïù¥ÎØ∏ÏßÄÏùò Í∞Å ÌîΩÏÖÄÍ≥º Ïó∞Í≤∞Îêú Îâ¥Îü∞Ïù¥ ÏûàÏúºÎ©∞, MNIST Ïù¥ÎØ∏ÏßÄÏùò Í∞Å ÌîΩÏÖÄÎßàÎã§ ÌïòÎÇòÏî© Ï¥ù 28 * 28 = 784Í∞úÏùò Îâ¥Îü∞Ïù¥ ÏûàÏùå\n",
        "- ÎåÄÍ∞ú Í∞Å ÌîΩÏÖÄÍ≥º Í¥ÄÎ†®Îêú Í∞íÏùÄ [0, 1] Î≤îÏúÑÏóêÏÑú Ï†ïÍ∑úÌôîÎêúÎã§.\n",
        "- Ï∂úÎ†•ÏùÄ 10ÏûêÎ¶¨ Î∂ÄÎ•ò(Class) Ï§ë ÌïòÎÇòÎ©∞, Í∞Å Ïà´ÏûêÎßàÎã§ ÌïòÎÇòÏùò Î∂ÄÎ•òÍ∞Ä ÏûàÎã§.\n",
        "- ÎßàÏßÄÎßâ Í≥ÑÏ∏µÏùÄ ÌôúÏÑ±Ìôî Ìï®ÏàòÍ∞Ä ÏÜåÌîÑÌä∏Îß•Ïä§(Softmax)Ïù∏ Îã®Ïùº Îâ¥Îü∞ÏúºÎ°ú, ÏãúÍ∑∏Î™®Ïù¥Îìú Ìï®ÏàòÎ•º ÏùºÎ∞òÌôîÌïú Í≤ÉÏù¥Îã§.\n",
        "    - ÏÜåÌîÑÌä∏Îß•Ïä§: ÏûÑÏùòÏùò Ïã§Ïàò Í∞íÏùò KÏ∞®Ïõê Î≤°ÌÑ∞Î•º (0, 1) Î≤îÏúÑÏùò Ïã§Ïàò Í∞íÏùÑ Í∞ÄÏßÑ KÏ∞®Ïõê Î≤°ÌÑ∞Î°ú Î∞ÄÏñ¥ ÎÑ£Ïñ¥ Ï¥ùÌï©Ïù¥ 1Ïù¥ ÎêòÍ≤å ÌïúÎã§."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9ZvA6tspaS_"
      },
      "source": [
        "# Î™®Îç∏ Íµ¨Ï∂ï\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(NB_CLASSES, input_shape=(RESHAPED,), name='dense_layer', activation='softmax'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG3dBJsKpaS_"
      },
      "source": [
        "- Î™®Îç∏ Ï†ïÏùò ÌõÑ TensorFlow 2.0ÏóêÏÑú Ïã§ÌñâÌï† Ïàò ÏûàÎèÑÎ°ù Î™®Îç∏ Ïª¥ÌååÏùº ÌïÑÏöî\n",
        "- Ïª¥ÌååÏùº Ï§ë ÏÑ§Ï†ï ÏÇ¨Ìï≠\n",
        "    1. ÏµúÏ†ÅÌôîÍ∏∞(optimizer) ÏÑ†ÌÉù\n",
        "        - optimizer: Î™®Îç∏ÏùÑ ÌõàÎ†®ÏãúÌÇ§Îäî ÎèôÏïà Í∞ÄÏ§ëÏπòÎ•º ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÎäî ÌäπÏ†ï ÏïåÍ≥†Î¶¨Ï¶ò\n",
        "    2. Î™©Ï†Å Ìï®Ïàò(objective function) ÏÑ†ÌÉù\n",
        "        - objective function: optimizerÍ∞Ä Í∞ÄÏ§ëÏπò Í≥µÍ∞ÑÏùÑ ÌÉêÏÉâÌïòÎäî Îç∞Ïóê ÏÇ¨Ïö©\n",
        "        - = ÏÜêÏã§ Ìï®Ïàò(loss function)Ïù¥ÎÇò ÎπÑÏö© Ìï®Ïàò(cost function)\n",
        "        - ÏµúÏ†ÅÌôî ÌîÑÎ°úÏÑ∏Îäî ÏÜêÏã§ ÏµúÏÜåÌôî ÌîÑÎ°úÏÑ∏Ïä§Î°ú Ï†ïÏùò\n",
        "        - MSE\n",
        "            - ÏòàÏ∏°Í≥º Ïã§Ï†ú Í∞í ÏÇ¨Ïù¥Ïùò ÌèâÍ∑† Ï†úÍ≥± Ïò§Ï∞®\n",
        "            - MùëÜùê∏= (1/n) * ‚àë(i=1,n)(ùëë‚àíùë¶)^2, d: ÏòàÏ∏° Î≤°ÌÑ∞, y: n Í¥ÄÏ∏°ÏπòÏùò Î≤°ÌÑ∞\n",
        "            - Í∞Å ÏòàÏ∏°ÏóêÏÑú Î∞úÏÉùÌïú Î™®Îì† Ïò§Ï∞®Ïùò ÌèâÍ∑†\n",
        "            - ÏòàÏ∏°Ïù¥ Ïã§Ï†ú Í∞íÍ≥º Î©ÄÏàòÎ°ù Ï†úÍ≥± Ïó∞ÏÇ¨Ïóê ÏùòÌï¥ ÎçîÏö± ÎöúÎ†∑Ìï¥ÏßÑÎã§.\n",
        "            - Ï†úÍ≥±ÏùÑ ÌÜµÌï¥ Ïò§Ï∞®Í∞Ä ÏñëÏàòÏù¥Îì† ÏùåÏàòÏù¥Îì† ÎàÑÏ†Å Í∞í Ï¶ùÍ∞Ä\n",
        "        - binary_crossentropy\n",
        "            - Î°úÍ∑∏ ÏÜêÏã§\n",
        "            - Î™©ÌëúÍ∞Ä cÏùº Îïå Î™®Îç∏Ïù¥ pÎ°ú ÏòàÏ∏°Ìïú Í≤ΩÏö∞: ÍµêÏ∞® ÏóîÌä∏Î°úÌîº(cross-entropy) L(p, c) = -c * ln(p)-(1-c)ln(1-p)\n",
        "            - Ïù¥ÏßÑ Î†àÏù¥Î∏î ÏòàÏ∏°Ïóê Ï†ÅÏ†à\n",
        "        - categorical_crossentropy\n",
        "            - Îã§Î∂ÄÎ•ò(multiclass) Î°úÍ∑∏ ÏÜêÏã§\n",
        "            - ÏòàÏ∏° Î∂ÑÌè¨Î•º Ï∞∏ Î∂ÑÌè¨ÏôÄ ÎπÑÍµê -> Ï∞∏ Î∂ÄÎ•òÏóê ÎåÄÌïú ÌôïÎ•†=1Î°ú ÏÑ§Ï†ï, ÎÇòÎ®∏ÏßÄÎäî 0ÏúºÎ°ú ÏÑ§Ï†ï -> ÏõêÌï´ Ïù∏ÏΩîÎî©\n",
        "            - Ï∞∏ Î∂ÄÎ•òÍ∞Ä cÏù∏Îç∞ yÎ°ú ÏòàÏ∏°ÌñàÎã§Î©¥, L(c, p) = -‚àë(i) ci * ln(pi)\n",
        "            - Ï∂úÎ†•Ïù¥ Ï∞∏ Î≤°ÌÑ∞Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù ÏÜêÏã§ ‚Üì\n",
        "            - Îã§Î∂ÄÎ•ò Î†àÏù¥Î∏î ÏòàÏ∏°Ïóê Ï†ÅÌï©\n",
        "    3. ÌõàÎ†®Îêú Î™®Îç∏ÏùÑ ÌèâÍ∞Ä by Ï≤ôÎèÑ(metrics)\n",
        "        - Ï†ïÌôïÎèÑ(Accuracy): ÌÉÄÍπÉ ÎåÄÎπÑ Ï†ïÌôïÌûà ÏòàÏ∏°Ìïú ÎπÑÏú®\n",
        "        - Ï†ïÎ∞ÄÎèÑ(Precision): positiveÏúºÎ°ú ÏòàÏ∏°Ìïú Í≤É Ï§ë Ïã§Ï†úÎ°ú Ï∞∏Ïù∏ Í≤ÉÏùò ÎπÑÏú®\n",
        "        - Ïû¨ÌòÑÏú®(Recall): Ïò¨Î∞îÎ•¥Í≤å ÏòàÏ∏°Ìïú Í≤É(Ï∞∏ÏùÄ positive, Í±∞ÏßìÏùÄ negativeÏúºÎ°ú ÏòàÏ∏°) Ï§ë positiveÏúºÎ°ú ÏòàÏ∏°Ìïú Í≤ÉÏù¥ Ïã§Ï†úÎ°ú Ï∞∏Ïù∏ ÎπÑÏú®\n",
        "        - Î™®Îç∏ ÌèâÍ∞ÄÏóêÎßå ÏÇ¨Ïö©, Ïã†Í≤ΩÎßùÏùò ÏÑ±Îä• ÌåêÎã®"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaY04XMXpaTA"
      },
      "source": [
        "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bAFTBCZpaTA"
      },
      "source": [
        "- ÌôïÎ•†Ï†Å Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌïòÍ∞ï(SGD, Stochastic Gradient Descent)\n",
        "    - ÏµúÏ†ÅÌôî ÏïåÍ≥†Î¶¨Ï¶òÏùò ÌäπÎ≥ÑÌïú Ï¢ÖÎ•ò\n",
        "    - Í∞Å ÌõàÎ†® ÏóêÌè≠(epoch)ÎßàÎã§ Ïã†Í≤ΩÎßùÏùò Ïò§Ï∞®Î•º Ï§ÑÏù¥Í≥†Ïûê ÏÇ¨Ïö©\n",
        "- Î™®Îç∏ Ïª¥ÌååÏùº ÌõÑ `fit()` Î©îÏÜåÎìúÎ°ú ÌõàÎ†®Ìï† Ïàò ÏûàÍ≥†, Ïù¥Îïå Îß§Í∞úÎ≥ÄÏàò Î™ÖÏãú Í∞ÄÎä•\n",
        "    - `epochs`\n",
        "        - Î™®Îç∏Ïù¥ ÌõàÎ†® ÏßëÌï©Ïóê ÎÖ∏Ï∂úÎêú ÌöüÏàò\n",
        "        - Í∞Å Î∞òÎ≥µÏóêÏÑú optimizerÎäî Î™©Ìëú Ìï®ÏàòÍ∞Ä ÏµúÏÜåÍ∞Ä ÎêòÎèÑÎ°ù Í∞ÄÏ§ëÏπòÎ•º Ï°∞Ï†ï\n",
        "    - `batch_size`\n",
        "        - optimizerÍ∞Ä Í∞ÄÏ§ëÏπò Í∞±Ïã†ÏùÑ ÏàòÌñâÌïòÍ∏∞ Ï†ÑÏóê Í¥ÄÏ∞∞Ìïú ÌõàÎ†® Ïù∏Ïä§ÌÑ¥Ïä§Ïùò Ïàò\n",
        "        - Ìïú ÏóêÌè≠Îãπ Ïó¨Îü¨ Î∞∞ÏπòÍ∞Ä ÏûàÎã§."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YWnvVuIpaTA",
        "outputId": "dd6386d7-73a5-4fa9-fac4-84d3d2e6a099"
      },
      "source": [
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 1.3597 - accuracy: 0.6863 - val_loss: 0.8869 - val_accuracy: 0.8321\n",
            "Epoch 2/200\n",
            " 3840/48000 [=>............................] - ETA: 0s - loss: 0.8951 - accuracy: 0.8266"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.7853 - accuracy: 0.8351 - val_loss: 0.6525 - val_accuracy: 0.8603\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.6389 - accuracy: 0.8544 - val_loss: 0.5588 - val_accuracy: 0.8706\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5680 - accuracy: 0.8644 - val_loss: 0.5070 - val_accuracy: 0.8772\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5248 - accuracy: 0.8704 - val_loss: 0.4731 - val_accuracy: 0.8831\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4950 - accuracy: 0.8746 - val_loss: 0.4491 - val_accuracy: 0.8871\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4730 - accuracy: 0.8784 - val_loss: 0.4315 - val_accuracy: 0.8911\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4558 - accuracy: 0.8809 - val_loss: 0.4175 - val_accuracy: 0.8938\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4420 - accuracy: 0.8837 - val_loss: 0.4060 - val_accuracy: 0.8950\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4305 - accuracy: 0.8863 - val_loss: 0.3964 - val_accuracy: 0.8965\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4209 - accuracy: 0.8881 - val_loss: 0.3885 - val_accuracy: 0.8982\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4125 - accuracy: 0.8895 - val_loss: 0.3815 - val_accuracy: 0.8998\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4052 - accuracy: 0.8914 - val_loss: 0.3755 - val_accuracy: 0.9007\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3987 - accuracy: 0.8918 - val_loss: 0.3702 - val_accuracy: 0.9018\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3930 - accuracy: 0.8935 - val_loss: 0.3654 - val_accuracy: 0.9029\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3879 - accuracy: 0.8946 - val_loss: 0.3610 - val_accuracy: 0.9038\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3832 - accuracy: 0.8956 - val_loss: 0.3573 - val_accuracy: 0.9050\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3789 - accuracy: 0.8963 - val_loss: 0.3537 - val_accuracy: 0.9057\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3750 - accuracy: 0.8974 - val_loss: 0.3506 - val_accuracy: 0.9061\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3714 - accuracy: 0.8981 - val_loss: 0.3476 - val_accuracy: 0.9066\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3681 - accuracy: 0.8987 - val_loss: 0.3449 - val_accuracy: 0.9068\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3650 - accuracy: 0.8996 - val_loss: 0.3423 - val_accuracy: 0.9080\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3621 - accuracy: 0.9001 - val_loss: 0.3400 - val_accuracy: 0.9080\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3594 - accuracy: 0.9009 - val_loss: 0.3378 - val_accuracy: 0.9089\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3568 - accuracy: 0.9012 - val_loss: 0.3359 - val_accuracy: 0.9087\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3545 - accuracy: 0.9019 - val_loss: 0.3337 - val_accuracy: 0.9096\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3522 - accuracy: 0.9024 - val_loss: 0.3320 - val_accuracy: 0.9098\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3501 - accuracy: 0.9031 - val_loss: 0.3303 - val_accuracy: 0.9103\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3481 - accuracy: 0.9036 - val_loss: 0.3285 - val_accuracy: 0.9103\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3462 - accuracy: 0.9040 - val_loss: 0.3270 - val_accuracy: 0.9118\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3444 - accuracy: 0.9043 - val_loss: 0.3256 - val_accuracy: 0.9114\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3426 - accuracy: 0.9052 - val_loss: 0.3243 - val_accuracy: 0.9115\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3409 - accuracy: 0.9054 - val_loss: 0.3229 - val_accuracy: 0.9120\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3394 - accuracy: 0.9060 - val_loss: 0.3216 - val_accuracy: 0.9127\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3379 - accuracy: 0.9064 - val_loss: 0.3204 - val_accuracy: 0.9130\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3364 - accuracy: 0.9068 - val_loss: 0.3193 - val_accuracy: 0.9128\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3350 - accuracy: 0.9070 - val_loss: 0.3183 - val_accuracy: 0.9132\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3337 - accuracy: 0.9074 - val_loss: 0.3170 - val_accuracy: 0.9140\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3324 - accuracy: 0.9079 - val_loss: 0.3161 - val_accuracy: 0.9136\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3312 - accuracy: 0.9083 - val_loss: 0.3151 - val_accuracy: 0.9126\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3300 - accuracy: 0.9086 - val_loss: 0.3142 - val_accuracy: 0.9135\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3289 - accuracy: 0.9090 - val_loss: 0.3132 - val_accuracy: 0.9141\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3277 - accuracy: 0.9093 - val_loss: 0.3124 - val_accuracy: 0.9150\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3267 - accuracy: 0.9097 - val_loss: 0.3115 - val_accuracy: 0.9147\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3256 - accuracy: 0.9097 - val_loss: 0.3107 - val_accuracy: 0.9149\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3246 - accuracy: 0.9102 - val_loss: 0.3100 - val_accuracy: 0.9152\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3237 - accuracy: 0.9106 - val_loss: 0.3092 - val_accuracy: 0.9149\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3227 - accuracy: 0.9103 - val_loss: 0.3084 - val_accuracy: 0.9155\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3218 - accuracy: 0.9110 - val_loss: 0.3078 - val_accuracy: 0.9152\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3209 - accuracy: 0.9113 - val_loss: 0.3071 - val_accuracy: 0.9155\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3201 - accuracy: 0.9115 - val_loss: 0.3064 - val_accuracy: 0.9156\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3192 - accuracy: 0.9115 - val_loss: 0.3057 - val_accuracy: 0.9157\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3184 - accuracy: 0.9121 - val_loss: 0.3051 - val_accuracy: 0.9157\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3176 - accuracy: 0.9121 - val_loss: 0.3044 - val_accuracy: 0.9158\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3169 - accuracy: 0.9126 - val_loss: 0.3039 - val_accuracy: 0.9156\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3161 - accuracy: 0.9124 - val_loss: 0.3034 - val_accuracy: 0.9159\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3154 - accuracy: 0.9129 - val_loss: 0.3029 - val_accuracy: 0.9158\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3147 - accuracy: 0.9131 - val_loss: 0.3022 - val_accuracy: 0.9160\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3140 - accuracy: 0.9130 - val_loss: 0.3017 - val_accuracy: 0.9161\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3133 - accuracy: 0.9131 - val_loss: 0.3011 - val_accuracy: 0.9164\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3127 - accuracy: 0.9134 - val_loss: 0.3007 - val_accuracy: 0.9162\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3120 - accuracy: 0.9134 - val_loss: 0.3002 - val_accuracy: 0.9164\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3114 - accuracy: 0.9138 - val_loss: 0.2998 - val_accuracy: 0.9163\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3108 - accuracy: 0.9140 - val_loss: 0.2993 - val_accuracy: 0.9167\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3102 - accuracy: 0.9141 - val_loss: 0.2988 - val_accuracy: 0.9168\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3096 - accuracy: 0.9144 - val_loss: 0.2985 - val_accuracy: 0.9169\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3090 - accuracy: 0.9144 - val_loss: 0.2979 - val_accuracy: 0.9174\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3084 - accuracy: 0.9146 - val_loss: 0.2974 - val_accuracy: 0.9172\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3079 - accuracy: 0.9145 - val_loss: 0.2972 - val_accuracy: 0.9172\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3074 - accuracy: 0.9149 - val_loss: 0.2968 - val_accuracy: 0.9176\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3068 - accuracy: 0.9150 - val_loss: 0.2963 - val_accuracy: 0.9178\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3063 - accuracy: 0.9148 - val_loss: 0.2960 - val_accuracy: 0.9183\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3058 - accuracy: 0.9150 - val_loss: 0.2955 - val_accuracy: 0.9177\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3053 - accuracy: 0.9152 - val_loss: 0.2952 - val_accuracy: 0.9183\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3049 - accuracy: 0.9153 - val_loss: 0.2948 - val_accuracy: 0.9181\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3044 - accuracy: 0.9153 - val_loss: 0.2945 - val_accuracy: 0.9183\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3039 - accuracy: 0.9159 - val_loss: 0.2941 - val_accuracy: 0.9184\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3034 - accuracy: 0.9157 - val_loss: 0.2938 - val_accuracy: 0.9185\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3029 - accuracy: 0.9157 - val_loss: 0.2935 - val_accuracy: 0.9183\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3025 - accuracy: 0.9159 - val_loss: 0.2931 - val_accuracy: 0.9180\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3021 - accuracy: 0.9159 - val_loss: 0.2929 - val_accuracy: 0.9187\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3017 - accuracy: 0.9160 - val_loss: 0.2925 - val_accuracy: 0.9184\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3013 - accuracy: 0.9164 - val_loss: 0.2923 - val_accuracy: 0.9182\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3009 - accuracy: 0.9161 - val_loss: 0.2919 - val_accuracy: 0.9186\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3004 - accuracy: 0.9166 - val_loss: 0.2917 - val_accuracy: 0.9181\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3000 - accuracy: 0.9163 - val_loss: 0.2915 - val_accuracy: 0.9189\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2996 - accuracy: 0.9164 - val_loss: 0.2911 - val_accuracy: 0.9192\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2993 - accuracy: 0.9168 - val_loss: 0.2908 - val_accuracy: 0.9191\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2989 - accuracy: 0.9169 - val_loss: 0.2905 - val_accuracy: 0.9193\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2985 - accuracy: 0.9170 - val_loss: 0.2902 - val_accuracy: 0.9194\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2981 - accuracy: 0.9171 - val_loss: 0.2901 - val_accuracy: 0.9197\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2978 - accuracy: 0.9169 - val_loss: 0.2897 - val_accuracy: 0.9192\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2974 - accuracy: 0.9173 - val_loss: 0.2895 - val_accuracy: 0.9196\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2971 - accuracy: 0.9171 - val_loss: 0.2891 - val_accuracy: 0.9193\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2967 - accuracy: 0.9174 - val_loss: 0.2890 - val_accuracy: 0.9196\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2964 - accuracy: 0.9174 - val_loss: 0.2887 - val_accuracy: 0.9197\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2961 - accuracy: 0.9177 - val_loss: 0.2885 - val_accuracy: 0.9195\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2957 - accuracy: 0.9176 - val_loss: 0.2882 - val_accuracy: 0.9203\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2954 - accuracy: 0.9175 - val_loss: 0.2880 - val_accuracy: 0.9200\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2951 - accuracy: 0.9179 - val_loss: 0.2878 - val_accuracy: 0.9202\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2948 - accuracy: 0.9181 - val_loss: 0.2876 - val_accuracy: 0.9206\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2944 - accuracy: 0.9179 - val_loss: 0.2873 - val_accuracy: 0.9202\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2941 - accuracy: 0.9179 - val_loss: 0.2871 - val_accuracy: 0.9203\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2939 - accuracy: 0.9185 - val_loss: 0.2869 - val_accuracy: 0.9210\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2936 - accuracy: 0.9185 - val_loss: 0.2867 - val_accuracy: 0.9206\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2933 - accuracy: 0.9185 - val_loss: 0.2865 - val_accuracy: 0.9208\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2930 - accuracy: 0.9185 - val_loss: 0.2863 - val_accuracy: 0.9207\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2926 - accuracy: 0.9184 - val_loss: 0.2862 - val_accuracy: 0.9203\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2924 - accuracy: 0.9189 - val_loss: 0.2859 - val_accuracy: 0.9206\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2921 - accuracy: 0.9189 - val_loss: 0.2857 - val_accuracy: 0.9208\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2919 - accuracy: 0.9189 - val_loss: 0.2855 - val_accuracy: 0.9204\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2916 - accuracy: 0.9191 - val_loss: 0.2854 - val_accuracy: 0.9211\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2913 - accuracy: 0.9187 - val_loss: 0.2851 - val_accuracy: 0.9205\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2911 - accuracy: 0.9191 - val_loss: 0.2849 - val_accuracy: 0.9208\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2908 - accuracy: 0.9193 - val_loss: 0.2847 - val_accuracy: 0.9209\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2905 - accuracy: 0.9193 - val_loss: 0.2846 - val_accuracy: 0.9209\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2902 - accuracy: 0.9192 - val_loss: 0.2845 - val_accuracy: 0.9208\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2900 - accuracy: 0.9193 - val_loss: 0.2843 - val_accuracy: 0.9210\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2898 - accuracy: 0.9193 - val_loss: 0.2841 - val_accuracy: 0.9210\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2895 - accuracy: 0.9194 - val_loss: 0.2840 - val_accuracy: 0.9210\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2893 - accuracy: 0.9192 - val_loss: 0.2838 - val_accuracy: 0.9207\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2890 - accuracy: 0.9196 - val_loss: 0.2837 - val_accuracy: 0.9212\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2888 - accuracy: 0.9196 - val_loss: 0.2835 - val_accuracy: 0.9212\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2886 - accuracy: 0.9194 - val_loss: 0.2833 - val_accuracy: 0.9212\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2883 - accuracy: 0.9197 - val_loss: 0.2832 - val_accuracy: 0.9211\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2881 - accuracy: 0.9202 - val_loss: 0.2831 - val_accuracy: 0.9204\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2878 - accuracy: 0.9196 - val_loss: 0.2829 - val_accuracy: 0.9212\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2876 - accuracy: 0.9199 - val_loss: 0.2827 - val_accuracy: 0.9212\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2874 - accuracy: 0.9196 - val_loss: 0.2826 - val_accuracy: 0.9212\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2872 - accuracy: 0.9197 - val_loss: 0.2824 - val_accuracy: 0.9211\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2870 - accuracy: 0.9201 - val_loss: 0.2823 - val_accuracy: 0.9213\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2868 - accuracy: 0.9200 - val_loss: 0.2821 - val_accuracy: 0.9215\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2866 - accuracy: 0.9200 - val_loss: 0.2819 - val_accuracy: 0.9212\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2863 - accuracy: 0.9201 - val_loss: 0.2818 - val_accuracy: 0.9216\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2861 - accuracy: 0.9202 - val_loss: 0.2818 - val_accuracy: 0.9208\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2859 - accuracy: 0.9203 - val_loss: 0.2816 - val_accuracy: 0.9217\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2857 - accuracy: 0.9201 - val_loss: 0.2815 - val_accuracy: 0.9218\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2855 - accuracy: 0.9203 - val_loss: 0.2813 - val_accuracy: 0.9213\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2853 - accuracy: 0.9203 - val_loss: 0.2811 - val_accuracy: 0.9211\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2851 - accuracy: 0.9204 - val_loss: 0.2810 - val_accuracy: 0.9212\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2849 - accuracy: 0.9203 - val_loss: 0.2809 - val_accuracy: 0.9221\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2847 - accuracy: 0.9204 - val_loss: 0.2808 - val_accuracy: 0.9212\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2845 - accuracy: 0.9204 - val_loss: 0.2807 - val_accuracy: 0.9219\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2844 - accuracy: 0.9203 - val_loss: 0.2805 - val_accuracy: 0.9218\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2842 - accuracy: 0.9207 - val_loss: 0.2804 - val_accuracy: 0.9220\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2840 - accuracy: 0.9206 - val_loss: 0.2803 - val_accuracy: 0.9220\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2838 - accuracy: 0.9207 - val_loss: 0.2802 - val_accuracy: 0.9223\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2836 - accuracy: 0.9206 - val_loss: 0.2801 - val_accuracy: 0.9221\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2834 - accuracy: 0.9209 - val_loss: 0.2800 - val_accuracy: 0.9217\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2832 - accuracy: 0.9206 - val_loss: 0.2798 - val_accuracy: 0.9218\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2831 - accuracy: 0.9208 - val_loss: 0.2797 - val_accuracy: 0.9222\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2829 - accuracy: 0.9209 - val_loss: 0.2797 - val_accuracy: 0.9222\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2827 - accuracy: 0.9208 - val_loss: 0.2795 - val_accuracy: 0.9220\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2825 - accuracy: 0.9210 - val_loss: 0.2794 - val_accuracy: 0.9223\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2824 - accuracy: 0.9211 - val_loss: 0.2792 - val_accuracy: 0.9222\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2822 - accuracy: 0.9211 - val_loss: 0.2791 - val_accuracy: 0.9224\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2820 - accuracy: 0.9212 - val_loss: 0.2790 - val_accuracy: 0.9227\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2819 - accuracy: 0.9214 - val_loss: 0.2789 - val_accuracy: 0.9224\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2817 - accuracy: 0.9214 - val_loss: 0.2790 - val_accuracy: 0.9220\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2815 - accuracy: 0.9212 - val_loss: 0.2788 - val_accuracy: 0.9222\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2814 - accuracy: 0.9213 - val_loss: 0.2787 - val_accuracy: 0.9221\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2812 - accuracy: 0.9215 - val_loss: 0.2787 - val_accuracy: 0.9227\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2811 - accuracy: 0.9213 - val_loss: 0.2784 - val_accuracy: 0.9225\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2809 - accuracy: 0.9215 - val_loss: 0.2783 - val_accuracy: 0.9224\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2807 - accuracy: 0.9216 - val_loss: 0.2783 - val_accuracy: 0.9224\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2805 - accuracy: 0.9216 - val_loss: 0.2784 - val_accuracy: 0.9229\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2805 - accuracy: 0.9217 - val_loss: 0.2781 - val_accuracy: 0.9221\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2803 - accuracy: 0.9216 - val_loss: 0.2780 - val_accuracy: 0.9229\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2801 - accuracy: 0.9216 - val_loss: 0.2779 - val_accuracy: 0.9226\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2800 - accuracy: 0.9217 - val_loss: 0.2779 - val_accuracy: 0.9227\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2798 - accuracy: 0.9217 - val_loss: 0.2777 - val_accuracy: 0.9225\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2797 - accuracy: 0.9217 - val_loss: 0.2777 - val_accuracy: 0.9228\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2795 - accuracy: 0.9218 - val_loss: 0.2775 - val_accuracy: 0.9227\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2794 - accuracy: 0.9218 - val_loss: 0.2775 - val_accuracy: 0.9225\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2792 - accuracy: 0.9220 - val_loss: 0.2773 - val_accuracy: 0.9225\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2791 - accuracy: 0.9219 - val_loss: 0.2773 - val_accuracy: 0.9224\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2790 - accuracy: 0.9221 - val_loss: 0.2772 - val_accuracy: 0.9225\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2788 - accuracy: 0.9221 - val_loss: 0.2771 - val_accuracy: 0.9230\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2787 - accuracy: 0.9218 - val_loss: 0.2770 - val_accuracy: 0.9232\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2785 - accuracy: 0.9219 - val_loss: 0.2769 - val_accuracy: 0.9232\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2784 - accuracy: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.9227\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2782 - accuracy: 0.9222 - val_loss: 0.2768 - val_accuracy: 0.9227\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2781 - accuracy: 0.9224 - val_loss: 0.2767 - val_accuracy: 0.9224\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2780 - accuracy: 0.9221 - val_loss: 0.2766 - val_accuracy: 0.9227\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2778 - accuracy: 0.9227 - val_loss: 0.2766 - val_accuracy: 0.9231\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2777 - accuracy: 0.9224 - val_loss: 0.2765 - val_accuracy: 0.9226\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2776 - accuracy: 0.9224 - val_loss: 0.2763 - val_accuracy: 0.9232\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2774 - accuracy: 0.9223 - val_loss: 0.2763 - val_accuracy: 0.9226\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2773 - accuracy: 0.9224 - val_loss: 0.2762 - val_accuracy: 0.9229\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2772 - accuracy: 0.9226 - val_loss: 0.2762 - val_accuracy: 0.9235\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2770 - accuracy: 0.9225 - val_loss: 0.2760 - val_accuracy: 0.9235\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2769 - accuracy: 0.9225 - val_loss: 0.2760 - val_accuracy: 0.9234\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2768 - accuracy: 0.9228 - val_loss: 0.2759 - val_accuracy: 0.9232\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2767 - accuracy: 0.9229 - val_loss: 0.2758 - val_accuracy: 0.9234\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2765 - accuracy: 0.9227 - val_loss: 0.2758 - val_accuracy: 0.9237\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2764 - accuracy: 0.9229 - val_loss: 0.2757 - val_accuracy: 0.9242\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2763 - accuracy: 0.9227 - val_loss: 0.2756 - val_accuracy: 0.9236\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2762 - accuracy: 0.9229 - val_loss: 0.2755 - val_accuracy: 0.9232\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2761 - accuracy: 0.9228 - val_loss: 0.2755 - val_accuracy: 0.9233\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2759 - accuracy: 0.9229 - val_loss: 0.2754 - val_accuracy: 0.9237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9b234b0c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HVBmjiYpaTB"
      },
      "source": [
        "- ÌõàÎ†®ÌïòÎäî ÎèôÏïà Ïú†Ìö®ÏÑ± ÏÑ±Îä•ÏùÑ Ï∏°Ï†ïÌïòÍ≥†Ïûê ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïùò ÏùºÎ∂ÄÎ•º ÎÇ®Í≤®ÎëîÎã§.\n",
        "- Î™®Îç∏ ÌõàÎ†® ÌõÑ ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑú Î™®Îç∏Ïù¥ Ìïú Î≤àÎèÑ Î≥∏ Ï†Å ÏóÜÎäî ÏÉàÎ°úÏö¥ ÏòàÏãúÏùò ÌÖåÏä§Ìä∏ ÏßëÌï©ÏúºÎ°ú ÌèâÍ∞Ä\n",
        "- `evaluate(X_test, Y_test)`Î°ú `test_loss`ÏôÄ `test_acc` Í≥ÑÏÇ∞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxNnkjd8paTB",
        "outputId": "8e288db0-c804-4e2c-adbf-703a969545f7"
      },
      "source": [
        "# Î™®Îç∏ ÌèâÍ∞Ä\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUv8k5RzpaTB"
      },
      "source": [
        "### Îã®Ïàú TensorFlow 2.0 Ïã†Í≤ΩÎßù Ïã§ÌñâÍ≥º Î≤†Ïù¥Ïä§ÎùºÏù∏ Íµ¨Ï∂ï\n",
        "- Ïã†Í≤ΩÎßùÏùò ÏïÑÌÇ§ÌÖçÏ≤òÍ∞Ä Ï∂úÎ†•ÎêòÍ≥†, ÏÇ¨Ïö©Îêú Ïó¨Îü¨ Í≥ÑÏ∏µÏùò Ïú†Ìòï, Ï∂úÎ†• ÌòïÌÉú, ÏµúÏ†ÅÌôîÌï¥Ïïº Ìï† Îß§Í∞úÎ≥ÄÏàò Í∞úÏàò(= Í∞ÄÏ§ëÏπò Ïàò)ÏôÄ Ïó∞Í≤∞ Î∞©Ïãù ÌôïÏù∏\n",
        "- Ïã†Í≤ΩÎßùÏùÄ 48000Í∞úÏùò ÌëúÎ≥∏ÏúºÎ°ú ÌõàÎ†®, 12000Í∞úÏùò ÌëúÎ≥∏ÏùÄ Í≤ÄÏ¶ùÏùÑ ÏúÑÌï¥ ÏÇ¨Ïö©Îê®\n",
        "- ÌõàÎ†® ÌõÑÏóê ÌÖåÏä§Ìä∏ ÏßëÌï©ÏóêÏÑú Î™®Îç∏ÏùÑ ÌÖåÏä§Ìä∏\n",
        "\n",
        "### TensorFlow 2.0Ïùò Îã®Ïàú Ïã†Í≤ΩÎßùÏùÑ ÏùÄÎãâÏ∏µÏúºÎ°ú Í∞úÏÑ†\n",
        "- Í∞úÏÑ†Î≤ï: Ïã†Í≤ΩÎßùÏóê Í≥ÑÏ∏µ Ï∂îÍ∞Ä\n",
        "    - ‚àµ Ï∂îÍ∞Ä Îâ¥Îü∞ÏùÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ï¢Ä Îçî Î≥µÏû°Ìïú Ìå®ÌÑ¥ÏùÑ ÌïôÏäµÌïòÎäî Îç∞ ÎèÑÏõÄ\n",
        "    - -> Í≥ÑÏ∏µ Ï∂îÍ∞ÄÎ°ú Îß§Í∞úÎ≥ÄÏàòÍ∞Ä Ï∂îÍ∞ÄÎèº Î™®Îç∏Ïù¥ Îçî Î≥µÏû°Ìïú Ìå®ÌÑ¥ÏùÑ Í∏∞ÏñµÌï† Ïàò ÏûàÍ≤å ÎêúÎã§.\n",
        "    - -> Î≥ÄÍ≤Ω: ÏûÖÎ†• Í≥ÑÏ∏µ -> N_HIDDEN Îâ¥Îü∞Í≥º ÌôúÏÑ±Ìôî Ìï®Ïàò ReLUÎ°ú Ï≤´ Î≤àÏß∏ Î∞ÄÏßë Í≥ÑÏ∏µ Ï∂îÍ∞Ä(= hidden) -> N_HIDDEN Îâ¥Îü∞ÏùÑ Í∞ÄÏßÑ 2Î≤àÏß∏ ÏùÄÎãâÏ∏µ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzMaFY9LpaTB",
        "outputId": "69ce5e08-63ad-48c7-df12-91832003623b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ïã†Í≤ΩÎßùÍ≥º ÌõàÎ†®\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10 # Ï∂úÎ†• Í∞úÏàò = Ïà´Ïûê Í∞úÏàò\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # Í≤ÄÏ¶ùÏóê ÎÇ®Í≤®Îëò ÌõàÎ†® ÏßëÌï© Î∂ÄÎ∂Ñ\n",
        "\n",
        "# MNIST Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "# Î†àÏù¥Î∏îÏùÄ ÏõêÌï´ ÌëúÍ∏∞Î°ú ÎêòÏñ¥ ÏûàÎã§.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_trainÏùÄ 60000Í∞ú ÌñâÏùò 28*28 Í∞í -> 60000*784 ÌòïÌÉúÎ°ú Î≥ÄÍ≤Ω\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# ÏûÖÎ†•ÏùÑ [0, 1] ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Î™®Îç∏ Íµ¨Ï∂ï\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# Î™®Îç∏ ÏöîÏïΩ\n",
        "model.summary()\n",
        "\n",
        "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/50\n",
            "48000/48000 [==============================] - 1s 29us/sample - loss: 1.4358 - accuracy: 0.6379 - val_loss: 0.7136 - val_accuracy: 0.8407\n",
            "Epoch 2/50\n",
            "  128/48000 [..............................] - ETA: 1s - loss: 0.8927 - accuracy: 0.7656"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.5795 - accuracy: 0.8541 - val_loss: 0.4430 - val_accuracy: 0.8852\n",
            "Epoch 3/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.4318 - accuracy: 0.8834 - val_loss: 0.3700 - val_accuracy: 0.8986\n",
            "Epoch 4/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3765 - accuracy: 0.8949 - val_loss: 0.3373 - val_accuracy: 0.9047\n",
            "Epoch 5/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3449 - accuracy: 0.9024 - val_loss: 0.3124 - val_accuracy: 0.9112\n",
            "Epoch 6/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3231 - accuracy: 0.9084 - val_loss: 0.2969 - val_accuracy: 0.9153\n",
            "Epoch 7/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3062 - accuracy: 0.9132 - val_loss: 0.2834 - val_accuracy: 0.9198\n",
            "Epoch 8/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2924 - accuracy: 0.9172 - val_loss: 0.2714 - val_accuracy: 0.9237\n",
            "Epoch 9/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2805 - accuracy: 0.9205 - val_loss: 0.2621 - val_accuracy: 0.9248\n",
            "Epoch 10/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2695 - accuracy: 0.9236 - val_loss: 0.2560 - val_accuracy: 0.9290\n",
            "Epoch 11/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2598 - accuracy: 0.9271 - val_loss: 0.2473 - val_accuracy: 0.9308\n",
            "Epoch 12/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2510 - accuracy: 0.9289 - val_loss: 0.2388 - val_accuracy: 0.9312\n",
            "Epoch 13/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2428 - accuracy: 0.9310 - val_loss: 0.2319 - val_accuracy: 0.9349\n",
            "Epoch 14/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2350 - accuracy: 0.9334 - val_loss: 0.2271 - val_accuracy: 0.9350\n",
            "Epoch 15/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2275 - accuracy: 0.9351 - val_loss: 0.2200 - val_accuracy: 0.9384\n",
            "Epoch 16/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2207 - accuracy: 0.9373 - val_loss: 0.2143 - val_accuracy: 0.9396\n",
            "Epoch 17/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2143 - accuracy: 0.9396 - val_loss: 0.2080 - val_accuracy: 0.9414\n",
            "Epoch 18/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2078 - accuracy: 0.9412 - val_loss: 0.2032 - val_accuracy: 0.9428\n",
            "Epoch 19/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2018 - accuracy: 0.9433 - val_loss: 0.1980 - val_accuracy: 0.9451\n",
            "Epoch 20/50\n",
            "48000/48000 [==============================] - 1s 29us/sample - loss: 0.1961 - accuracy: 0.9445 - val_loss: 0.1941 - val_accuracy: 0.9454\n",
            "Epoch 21/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1907 - accuracy: 0.9467 - val_loss: 0.1890 - val_accuracy: 0.9473\n",
            "Epoch 22/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1855 - accuracy: 0.9477 - val_loss: 0.1845 - val_accuracy: 0.9485\n",
            "Epoch 23/50\n",
            "48000/48000 [==============================] - 1s 30us/sample - loss: 0.1805 - accuracy: 0.9487 - val_loss: 0.1808 - val_accuracy: 0.9503\n",
            "Epoch 24/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1758 - accuracy: 0.9501 - val_loss: 0.1777 - val_accuracy: 0.9507\n",
            "Epoch 25/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1714 - accuracy: 0.9519 - val_loss: 0.1733 - val_accuracy: 0.9531\n",
            "Epoch 26/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1673 - accuracy: 0.9521 - val_loss: 0.1713 - val_accuracy: 0.9522\n",
            "Epoch 27/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1630 - accuracy: 0.9538 - val_loss: 0.1683 - val_accuracy: 0.9532\n",
            "Epoch 28/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1591 - accuracy: 0.9547 - val_loss: 0.1642 - val_accuracy: 0.9553\n",
            "Epoch 29/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1555 - accuracy: 0.9558 - val_loss: 0.1612 - val_accuracy: 0.9565\n",
            "Epoch 30/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1518 - accuracy: 0.9566 - val_loss: 0.1583 - val_accuracy: 0.9575\n",
            "Epoch 31/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1485 - accuracy: 0.9578 - val_loss: 0.1559 - val_accuracy: 0.9575\n",
            "Epoch 32/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1448 - accuracy: 0.9591 - val_loss: 0.1537 - val_accuracy: 0.9585\n",
            "Epoch 33/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1419 - accuracy: 0.9597 - val_loss: 0.1519 - val_accuracy: 0.9582\n",
            "Epoch 34/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1387 - accuracy: 0.9607 - val_loss: 0.1499 - val_accuracy: 0.9582\n",
            "Epoch 35/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1358 - accuracy: 0.9618 - val_loss: 0.1472 - val_accuracy: 0.9595\n",
            "Epoch 36/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1329 - accuracy: 0.9623 - val_loss: 0.1452 - val_accuracy: 0.9597\n",
            "Epoch 37/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1302 - accuracy: 0.9635 - val_loss: 0.1426 - val_accuracy: 0.9599\n",
            "Epoch 38/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1275 - accuracy: 0.9643 - val_loss: 0.1410 - val_accuracy: 0.9604\n",
            "Epoch 39/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1251 - accuracy: 0.9650 - val_loss: 0.1399 - val_accuracy: 0.9606\n",
            "Epoch 40/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1226 - accuracy: 0.9658 - val_loss: 0.1373 - val_accuracy: 0.9620\n",
            "Epoch 41/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1202 - accuracy: 0.9667 - val_loss: 0.1357 - val_accuracy: 0.9620\n",
            "Epoch 42/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1179 - accuracy: 0.9671 - val_loss: 0.1349 - val_accuracy: 0.9624\n",
            "Epoch 43/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1157 - accuracy: 0.9674 - val_loss: 0.1330 - val_accuracy: 0.9620\n",
            "Epoch 44/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1133 - accuracy: 0.9685 - val_loss: 0.1321 - val_accuracy: 0.9628\n",
            "Epoch 45/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1114 - accuracy: 0.9690 - val_loss: 0.1296 - val_accuracy: 0.9634\n",
            "Epoch 46/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1094 - accuracy: 0.9693 - val_loss: 0.1288 - val_accuracy: 0.9638\n",
            "Epoch 47/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1074 - accuracy: 0.9703 - val_loss: 0.1273 - val_accuracy: 0.9638\n",
            "Epoch 48/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1054 - accuracy: 0.9709 - val_loss: 0.1258 - val_accuracy: 0.9647\n",
            "Epoch 49/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1035 - accuracy: 0.9713 - val_loss: 0.1247 - val_accuracy: 0.9647\n",
            "Epoch 50/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1017 - accuracy: 0.9720 - val_loss: 0.1232 - val_accuracy: 0.9653\n",
            "Test accuracy:  0.9639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UiroRMMpaTC"
      },
      "source": [
        "- `to_categorical(Y_train, NB_CLASSES)`: Î∞∞Ïó¥ `Y_train`ÏùÑ Î∂ÄÎ•ò Í∞úÏàòÎßåÌÅº Ïó¥ÏùÑ Í∞ÄÏßÑ ÌñâÎ†¨Î°ú Î≥ÄÌôò\n",
        "- 1Î≤àÏß∏ ÌïôÏäµ vs. 2Î≤àÏß∏ ÌïôÏäµ(ÏùÄÎãâÏ∏µ 2Í∞ú Ï∂îÍ∞Ä): ÌÖåÏä§Ìä∏ Ï†ïÌôïÎèÑ ‚Üë, Î∞òÎ≥µ ÌöüÏàò 200 -> 50\n",
        "- **ÏàòÎ†¥ convergence**: ÏùºÏ†ï ÏóêÌè≠ÏùÑ ÎÑòÏñ¥ÏÑúÎ©¥ Í∞úÏÑ†Ïùò Ìö®Í≥ºÍ∞Ä ÎØ∏ÎØ∏Ìï®\n",
        "\n",
        "### TensorFlowÏóêÏÑú ÎìúÎ°≠ÏïÑÏõÉ(Dropout)ÏúºÎ°ú Îã®ÏàúÎßù Í∞úÏÑ†\n",
        "- ÌõàÎ†® Ï§ëÏóê ÏùÄÎãâÏ∏µ ÎÇ¥Î∂Ä Î∞ÄÏßë Ïã†Í≤ΩÎßùÏóê Ï†ÑÌååÎêú Í∞í Ï§ë ÏùºÎ∂ÄÎ•º Î¨¥ÏûëÏúÑÎ°ú Ï†úÍ±∞ -> ÏÑ±Îä• Ìñ•ÏÉÅ\n",
        "- **Î¨¥ÏûëÏúÑ ÎìúÎ°≠ÏïÑÏõÉ Random Dropout**: Ïã†Í≤ΩÎßùÏùò ÏùºÎ∞òÌôîÎ•º Ìñ•ÏÉÅÏãúÌÇ§Îäî Îç∞ ÎèÑÏõÄ ÎêòÎäî Ïú†Ïö©Ìïú Ï§ëÎ≥µ Ìå®ÌÑ¥ ÌïôÏäµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXxovtKLpaTC",
        "outputId": "9d4cb67f-2461-4399-8b0a-25495cbf61f8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ïã†Í≤ΩÎßùÍ≥º ÌõàÎ†®\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # Ï∂úÎ†• Í∞úÏàò = Ïà´Ïûê Í∞úÏàò\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATIONÏùÑ ÏúÑÌï¥ ÏòàÏïΩÎêú TRAINÏùò Ïñë\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "# Î†àÏù¥Î∏îÏùÄ ÏõêÌï´ Ïù∏ÏΩîÎî©ÏúºÎ°ú ÌëúÌòÑ\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_trainÏùÄ 60000Í∞ú ÌñâÏùò 28*28 Í∞í -> 60000*784 ÌòïÌÉúÎ°ú Î≥ÄÍ≤Ω\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# ÏûÖÎ†•ÏùÑ [0, 1] ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Î™®Îç∏ Íµ¨Ï∂ï\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# Î™®Îç∏ ÏöîÏïΩ\n",
        "model.summary()\n",
        "\n",
        "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 1.6911 - accuracy: 0.4684 - val_loss: 0.8806 - val_accuracy: 0.8147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.9110 - accuracy: 0.7175 - val_loss: 0.5224 - val_accuracy: 0.8698\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.6921 - accuracy: 0.7885 - val_loss: 0.4122 - val_accuracy: 0.8927\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.5934 - accuracy: 0.8190 - val_loss: 0.3643 - val_accuracy: 0.9025\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.5299 - accuracy: 0.8396 - val_loss: 0.3309 - val_accuracy: 0.9090\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4858 - accuracy: 0.8556 - val_loss: 0.3089 - val_accuracy: 0.9141\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4519 - accuracy: 0.8647 - val_loss: 0.2909 - val_accuracy: 0.9171\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.4302 - accuracy: 0.8730 - val_loss: 0.2760 - val_accuracy: 0.9213\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4098 - accuracy: 0.8779 - val_loss: 0.2646 - val_accuracy: 0.9237\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3930 - accuracy: 0.8839 - val_loss: 0.2537 - val_accuracy: 0.9256\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3764 - accuracy: 0.8891 - val_loss: 0.2455 - val_accuracy: 0.9273\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3643 - accuracy: 0.8935 - val_loss: 0.2352 - val_accuracy: 0.9308\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3491 - accuracy: 0.8971 - val_loss: 0.2278 - val_accuracy: 0.9324\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3373 - accuracy: 0.9002 - val_loss: 0.2206 - val_accuracy: 0.9339\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3280 - accuracy: 0.9037 - val_loss: 0.2145 - val_accuracy: 0.9374\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3198 - accuracy: 0.9066 - val_loss: 0.2088 - val_accuracy: 0.9389\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3094 - accuracy: 0.9086 - val_loss: 0.2025 - val_accuracy: 0.9398\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3041 - accuracy: 0.9098 - val_loss: 0.1969 - val_accuracy: 0.9419\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.2965 - accuracy: 0.9131 - val_loss: 0.1927 - val_accuracy: 0.9438\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2819 - accuracy: 0.9172 - val_loss: 0.1879 - val_accuracy: 0.9457\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2789 - accuracy: 0.9187 - val_loss: 0.1836 - val_accuracy: 0.9463\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.2723 - accuracy: 0.9193 - val_loss: 0.1807 - val_accuracy: 0.9468\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2709 - accuracy: 0.9193 - val_loss: 0.1762 - val_accuracy: 0.9488\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2632 - accuracy: 0.9227 - val_loss: 0.1726 - val_accuracy: 0.9495\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2581 - accuracy: 0.9246 - val_loss: 0.1696 - val_accuracy: 0.9499\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2500 - accuracy: 0.9259 - val_loss: 0.1656 - val_accuracy: 0.9513\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2480 - accuracy: 0.9284 - val_loss: 0.1625 - val_accuracy: 0.9524\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2409 - accuracy: 0.9288 - val_loss: 0.1604 - val_accuracy: 0.9531\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2435 - accuracy: 0.9284 - val_loss: 0.1582 - val_accuracy: 0.9535\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2370 - accuracy: 0.9307 - val_loss: 0.1549 - val_accuracy: 0.9545\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2351 - accuracy: 0.9317 - val_loss: 0.1538 - val_accuracy: 0.9547\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2274 - accuracy: 0.9337 - val_loss: 0.1507 - val_accuracy: 0.9557\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2264 - accuracy: 0.9334 - val_loss: 0.1495 - val_accuracy: 0.9562\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2204 - accuracy: 0.9346 - val_loss: 0.1463 - val_accuracy: 0.9569\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2201 - accuracy: 0.9357 - val_loss: 0.1443 - val_accuracy: 0.9572\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2147 - accuracy: 0.9361 - val_loss: 0.1423 - val_accuracy: 0.9582\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2124 - accuracy: 0.9375 - val_loss: 0.1403 - val_accuracy: 0.9577\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2078 - accuracy: 0.9384 - val_loss: 0.1389 - val_accuracy: 0.9581\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.2092 - accuracy: 0.9383 - val_loss: 0.1370 - val_accuracy: 0.9589\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2020 - accuracy: 0.9395 - val_loss: 0.1354 - val_accuracy: 0.9597\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2012 - accuracy: 0.9404 - val_loss: 0.1341 - val_accuracy: 0.9603\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1980 - accuracy: 0.9417 - val_loss: 0.1317 - val_accuracy: 0.9603\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1965 - accuracy: 0.9432 - val_loss: 0.1316 - val_accuracy: 0.9607\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1930 - accuracy: 0.9435 - val_loss: 0.1300 - val_accuracy: 0.9612\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1894 - accuracy: 0.9442 - val_loss: 0.1289 - val_accuracy: 0.9619\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1859 - accuracy: 0.9452 - val_loss: 0.1277 - val_accuracy: 0.9624\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1851 - accuracy: 0.9435 - val_loss: 0.1260 - val_accuracy: 0.9617\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1832 - accuracy: 0.9464 - val_loss: 0.1248 - val_accuracy: 0.9624\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1835 - accuracy: 0.9457 - val_loss: 0.1236 - val_accuracy: 0.9628\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1792 - accuracy: 0.9465 - val_loss: 0.1228 - val_accuracy: 0.9628\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1790 - accuracy: 0.9471 - val_loss: 0.1213 - val_accuracy: 0.9638\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1751 - accuracy: 0.9480 - val_loss: 0.1205 - val_accuracy: 0.9638\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1721 - accuracy: 0.9492 - val_loss: 0.1196 - val_accuracy: 0.9635\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1686 - accuracy: 0.9498 - val_loss: 0.1184 - val_accuracy: 0.9652\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1723 - accuracy: 0.9501 - val_loss: 0.1172 - val_accuracy: 0.9654\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1720 - accuracy: 0.9487 - val_loss: 0.1164 - val_accuracy: 0.9651\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1667 - accuracy: 0.9511 - val_loss: 0.1167 - val_accuracy: 0.9648\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1664 - accuracy: 0.9503 - val_loss: 0.1144 - val_accuracy: 0.9659\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1655 - accuracy: 0.9504 - val_loss: 0.1137 - val_accuracy: 0.9659\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1617 - accuracy: 0.9520 - val_loss: 0.1130 - val_accuracy: 0.9657\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1602 - accuracy: 0.9533 - val_loss: 0.1128 - val_accuracy: 0.9660\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1585 - accuracy: 0.9535 - val_loss: 0.1117 - val_accuracy: 0.9659\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1586 - accuracy: 0.9536 - val_loss: 0.1112 - val_accuracy: 0.9664\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1565 - accuracy: 0.9531 - val_loss: 0.1108 - val_accuracy: 0.9668\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1563 - accuracy: 0.9538 - val_loss: 0.1093 - val_accuracy: 0.9670\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1536 - accuracy: 0.9546 - val_loss: 0.1087 - val_accuracy: 0.9679\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1516 - accuracy: 0.9545 - val_loss: 0.1078 - val_accuracy: 0.9674\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1497 - accuracy: 0.9554 - val_loss: 0.1077 - val_accuracy: 0.9672\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1477 - accuracy: 0.9563 - val_loss: 0.1075 - val_accuracy: 0.9674\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1488 - accuracy: 0.9557 - val_loss: 0.1061 - val_accuracy: 0.9682\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1468 - accuracy: 0.9556 - val_loss: 0.1062 - val_accuracy: 0.9678\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1442 - accuracy: 0.9568 - val_loss: 0.1048 - val_accuracy: 0.9688\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1449 - accuracy: 0.9563 - val_loss: 0.1045 - val_accuracy: 0.9688\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1442 - accuracy: 0.9578 - val_loss: 0.1039 - val_accuracy: 0.9686\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1443 - accuracy: 0.9571 - val_loss: 0.1028 - val_accuracy: 0.9688\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1423 - accuracy: 0.9564 - val_loss: 0.1022 - val_accuracy: 0.9691\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1411 - accuracy: 0.9581 - val_loss: 0.1029 - val_accuracy: 0.9691\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1418 - accuracy: 0.9575 - val_loss: 0.1021 - val_accuracy: 0.9697\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1372 - accuracy: 0.9592 - val_loss: 0.1016 - val_accuracy: 0.9695\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1377 - accuracy: 0.9590 - val_loss: 0.1013 - val_accuracy: 0.9686\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1360 - accuracy: 0.9595 - val_loss: 0.1003 - val_accuracy: 0.9698\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1359 - accuracy: 0.9587 - val_loss: 0.1004 - val_accuracy: 0.9699\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1336 - accuracy: 0.9601 - val_loss: 0.0991 - val_accuracy: 0.9700\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1324 - accuracy: 0.9604 - val_loss: 0.0991 - val_accuracy: 0.9706\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1303 - accuracy: 0.9610 - val_loss: 0.0982 - val_accuracy: 0.9712\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1308 - accuracy: 0.9613 - val_loss: 0.0979 - val_accuracy: 0.9703\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1300 - accuracy: 0.9617 - val_loss: 0.0976 - val_accuracy: 0.9703\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1297 - accuracy: 0.9618 - val_loss: 0.0967 - val_accuracy: 0.9707\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1271 - accuracy: 0.9626 - val_loss: 0.0968 - val_accuracy: 0.9706\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1289 - accuracy: 0.9615 - val_loss: 0.0973 - val_accuracy: 0.9705\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1261 - accuracy: 0.9620 - val_loss: 0.0955 - val_accuracy: 0.9709\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1270 - accuracy: 0.9623 - val_loss: 0.0954 - val_accuracy: 0.9710\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1250 - accuracy: 0.9636 - val_loss: 0.0950 - val_accuracy: 0.9708\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1269 - accuracy: 0.9622 - val_loss: 0.0948 - val_accuracy: 0.9710\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1230 - accuracy: 0.9635 - val_loss: 0.0945 - val_accuracy: 0.9712\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1226 - accuracy: 0.9627 - val_loss: 0.0943 - val_accuracy: 0.9719\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1211 - accuracy: 0.9644 - val_loss: 0.0935 - val_accuracy: 0.9716\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1199 - accuracy: 0.9644 - val_loss: 0.0934 - val_accuracy: 0.9724\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1225 - accuracy: 0.9640 - val_loss: 0.0926 - val_accuracy: 0.9722\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1191 - accuracy: 0.9640 - val_loss: 0.0930 - val_accuracy: 0.9717\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1182 - accuracy: 0.9649 - val_loss: 0.0928 - val_accuracy: 0.9719\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1176 - accuracy: 0.9642 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1192 - accuracy: 0.9634 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1191 - accuracy: 0.9645 - val_loss: 0.0913 - val_accuracy: 0.9721\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1150 - accuracy: 0.9656 - val_loss: 0.0912 - val_accuracy: 0.9725\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1147 - accuracy: 0.9663 - val_loss: 0.0909 - val_accuracy: 0.9725\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1161 - accuracy: 0.9647 - val_loss: 0.0911 - val_accuracy: 0.9722\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1132 - accuracy: 0.9664 - val_loss: 0.0914 - val_accuracy: 0.9729\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1116 - accuracy: 0.9664 - val_loss: 0.0905 - val_accuracy: 0.9728\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1128 - accuracy: 0.9665 - val_loss: 0.0900 - val_accuracy: 0.9728\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1118 - accuracy: 0.9665 - val_loss: 0.0897 - val_accuracy: 0.9733\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1130 - accuracy: 0.9664 - val_loss: 0.0888 - val_accuracy: 0.9731\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1106 - accuracy: 0.9671 - val_loss: 0.0891 - val_accuracy: 0.9732\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1100 - accuracy: 0.9671 - val_loss: 0.0892 - val_accuracy: 0.9733\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1100 - accuracy: 0.9678 - val_loss: 0.0887 - val_accuracy: 0.9735\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1113 - accuracy: 0.9674 - val_loss: 0.0881 - val_accuracy: 0.9736\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1079 - accuracy: 0.9671 - val_loss: 0.0890 - val_accuracy: 0.9737\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1076 - accuracy: 0.9675 - val_loss: 0.0881 - val_accuracy: 0.9736\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1115 - accuracy: 0.9662 - val_loss: 0.0875 - val_accuracy: 0.9739\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1096 - accuracy: 0.9670 - val_loss: 0.0877 - val_accuracy: 0.9742\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1063 - accuracy: 0.9682 - val_loss: 0.0868 - val_accuracy: 0.9738\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1048 - accuracy: 0.9686 - val_loss: 0.0872 - val_accuracy: 0.9738\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1041 - accuracy: 0.9687 - val_loss: 0.0869 - val_accuracy: 0.9740\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1052 - accuracy: 0.9687 - val_loss: 0.0867 - val_accuracy: 0.9738\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1040 - accuracy: 0.9694 - val_loss: 0.0861 - val_accuracy: 0.9737\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1046 - accuracy: 0.9684 - val_loss: 0.0864 - val_accuracy: 0.9744\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1030 - accuracy: 0.9692 - val_loss: 0.0856 - val_accuracy: 0.9737\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1034 - accuracy: 0.9682 - val_loss: 0.0855 - val_accuracy: 0.9742\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1022 - accuracy: 0.9699 - val_loss: 0.0863 - val_accuracy: 0.9745\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0997 - accuracy: 0.9698 - val_loss: 0.0856 - val_accuracy: 0.9746\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1019 - accuracy: 0.9693 - val_loss: 0.0848 - val_accuracy: 0.9747\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0994 - accuracy: 0.9703 - val_loss: 0.0852 - val_accuracy: 0.9747\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0995 - accuracy: 0.9697 - val_loss: 0.0849 - val_accuracy: 0.9740\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0984 - accuracy: 0.9701 - val_loss: 0.0845 - val_accuracy: 0.9746\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0975 - accuracy: 0.9706 - val_loss: 0.0847 - val_accuracy: 0.9742\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0976 - accuracy: 0.9710 - val_loss: 0.0845 - val_accuracy: 0.9742\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0975 - accuracy: 0.9705 - val_loss: 0.0846 - val_accuracy: 0.9748\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0955 - accuracy: 0.9712 - val_loss: 0.0840 - val_accuracy: 0.9753\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0963 - accuracy: 0.9705 - val_loss: 0.0838 - val_accuracy: 0.9743\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0962 - accuracy: 0.9704 - val_loss: 0.0835 - val_accuracy: 0.9747\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0941 - accuracy: 0.9725 - val_loss: 0.0841 - val_accuracy: 0.9750\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0946 - accuracy: 0.9712 - val_loss: 0.0832 - val_accuracy: 0.9752\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0975 - accuracy: 0.9697 - val_loss: 0.0831 - val_accuracy: 0.9753\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0921 - accuracy: 0.9718 - val_loss: 0.0827 - val_accuracy: 0.9756\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0952 - accuracy: 0.9706 - val_loss: 0.0827 - val_accuracy: 0.9745\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0948 - accuracy: 0.9715 - val_loss: 0.0831 - val_accuracy: 0.9747\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0931 - accuracy: 0.9716 - val_loss: 0.0827 - val_accuracy: 0.9748\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0918 - accuracy: 0.9722 - val_loss: 0.0828 - val_accuracy: 0.9749\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0914 - accuracy: 0.9734 - val_loss: 0.0831 - val_accuracy: 0.9751\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0907 - accuracy: 0.9722 - val_loss: 0.0825 - val_accuracy: 0.9754\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0902 - accuracy: 0.9722 - val_loss: 0.0828 - val_accuracy: 0.9753\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0901 - accuracy: 0.9721 - val_loss: 0.0820 - val_accuracy: 0.9753\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0921 - accuracy: 0.9721 - val_loss: 0.0816 - val_accuracy: 0.9754\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0886 - accuracy: 0.9730 - val_loss: 0.0824 - val_accuracy: 0.9759\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0888 - accuracy: 0.9724 - val_loss: 0.0819 - val_accuracy: 0.9751\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0889 - accuracy: 0.9728 - val_loss: 0.0817 - val_accuracy: 0.9755\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0907 - accuracy: 0.9726 - val_loss: 0.0814 - val_accuracy: 0.9753\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0883 - accuracy: 0.9731 - val_loss: 0.0810 - val_accuracy: 0.9758\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0896 - accuracy: 0.9729 - val_loss: 0.0815 - val_accuracy: 0.9753\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0883 - accuracy: 0.9729 - val_loss: 0.0812 - val_accuracy: 0.9756\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0889 - accuracy: 0.9730 - val_loss: 0.0811 - val_accuracy: 0.9760\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0892 - accuracy: 0.9728 - val_loss: 0.0807 - val_accuracy: 0.9760\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0866 - accuracy: 0.9745 - val_loss: 0.0807 - val_accuracy: 0.9761\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0858 - accuracy: 0.9736 - val_loss: 0.0811 - val_accuracy: 0.9762\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0857 - accuracy: 0.9741 - val_loss: 0.0802 - val_accuracy: 0.9758\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0849 - accuracy: 0.9738 - val_loss: 0.0805 - val_accuracy: 0.9768\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0858 - accuracy: 0.9734 - val_loss: 0.0802 - val_accuracy: 0.9761\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0865 - accuracy: 0.9738 - val_loss: 0.0805 - val_accuracy: 0.9763\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0834 - accuracy: 0.9740 - val_loss: 0.0804 - val_accuracy: 0.9758\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0853 - accuracy: 0.9736 - val_loss: 0.0798 - val_accuracy: 0.9765\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0832 - accuracy: 0.9751 - val_loss: 0.0804 - val_accuracy: 0.9774\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0835 - accuracy: 0.9745 - val_loss: 0.0795 - val_accuracy: 0.9765\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0850 - accuracy: 0.9741 - val_loss: 0.0795 - val_accuracy: 0.9768\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0833 - accuracy: 0.9740 - val_loss: 0.0801 - val_accuracy: 0.9770\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0815 - accuracy: 0.9750 - val_loss: 0.0804 - val_accuracy: 0.9768\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0820 - accuracy: 0.9750 - val_loss: 0.0792 - val_accuracy: 0.9764\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0808 - accuracy: 0.9752 - val_loss: 0.0790 - val_accuracy: 0.9764\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0803 - accuracy: 0.9760 - val_loss: 0.0798 - val_accuracy: 0.9768\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0814 - accuracy: 0.9749 - val_loss: 0.0793 - val_accuracy: 0.9769\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0796 - accuracy: 0.9756 - val_loss: 0.0790 - val_accuracy: 0.9772\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0829 - accuracy: 0.9741 - val_loss: 0.0784 - val_accuracy: 0.9767\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0792 - accuracy: 0.9758 - val_loss: 0.0789 - val_accuracy: 0.9763\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0828 - accuracy: 0.9750 - val_loss: 0.0784 - val_accuracy: 0.9775\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0793 - accuracy: 0.9754 - val_loss: 0.0791 - val_accuracy: 0.9770\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0789 - accuracy: 0.9758 - val_loss: 0.0791 - val_accuracy: 0.9774\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0794 - accuracy: 0.9761 - val_loss: 0.0790 - val_accuracy: 0.9764\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0803 - accuracy: 0.9750 - val_loss: 0.0790 - val_accuracy: 0.9770\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0772 - accuracy: 0.9760 - val_loss: 0.0784 - val_accuracy: 0.9770\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0790 - accuracy: 0.9749 - val_loss: 0.0790 - val_accuracy: 0.9774\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0769 - accuracy: 0.9768 - val_loss: 0.0781 - val_accuracy: 0.9778\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0790 - accuracy: 0.9752 - val_loss: 0.0779 - val_accuracy: 0.9774\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0765 - accuracy: 0.9766 - val_loss: 0.0784 - val_accuracy: 0.9775\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0796 - accuracy: 0.9754 - val_loss: 0.0782 - val_accuracy: 0.9775\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0762 - accuracy: 0.9772 - val_loss: 0.0776 - val_accuracy: 0.9771\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0763 - accuracy: 0.9768 - val_loss: 0.0779 - val_accuracy: 0.9778\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0749 - accuracy: 0.9778 - val_loss: 0.0778 - val_accuracy: 0.9781\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0765 - accuracy: 0.9759 - val_loss: 0.0771 - val_accuracy: 0.9778\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0750 - accuracy: 0.9765 - val_loss: 0.0769 - val_accuracy: 0.9782\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0745 - accuracy: 0.9771 - val_loss: 0.0773 - val_accuracy: 0.9781\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0776 - accuracy: 0.9758 - val_loss: 0.0770 - val_accuracy: 0.9776\n",
            "Test accuracy:  0.9765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAm4FirpaTD"
      },
      "source": [
        "- ÎÇ¥Î∂Ä ÏùÄÎãâÏ∏µÏóêÏÑú Î¨¥ÏûëÏúÑÎ°ú ÎìúÎ°≠ÏïÑÏõÉÌïòÎäî Ïã†Í≤ΩÎßùÏù¥ ÌÖåÏä§Ìä∏ ÏßëÌï©Ïóê Ìè¨Ìï®Îêú ÎÇØÏÑ†(unseen) ÏòàÏãúÎ•º Ïûò 'ÏùºÎ∞òÌôî'ÌïúÎã§.\n",
        "    - ‚àµ Í∞ÅÍ∞ÅÏùò Îâ¥Îü∞Ïù¥ ÏûêÍ∏∞ Ïù¥ÏõÉÏóê ÏùòÏ°¥Ìï† Ïàò ÏóÜÎã§Îäî Í≤ÉÏùÑ Ïù∏Ïãù, Ï§ëÎ≥µÎêú Î∞©ÏãùÏúºÎ°ú Ï†ïÎ≥¥Í∞Ä Ï†ÄÏû•ÎêòÎèÑÎ°ù Í∞ïÏ†ú\n",
        "- ÌÖåÏä§Ìä∏ Ï§ëÏóêÎäî ÎìúÎ°≠ÏïÑÏõÉÏù¥ ÏóÜÏúºÎØÄÎ°ú, Î™®Îì† Îâ¥Îü∞Ïù¥ ÏÇ¨Ïö©ÎêúÎã§.\n",
        "- ÌõàÎ†® Ï†ïÌôïÎèÑÎäî ÌÖåÏä§Ìä∏ Ï†ïÌôïÎèÑÎ≥¥Îã§ ÎÜíÏïÑÏïº ÌïúÎã§. Í∑∏Î†áÏßÄ ÏïäÎã§Î©¥ ÏóêÌè≠Ïùò ÏàòÎ•º Îçî ÎäòÎ†§Ïïº ÌïúÎã§.\n",
        "\n",
        "### TensorFlow 2.0ÏóêÏÑú Ïó¨Îü¨ Optimizer ÌÖåÏä§Ìä∏\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117948802-72369c00-b34c-11eb-93ae-9d2cd74a3a09.png)\n",
        "\n",
        "- Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌïòÍ∞ï(Gradient Descent, GD)\n",
        "    - ÌïòÎÇòÏùò Îã®Ïùº Î≥ÄÏàò wÏóê ÎåÄÌï¥ ÏùºÎ∞òÏ†ÅÏù∏ ÎπÑÏö© Ìï®Ïàò C(w)Í∞Ä ÏûàÎã§Í≥† Í∞ÄÏ†ïÌïòÏûê.\n",
        "    - Î™©Ìëú: Ï∂úÎ∞úÏ†ê w0ÏóêÏÑú ÏïÑÏ£º Ï°∞Í∏àÏî© ÏõÄÏßÅÏù¥Î©¥ÏÑú Í≤ΩÏÇ¨(Ìï®Ïàò C)Î©¥ÏùÑ Îî∞Îùº ÎÇ¥Î†§Í∞Ä ÎèÑÎûë(ÏµúÏÜå Cmin) Ï∞æÍ∏∞\n",
        "    - Í∞Å Îã®Í≥Ñ rÏóêÏÑú gradient = ÏµúÎåÄ Ï¶ùÍ∞Ä Î∞©Ìñ• -> Îã®Í≥Ñ rÏóêÏÑú ÎèÑÎã¨Ìïú ÏßÄÏ†ê wrÏóêÏÑú Í≥ÑÏÇ∞Îêú Ìé∏ÎØ∏Î∂Ñ Í∞í ‚àÇC/‚àÇw\n",
        "    - => Î∞òÎåÄ Î∞©Ìñ•Ïù∏ -(‚àÇC/‚àÇw)(wr)ÏùÑ ÌÉùÌïòÎ©¥ ÎèÑÎûëÏùÑ Ìñ•Ìï¥ ÎÇòÏïÑÍ∞à Ïàò ÏûàÎã§.\n",
        "    - ÌïôÏäµÎ•† Œ∑(>= 0): Îã§Ïùå Îã®Í≥ÑÏùò Î≥¥Ìè≠\n",
        "        - Œ∑Ïù¥ ÎÑàÎ¨¥ ÏûëÎã§Î©¥ Ï≤úÏ≤úÌûà Ïù¥Îèô\n",
        "        - Œ∑Ïù¥ ÎÑàÎ¨¥ ÌÅ¨Îã§Î©¥ ÎèÑÎûëÏùÑ ÏßÄÎÇòÏπ† Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÎã§.\n",
        "    - sigmoid function\n",
        "        - Ïó∞ÏÜçÏù¥Í≥† ÎØ∏Î∂Ñ Í∞ÄÎä•\n",
        "        - ùúé(ùë•)= 1/(1+ùëí^(‚àíùë•))Ïù¥Î©¥, dùúé(ùë•)/d(x) = ùúé(ùë•)(1-ùúé(ùë•))\n",
        "    - ReLU function\n",
        "        - 0ÏóêÏÑú ÎØ∏Î∂Ñ Î∂àÍ∞ÄÎä•\n",
        "        - -> 0ÏóêÏÑú ÎØ∏Î∂Ñ Í∞íÏùÑ 0Ïù¥ÎÇò 1Î°ú ÏûÑÏùò ÏßÄÏ†ïÌïòÎ©¥ Ï†ÑÏ≤¥ Î≤îÏúÑÎ°ú ÌôïÏû• Í∞ÄÎä•\n",
        "        - y = max(0, x)Ïùò Î∂ÄÎ∂Ñ ÎØ∏Î∂Ñ dy/dx = 0(x <= 0), 1(x > 0) -> GDÎ•º ÌÜµÌï¥ ÎßùÏùÑ ÏµúÏ†ÅÌôîÌï† Ïàò ÏûàÎã§.\n",
        "- TensorFlowÏóêÎäî GDÏùò ÏÜçÎèÑÎ•º ÎÜíÏù¥Í∏∞ ÏúÑÌïú Î≥ÄÌòïÏù∏ SGDÏôÄ RMSProp, Adam Îì±Ïùò ÏµúÏ†ÅÌôî Í∏∞Ïà†Ïù¥ Ï†úÍ≥µÎêúÎã§.\n",
        "    - RMSProp, AdamÏùÄ SGDÏùò Í∞ÄÏÜç Íµ¨ÏÑ± ÏöîÏÜå Ïô∏ÏóêÎèÑ momentum(ÏÜçÎèÑ ÏöîÏÜå) Í∞úÎÖê Ìè¨Ìï® -> ÎßéÏùÄ Í≥ÑÏÇ∞ÏùÑ ÌÜµÌïú Îçî Îπ†Î•∏ ÏàòÎ†¥\n",
        "        - momentumÏùÑ ÌÜµÌï¥ SGDÎ•º Ïú†Í¥Ä Î∞©Ìñ•ÏúºÎ°ú Í∞ÄÏÜçÌôîÌïòÍ≥† Ïù¥ÎèôÌïòÎäî Í≤ÉÏùÑ Ï§ÑÏùº Ïàò ÏûàÎã§."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQGFnHKwpaTD",
        "outputId": "aad34f91-f4f0-4084-9273-4436171bf30c"
      },
      "source": [
        "# RMSProp, epochs = 200\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ïã†Í≤ΩÎßùÍ≥º ÌõàÎ†®\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # Ï∂úÎ†• Í∞úÏàò = Ïà´Ïûê Í∞úÏàò\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATIONÏùÑ ÏúÑÌï¥ ÏòàÏïΩÎêú TRAINÏùò Ïñë\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "# Î†àÏù¥Î∏îÏùÄ ÏõêÌï´ Ïù∏ÏΩîÎî©ÏúºÎ°ú ÌëúÌòÑ\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_trainÏùÄ 60000Í∞ú ÌñâÏùò 28*28 Í∞í -> 60000*784 ÌòïÌÉúÎ°ú Î≥ÄÍ≤Ω\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# ÏûÖÎ†•ÏùÑ [0, 1] ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Î™®Îç∏ Íµ¨Ï∂ï\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# Î™®Îç∏ ÏöîÏïΩ\n",
        "model.summary()\n",
        "\n",
        "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
        "# Î≥ÄÍ≤Ω Î∂ÄÎ∂Ñ\n",
        "model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "47616/48000 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.8571"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.4723 - accuracy: 0.8575 - val_loss: 0.1786 - val_accuracy: 0.9469\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.2224 - accuracy: 0.9338 - val_loss: 0.1324 - val_accuracy: 0.9599\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1737 - accuracy: 0.9481 - val_loss: 0.1204 - val_accuracy: 0.9639\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1505 - accuracy: 0.9559 - val_loss: 0.1072 - val_accuracy: 0.9688\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1321 - accuracy: 0.9600 - val_loss: 0.1118 - val_accuracy: 0.9670\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1254 - accuracy: 0.9624 - val_loss: 0.1066 - val_accuracy: 0.9718\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1102 - accuracy: 0.9681 - val_loss: 0.1005 - val_accuracy: 0.9723\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1046 - accuracy: 0.9694 - val_loss: 0.1041 - val_accuracy: 0.9717\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0995 - accuracy: 0.9709 - val_loss: 0.0965 - val_accuracy: 0.9750\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0918 - accuracy: 0.9721 - val_loss: 0.1023 - val_accuracy: 0.9751\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0884 - accuracy: 0.9736 - val_loss: 0.0995 - val_accuracy: 0.9742\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0854 - accuracy: 0.9743 - val_loss: 0.0984 - val_accuracy: 0.9747\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0851 - accuracy: 0.9751 - val_loss: 0.1023 - val_accuracy: 0.9747\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0751 - accuracy: 0.9771 - val_loss: 0.1057 - val_accuracy: 0.9745\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0801 - accuracy: 0.9763 - val_loss: 0.1002 - val_accuracy: 0.9774\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0754 - accuracy: 0.9778 - val_loss: 0.1086 - val_accuracy: 0.9771\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0738 - accuracy: 0.9789 - val_loss: 0.1078 - val_accuracy: 0.9774\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0719 - accuracy: 0.9787 - val_loss: 0.1068 - val_accuracy: 0.9769\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0697 - accuracy: 0.9793 - val_loss: 0.1090 - val_accuracy: 0.9773\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0680 - accuracy: 0.9799 - val_loss: 0.1120 - val_accuracy: 0.9776\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0679 - accuracy: 0.9803 - val_loss: 0.1129 - val_accuracy: 0.9780\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0646 - accuracy: 0.9807 - val_loss: 0.1113 - val_accuracy: 0.9783\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0676 - accuracy: 0.9812 - val_loss: 0.1100 - val_accuracy: 0.9764\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0673 - accuracy: 0.9802 - val_loss: 0.1139 - val_accuracy: 0.9791\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0630 - accuracy: 0.9814 - val_loss: 0.1281 - val_accuracy: 0.9774\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0627 - accuracy: 0.9819 - val_loss: 0.1309 - val_accuracy: 0.9770\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0628 - accuracy: 0.9825 - val_loss: 0.1261 - val_accuracy: 0.9781\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0636 - accuracy: 0.9821 - val_loss: 0.1204 - val_accuracy: 0.9773\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0631 - accuracy: 0.9825 - val_loss: 0.1234 - val_accuracy: 0.9790\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0593 - accuracy: 0.9822 - val_loss: 0.1249 - val_accuracy: 0.9779\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0610 - accuracy: 0.9831 - val_loss: 0.1303 - val_accuracy: 0.9776\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.1346 - val_accuracy: 0.9771\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0547 - accuracy: 0.9841 - val_loss: 0.1349 - val_accuracy: 0.9772\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0561 - accuracy: 0.9844 - val_loss: 0.1368 - val_accuracy: 0.9768\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0559 - accuracy: 0.9851 - val_loss: 0.1411 - val_accuracy: 0.9774\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0588 - accuracy: 0.9834 - val_loss: 0.1344 - val_accuracy: 0.9780\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0573 - accuracy: 0.9841 - val_loss: 0.1329 - val_accuracy: 0.9775\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0578 - accuracy: 0.9841 - val_loss: 0.1492 - val_accuracy: 0.9771\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0554 - accuracy: 0.9848 - val_loss: 0.1458 - val_accuracy: 0.9780\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0564 - accuracy: 0.9846 - val_loss: 0.1449 - val_accuracy: 0.9774\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0538 - accuracy: 0.9856 - val_loss: 0.1438 - val_accuracy: 0.9778\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0584 - accuracy: 0.9846 - val_loss: 0.1451 - val_accuracy: 0.9765\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0554 - accuracy: 0.9852 - val_loss: 0.1484 - val_accuracy: 0.9776\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0577 - accuracy: 0.9852 - val_loss: 0.1518 - val_accuracy: 0.9759\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0541 - accuracy: 0.9850 - val_loss: 0.1567 - val_accuracy: 0.9775\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0537 - accuracy: 0.9853 - val_loss: 0.1578 - val_accuracy: 0.9772\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0560 - accuracy: 0.9858 - val_loss: 0.1593 - val_accuracy: 0.9768\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0548 - accuracy: 0.9858 - val_loss: 0.1586 - val_accuracy: 0.9763\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0553 - accuracy: 0.9849 - val_loss: 0.1549 - val_accuracy: 0.9786\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0513 - accuracy: 0.9862 - val_loss: 0.1599 - val_accuracy: 0.9780\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0549 - accuracy: 0.9859 - val_loss: 0.1631 - val_accuracy: 0.9767\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0533 - accuracy: 0.9858 - val_loss: 0.1876 - val_accuracy: 0.9761\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0540 - accuracy: 0.9860 - val_loss: 0.1739 - val_accuracy: 0.9772\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0521 - accuracy: 0.9859 - val_loss: 0.1717 - val_accuracy: 0.9769\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0530 - accuracy: 0.9858 - val_loss: 0.1698 - val_accuracy: 0.9770\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0534 - accuracy: 0.9864 - val_loss: 0.1802 - val_accuracy: 0.9766\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0521 - accuracy: 0.9865 - val_loss: 0.1695 - val_accuracy: 0.9786\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0556 - accuracy: 0.9861 - val_loss: 0.1826 - val_accuracy: 0.9774\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0506 - accuracy: 0.9864 - val_loss: 0.1841 - val_accuracy: 0.9773\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0500 - accuracy: 0.9870 - val_loss: 0.1787 - val_accuracy: 0.9781\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0518 - accuracy: 0.9873 - val_loss: 0.1805 - val_accuracy: 0.9778\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9871 - val_loss: 0.1776 - val_accuracy: 0.9766\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9870 - val_loss: 0.1951 - val_accuracy: 0.9768\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0539 - accuracy: 0.9865 - val_loss: 0.1959 - val_accuracy: 0.9763\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0505 - accuracy: 0.9869 - val_loss: 0.1922 - val_accuracy: 0.9773\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0500 - accuracy: 0.9869 - val_loss: 0.1804 - val_accuracy: 0.9773\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0513 - accuracy: 0.9870 - val_loss: 0.2048 - val_accuracy: 0.9775\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0519 - accuracy: 0.9871 - val_loss: 0.1994 - val_accuracy: 0.9764\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0529 - accuracy: 0.9868 - val_loss: 0.1835 - val_accuracy: 0.9762\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0528 - accuracy: 0.9873 - val_loss: 0.1819 - val_accuracy: 0.9764\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0523 - accuracy: 0.9875 - val_loss: 0.1916 - val_accuracy: 0.9778\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0494 - accuracy: 0.9877 - val_loss: 0.1999 - val_accuracy: 0.9771\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9872 - val_loss: 0.1916 - val_accuracy: 0.9785\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0520 - accuracy: 0.9875 - val_loss: 0.2013 - val_accuracy: 0.9780\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0510 - accuracy: 0.9880 - val_loss: 0.1948 - val_accuracy: 0.9772\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0487 - accuracy: 0.9875 - val_loss: 0.2008 - val_accuracy: 0.9767\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0459 - accuracy: 0.9880 - val_loss: 0.2065 - val_accuracy: 0.9770\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0548 - accuracy: 0.9870 - val_loss: 0.1937 - val_accuracy: 0.9785\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0523 - accuracy: 0.9874 - val_loss: 0.1925 - val_accuracy: 0.9776\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0462 - accuracy: 0.9877 - val_loss: 0.2057 - val_accuracy: 0.9776\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0499 - accuracy: 0.9876 - val_loss: 0.2029 - val_accuracy: 0.9775\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0535 - accuracy: 0.9869 - val_loss: 0.2133 - val_accuracy: 0.9774\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0518 - accuracy: 0.9867 - val_loss: 0.2031 - val_accuracy: 0.9762\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0515 - accuracy: 0.9875 - val_loss: 0.2103 - val_accuracy: 0.9766\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0499 - accuracy: 0.9875 - val_loss: 0.2284 - val_accuracy: 0.9760\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0526 - accuracy: 0.9875 - val_loss: 0.2078 - val_accuracy: 0.9765\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0505 - accuracy: 0.9883 - val_loss: 0.2024 - val_accuracy: 0.9772\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0480 - accuracy: 0.9877 - val_loss: 0.2097 - val_accuracy: 0.9783\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0518 - accuracy: 0.9878 - val_loss: 0.2207 - val_accuracy: 0.9768\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0492 - accuracy: 0.9883 - val_loss: 0.2254 - val_accuracy: 0.9778\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0465 - accuracy: 0.9884 - val_loss: 0.2266 - val_accuracy: 0.9786\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0490 - accuracy: 0.9880 - val_loss: 0.2309 - val_accuracy: 0.9763\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0504 - accuracy: 0.9885 - val_loss: 0.2244 - val_accuracy: 0.9769\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0507 - accuracy: 0.9879 - val_loss: 0.2380 - val_accuracy: 0.9772\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0484 - accuracy: 0.9879 - val_loss: 0.2130 - val_accuracy: 0.9786\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0476 - accuracy: 0.9883 - val_loss: 0.2252 - val_accuracy: 0.9783\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0485 - accuracy: 0.9891 - val_loss: 0.2284 - val_accuracy: 0.9768\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0570 - accuracy: 0.9887 - val_loss: 0.2245 - val_accuracy: 0.9773\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0496 - accuracy: 0.9879 - val_loss: 0.2328 - val_accuracy: 0.9766\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0521 - accuracy: 0.9883 - val_loss: 0.2206 - val_accuracy: 0.9779\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0488 - accuracy: 0.9884 - val_loss: 0.2203 - val_accuracy: 0.9779\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9890 - val_loss: 0.2403 - val_accuracy: 0.9769\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0526 - accuracy: 0.9880 - val_loss: 0.2337 - val_accuracy: 0.9776\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0498 - accuracy: 0.9884 - val_loss: 0.2330 - val_accuracy: 0.9772\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0514 - accuracy: 0.9892 - val_loss: 0.2311 - val_accuracy: 0.9781\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0464 - accuracy: 0.9884 - val_loss: 0.2366 - val_accuracy: 0.9773\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0490 - accuracy: 0.9891 - val_loss: 0.2471 - val_accuracy: 0.9769\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0539 - accuracy: 0.9885 - val_loss: 0.2245 - val_accuracy: 0.9776\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9881 - val_loss: 0.2361 - val_accuracy: 0.9772\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0531 - accuracy: 0.9882 - val_loss: 0.2303 - val_accuracy: 0.9770\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0477 - accuracy: 0.9891 - val_loss: 0.2599 - val_accuracy: 0.9768\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0510 - accuracy: 0.9891 - val_loss: 0.2431 - val_accuracy: 0.9775\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0483 - accuracy: 0.9894 - val_loss: 0.2431 - val_accuracy: 0.9775\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9886 - val_loss: 0.2583 - val_accuracy: 0.9770\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9884 - val_loss: 0.2599 - val_accuracy: 0.9764\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0514 - accuracy: 0.9886 - val_loss: 0.2663 - val_accuracy: 0.9773\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0511 - accuracy: 0.9888 - val_loss: 0.2345 - val_accuracy: 0.9773\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0525 - accuracy: 0.9884 - val_loss: 0.2443 - val_accuracy: 0.9768\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0555 - accuracy: 0.9883 - val_loss: 0.2585 - val_accuracy: 0.9766\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0534 - accuracy: 0.9881 - val_loss: 0.2496 - val_accuracy: 0.9758\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0532 - accuracy: 0.9881 - val_loss: 0.2380 - val_accuracy: 0.9767\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0504 - accuracy: 0.9887 - val_loss: 0.2562 - val_accuracy: 0.9752\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0482 - accuracy: 0.9889 - val_loss: 0.2593 - val_accuracy: 0.9762\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0489 - accuracy: 0.9890 - val_loss: 0.2554 - val_accuracy: 0.9780\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0542 - accuracy: 0.9882 - val_loss: 0.2513 - val_accuracy: 0.9776\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0510 - accuracy: 0.9891 - val_loss: 0.2508 - val_accuracy: 0.9768\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.2654 - val_accuracy: 0.9776\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0492 - accuracy: 0.9889 - val_loss: 0.2510 - val_accuracy: 0.9778\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0524 - accuracy: 0.9893 - val_loss: 0.2682 - val_accuracy: 0.9766\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0507 - accuracy: 0.9891 - val_loss: 0.2596 - val_accuracy: 0.9759\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0498 - accuracy: 0.9888 - val_loss: 0.2701 - val_accuracy: 0.9762\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0503 - accuracy: 0.9895 - val_loss: 0.2750 - val_accuracy: 0.9787\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0495 - accuracy: 0.9897 - val_loss: 0.2833 - val_accuracy: 0.9770\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9890 - val_loss: 0.2544 - val_accuracy: 0.9778\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0535 - accuracy: 0.9892 - val_loss: 0.2746 - val_accuracy: 0.9758\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.2558 - val_accuracy: 0.9770\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0518 - accuracy: 0.9891 - val_loss: 0.2413 - val_accuracy: 0.9767\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0490 - accuracy: 0.9891 - val_loss: 0.2599 - val_accuracy: 0.9766\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0469 - accuracy: 0.9891 - val_loss: 0.2760 - val_accuracy: 0.9766\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0537 - accuracy: 0.9887 - val_loss: 0.2527 - val_accuracy: 0.9762\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0576 - accuracy: 0.9877 - val_loss: 0.2660 - val_accuracy: 0.9784\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0554 - accuracy: 0.9880 - val_loss: 0.2590 - val_accuracy: 0.9766\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0525 - accuracy: 0.9889 - val_loss: 0.2492 - val_accuracy: 0.9777\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9890 - val_loss: 0.2751 - val_accuracy: 0.9762\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0485 - accuracy: 0.9902 - val_loss: 0.2582 - val_accuracy: 0.9778\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0486 - accuracy: 0.9898 - val_loss: 0.2713 - val_accuracy: 0.9778\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0553 - accuracy: 0.9887 - val_loss: 0.2713 - val_accuracy: 0.9774\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0508 - accuracy: 0.9894 - val_loss: 0.2732 - val_accuracy: 0.9776\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0496 - accuracy: 0.9896 - val_loss: 0.2710 - val_accuracy: 0.9772\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0521 - accuracy: 0.9892 - val_loss: 0.2877 - val_accuracy: 0.9768\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0584 - accuracy: 0.9883 - val_loss: 0.2810 - val_accuracy: 0.9769\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0555 - accuracy: 0.9890 - val_loss: 0.2774 - val_accuracy: 0.9780\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0513 - accuracy: 0.9899 - val_loss: 0.2661 - val_accuracy: 0.9778\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0524 - accuracy: 0.9889 - val_loss: 0.2632 - val_accuracy: 0.9768\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0488 - accuracy: 0.9897 - val_loss: 0.2788 - val_accuracy: 0.9774\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0486 - accuracy: 0.9894 - val_loss: 0.2663 - val_accuracy: 0.9772\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9892 - val_loss: 0.2853 - val_accuracy: 0.9773\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0529 - accuracy: 0.9889 - val_loss: 0.2806 - val_accuracy: 0.9772\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0461 - accuracy: 0.9902 - val_loss: 0.3113 - val_accuracy: 0.9770\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0505 - accuracy: 0.9896 - val_loss: 0.2833 - val_accuracy: 0.9767\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0494 - accuracy: 0.9898 - val_loss: 0.2740 - val_accuracy: 0.9777\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0500 - accuracy: 0.9890 - val_loss: 0.2863 - val_accuracy: 0.9762\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9888 - val_loss: 0.2765 - val_accuracy: 0.9780\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0484 - accuracy: 0.9902 - val_loss: 0.2756 - val_accuracy: 0.9775\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0528 - accuracy: 0.9899 - val_loss: 0.2811 - val_accuracy: 0.9766\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0543 - accuracy: 0.9894 - val_loss: 0.2936 - val_accuracy: 0.9776\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0516 - accuracy: 0.9892 - val_loss: 0.2924 - val_accuracy: 0.9768\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0465 - accuracy: 0.9899 - val_loss: 0.2957 - val_accuracy: 0.9771\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0497 - accuracy: 0.9893 - val_loss: 0.2952 - val_accuracy: 0.9768\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0499 - accuracy: 0.9898 - val_loss: 0.2890 - val_accuracy: 0.9770\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0507 - accuracy: 0.9892 - val_loss: 0.2843 - val_accuracy: 0.9776\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0511 - accuracy: 0.9897 - val_loss: 0.3011 - val_accuracy: 0.9779\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0472 - accuracy: 0.9902 - val_loss: 0.2973 - val_accuracy: 0.9770\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0524 - accuracy: 0.9899 - val_loss: 0.2974 - val_accuracy: 0.9772\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0540 - accuracy: 0.9889 - val_loss: 0.2888 - val_accuracy: 0.9775\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0545 - accuracy: 0.9895 - val_loss: 0.2828 - val_accuracy: 0.9767\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0537 - accuracy: 0.9895 - val_loss: 0.2980 - val_accuracy: 0.9779\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0525 - accuracy: 0.9898 - val_loss: 0.3004 - val_accuracy: 0.9769\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0546 - accuracy: 0.9893 - val_loss: 0.3089 - val_accuracy: 0.9773\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0514 - accuracy: 0.9900 - val_loss: 0.3222 - val_accuracy: 0.9769\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0560 - accuracy: 0.9893 - val_loss: 0.2967 - val_accuracy: 0.9772\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0611 - accuracy: 0.9887 - val_loss: 0.2962 - val_accuracy: 0.9768\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0523 - accuracy: 0.9891 - val_loss: 0.2949 - val_accuracy: 0.9768\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0498 - accuracy: 0.9900 - val_loss: 0.2801 - val_accuracy: 0.9779\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0483 - accuracy: 0.9898 - val_loss: 0.2906 - val_accuracy: 0.9776\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9895 - val_loss: 0.3337 - val_accuracy: 0.9768\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0545 - accuracy: 0.9891 - val_loss: 0.3137 - val_accuracy: 0.9771\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0476 - accuracy: 0.9903 - val_loss: 0.3081 - val_accuracy: 0.9781\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0525 - accuracy: 0.9897 - val_loss: 0.2887 - val_accuracy: 0.9777\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0536 - accuracy: 0.9893 - val_loss: 0.3169 - val_accuracy: 0.9782\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0550 - accuracy: 0.9897 - val_loss: 0.3196 - val_accuracy: 0.9761\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0507 - accuracy: 0.9896 - val_loss: 0.3055 - val_accuracy: 0.9779\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0482 - accuracy: 0.9900 - val_loss: 0.3178 - val_accuracy: 0.9768\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0552 - accuracy: 0.9898 - val_loss: 0.3192 - val_accuracy: 0.9778\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0577 - accuracy: 0.9889 - val_loss: 0.3191 - val_accuracy: 0.9766\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0546 - accuracy: 0.9892 - val_loss: 0.3116 - val_accuracy: 0.9777\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0576 - accuracy: 0.9885 - val_loss: 0.2998 - val_accuracy: 0.9769\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0494 - accuracy: 0.9903 - val_loss: 0.3199 - val_accuracy: 0.9769\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0504 - accuracy: 0.9898 - val_loss: 0.3293 - val_accuracy: 0.9772\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0530 - accuracy: 0.9902 - val_loss: 0.2981 - val_accuracy: 0.9771\n",
            "Test accuracy:  0.9781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt0UTVu4paTE",
        "outputId": "d876325c-7dc4-4047-b1c6-1a3200315ef3"
      },
      "source": [
        "# RMSProp, epochs = 250\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ïã†Í≤ΩÎßùÍ≥º ÌõàÎ†®\n",
        "EPOCHS = 250\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # Ï∂úÎ†• Í∞úÏàò = Ïà´Ïûê Í∞úÏàò\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATIONÏùÑ ÏúÑÌï¥ ÏòàÏïΩÎêú TRAINÏùò Ïñë\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "# Î†àÏù¥Î∏îÏùÄ ÏõêÌï´ Ïù∏ÏΩîÎî©ÏúºÎ°ú ÌëúÌòÑ\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_trainÏùÄ 60000Í∞ú ÌñâÏùò 28*28 Í∞í -> 60000*784 ÌòïÌÉúÎ°ú Î≥ÄÍ≤Ω\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# ÏûÖÎ†•ÏùÑ [0, 1] ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Î™®Îç∏ Íµ¨Ï∂ï\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# Î™®Îç∏ ÏöîÏïΩ\n",
        "model.summary()\n",
        "\n",
        "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
        "# Î≥ÄÍ≤Ω Î∂ÄÎ∂Ñ\n",
        "model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/250\n",
            "47104/48000 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.8567"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.4700 - accuracy: 0.8581 - val_loss: 0.1716 - val_accuracy: 0.9508\n",
            "Epoch 2/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.2192 - accuracy: 0.9351 - val_loss: 0.1384 - val_accuracy: 0.9590\n",
            "Epoch 3/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.1678 - accuracy: 0.9506 - val_loss: 0.1162 - val_accuracy: 0.9668\n",
            "Epoch 4/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1441 - accuracy: 0.9578 - val_loss: 0.1024 - val_accuracy: 0.9706\n",
            "Epoch 5/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1266 - accuracy: 0.9621 - val_loss: 0.1021 - val_accuracy: 0.9722\n",
            "Epoch 6/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1164 - accuracy: 0.9653 - val_loss: 0.0982 - val_accuracy: 0.9712\n",
            "Epoch 7/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1077 - accuracy: 0.9672 - val_loss: 0.0923 - val_accuracy: 0.9746\n",
            "Epoch 8/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1011 - accuracy: 0.9698 - val_loss: 0.0929 - val_accuracy: 0.9750\n",
            "Epoch 9/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0943 - accuracy: 0.9722 - val_loss: 0.0977 - val_accuracy: 0.9748\n",
            "Epoch 10/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0896 - accuracy: 0.9729 - val_loss: 0.0949 - val_accuracy: 0.9762\n",
            "Epoch 11/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0854 - accuracy: 0.9744 - val_loss: 0.0953 - val_accuracy: 0.9753\n",
            "Epoch 12/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0847 - accuracy: 0.9750 - val_loss: 0.0892 - val_accuracy: 0.9770\n",
            "Epoch 13/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0781 - accuracy: 0.9765 - val_loss: 0.1001 - val_accuracy: 0.9747\n",
            "Epoch 14/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0762 - accuracy: 0.9774 - val_loss: 0.1066 - val_accuracy: 0.9750\n",
            "Epoch 15/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0784 - accuracy: 0.9768 - val_loss: 0.0969 - val_accuracy: 0.9775\n",
            "Epoch 16/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0730 - accuracy: 0.9788 - val_loss: 0.1024 - val_accuracy: 0.9766\n",
            "Epoch 17/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0700 - accuracy: 0.9794 - val_loss: 0.1058 - val_accuracy: 0.9778\n",
            "Epoch 18/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0719 - accuracy: 0.9791 - val_loss: 0.1057 - val_accuracy: 0.9771\n",
            "Epoch 19/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0677 - accuracy: 0.9803 - val_loss: 0.1024 - val_accuracy: 0.9764\n",
            "Epoch 20/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0660 - accuracy: 0.9803 - val_loss: 0.1109 - val_accuracy: 0.9765\n",
            "Epoch 21/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.1099 - val_accuracy: 0.9776\n",
            "Epoch 22/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0641 - accuracy: 0.9815 - val_loss: 0.1115 - val_accuracy: 0.9780\n",
            "Epoch 23/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0652 - accuracy: 0.9811 - val_loss: 0.1107 - val_accuracy: 0.9775\n",
            "Epoch 24/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0634 - accuracy: 0.9821 - val_loss: 0.1167 - val_accuracy: 0.9778\n",
            "Epoch 25/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0615 - accuracy: 0.9820 - val_loss: 0.1144 - val_accuracy: 0.9787\n",
            "Epoch 26/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0591 - accuracy: 0.9831 - val_loss: 0.1255 - val_accuracy: 0.9768\n",
            "Epoch 27/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0594 - accuracy: 0.9828 - val_loss: 0.1147 - val_accuracy: 0.9785\n",
            "Epoch 28/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0612 - accuracy: 0.9825 - val_loss: 0.1177 - val_accuracy: 0.9772\n",
            "Epoch 29/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.1212 - val_accuracy: 0.9789\n",
            "Epoch 30/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0613 - accuracy: 0.9834 - val_loss: 0.1225 - val_accuracy: 0.9793\n",
            "Epoch 31/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0563 - accuracy: 0.9844 - val_loss: 0.1271 - val_accuracy: 0.9778\n",
            "Epoch 32/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0587 - accuracy: 0.9831 - val_loss: 0.1324 - val_accuracy: 0.9775\n",
            "Epoch 33/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0608 - accuracy: 0.9830 - val_loss: 0.1312 - val_accuracy: 0.9776\n",
            "Epoch 34/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0537 - accuracy: 0.9845 - val_loss: 0.1320 - val_accuracy: 0.9777\n",
            "Epoch 35/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0552 - accuracy: 0.9849 - val_loss: 0.1392 - val_accuracy: 0.9784\n",
            "Epoch 36/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0567 - accuracy: 0.9847 - val_loss: 0.1315 - val_accuracy: 0.9776\n",
            "Epoch 37/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0574 - accuracy: 0.9844 - val_loss: 0.1346 - val_accuracy: 0.9783\n",
            "Epoch 38/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0536 - accuracy: 0.9850 - val_loss: 0.1373 - val_accuracy: 0.9774\n",
            "Epoch 39/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0548 - accuracy: 0.9843 - val_loss: 0.1409 - val_accuracy: 0.9777\n",
            "Epoch 40/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0501 - accuracy: 0.9857 - val_loss: 0.1442 - val_accuracy: 0.9772\n",
            "Epoch 41/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0495 - accuracy: 0.9860 - val_loss: 0.1464 - val_accuracy: 0.9778\n",
            "Epoch 42/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0536 - accuracy: 0.9857 - val_loss: 0.1454 - val_accuracy: 0.9769\n",
            "Epoch 43/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0528 - accuracy: 0.9858 - val_loss: 0.1444 - val_accuracy: 0.9771\n",
            "Epoch 44/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0535 - accuracy: 0.9859 - val_loss: 0.1394 - val_accuracy: 0.9786\n",
            "Epoch 45/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0537 - accuracy: 0.9853 - val_loss: 0.1436 - val_accuracy: 0.9773\n",
            "Epoch 46/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0529 - accuracy: 0.9860 - val_loss: 0.1508 - val_accuracy: 0.9770\n",
            "Epoch 47/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0520 - accuracy: 0.9861 - val_loss: 0.1481 - val_accuracy: 0.9788\n",
            "Epoch 48/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0533 - accuracy: 0.9865 - val_loss: 0.1596 - val_accuracy: 0.9780\n",
            "Epoch 49/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9864 - val_loss: 0.1542 - val_accuracy: 0.9770\n",
            "Epoch 50/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0530 - accuracy: 0.9859 - val_loss: 0.1546 - val_accuracy: 0.9773\n",
            "Epoch 51/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0486 - accuracy: 0.9867 - val_loss: 0.1591 - val_accuracy: 0.9770\n",
            "Epoch 52/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0509 - accuracy: 0.9867 - val_loss: 0.1695 - val_accuracy: 0.9786\n",
            "Epoch 53/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0517 - accuracy: 0.9869 - val_loss: 0.1701 - val_accuracy: 0.9776\n",
            "Epoch 54/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0512 - accuracy: 0.9868 - val_loss: 0.1718 - val_accuracy: 0.9785\n",
            "Epoch 55/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0518 - accuracy: 0.9862 - val_loss: 0.1691 - val_accuracy: 0.9783\n",
            "Epoch 56/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0519 - accuracy: 0.9868 - val_loss: 0.1650 - val_accuracy: 0.9787\n",
            "Epoch 57/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0494 - accuracy: 0.9870 - val_loss: 0.1722 - val_accuracy: 0.9774\n",
            "Epoch 58/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0488 - accuracy: 0.9874 - val_loss: 0.1675 - val_accuracy: 0.9765\n",
            "Epoch 59/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0532 - accuracy: 0.9867 - val_loss: 0.1755 - val_accuracy: 0.9763\n",
            "Epoch 60/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0498 - accuracy: 0.9874 - val_loss: 0.1667 - val_accuracy: 0.9774\n",
            "Epoch 61/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0515 - accuracy: 0.9871 - val_loss: 0.1746 - val_accuracy: 0.9778\n",
            "Epoch 62/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0544 - accuracy: 0.9870 - val_loss: 0.1843 - val_accuracy: 0.9776\n",
            "Epoch 63/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0517 - accuracy: 0.9872 - val_loss: 0.1711 - val_accuracy: 0.9780\n",
            "Epoch 64/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0524 - accuracy: 0.9872 - val_loss: 0.1784 - val_accuracy: 0.9782\n",
            "Epoch 65/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0479 - accuracy: 0.9883 - val_loss: 0.1772 - val_accuracy: 0.9777\n",
            "Epoch 66/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0543 - accuracy: 0.9867 - val_loss: 0.1742 - val_accuracy: 0.9792\n",
            "Epoch 67/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0472 - accuracy: 0.9875 - val_loss: 0.1713 - val_accuracy: 0.9778\n",
            "Epoch 68/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0490 - accuracy: 0.9876 - val_loss: 0.1752 - val_accuracy: 0.9783\n",
            "Epoch 69/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0500 - accuracy: 0.9876 - val_loss: 0.1817 - val_accuracy: 0.9770\n",
            "Epoch 70/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0508 - accuracy: 0.9871 - val_loss: 0.1845 - val_accuracy: 0.9783\n",
            "Epoch 71/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0502 - accuracy: 0.9874 - val_loss: 0.1841 - val_accuracy: 0.9790\n",
            "Epoch 72/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0499 - accuracy: 0.9870 - val_loss: 0.1892 - val_accuracy: 0.9786\n",
            "Epoch 73/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0495 - accuracy: 0.9881 - val_loss: 0.1906 - val_accuracy: 0.9783\n",
            "Epoch 74/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0504 - accuracy: 0.9885 - val_loss: 0.1891 - val_accuracy: 0.9781\n",
            "Epoch 75/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0495 - accuracy: 0.9884 - val_loss: 0.1931 - val_accuracy: 0.9783\n",
            "Epoch 76/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0541 - accuracy: 0.9875 - val_loss: 0.2012 - val_accuracy: 0.9783\n",
            "Epoch 77/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0515 - accuracy: 0.9873 - val_loss: 0.1937 - val_accuracy: 0.9786\n",
            "Epoch 78/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0492 - accuracy: 0.9879 - val_loss: 0.1891 - val_accuracy: 0.9785\n",
            "Epoch 79/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0487 - accuracy: 0.9880 - val_loss: 0.1896 - val_accuracy: 0.9785\n",
            "Epoch 80/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0488 - accuracy: 0.9877 - val_loss: 0.1936 - val_accuracy: 0.9787\n",
            "Epoch 81/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0487 - accuracy: 0.9881 - val_loss: 0.2039 - val_accuracy: 0.9771\n",
            "Epoch 82/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0484 - accuracy: 0.9883 - val_loss: 0.2027 - val_accuracy: 0.9778\n",
            "Epoch 83/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0502 - accuracy: 0.9871 - val_loss: 0.2116 - val_accuracy: 0.9760\n",
            "Epoch 84/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0451 - accuracy: 0.9896 - val_loss: 0.2022 - val_accuracy: 0.9783\n",
            "Epoch 85/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0471 - accuracy: 0.9885 - val_loss: 0.2204 - val_accuracy: 0.9771\n",
            "Epoch 86/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0513 - accuracy: 0.9888 - val_loss: 0.2009 - val_accuracy: 0.9784\n",
            "Epoch 87/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0492 - accuracy: 0.9892 - val_loss: 0.2179 - val_accuracy: 0.9768\n",
            "Epoch 88/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0526 - accuracy: 0.9880 - val_loss: 0.2050 - val_accuracy: 0.9779\n",
            "Epoch 89/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0486 - accuracy: 0.9883 - val_loss: 0.2084 - val_accuracy: 0.9793\n",
            "Epoch 90/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0486 - accuracy: 0.9885 - val_loss: 0.2157 - val_accuracy: 0.9778\n",
            "Epoch 91/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0508 - accuracy: 0.9889 - val_loss: 0.2119 - val_accuracy: 0.9790\n",
            "Epoch 92/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.2166 - val_accuracy: 0.9782\n",
            "Epoch 93/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0473 - accuracy: 0.9891 - val_loss: 0.2086 - val_accuracy: 0.9781\n",
            "Epoch 94/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0514 - accuracy: 0.9889 - val_loss: 0.2229 - val_accuracy: 0.9783\n",
            "Epoch 95/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0465 - accuracy: 0.9887 - val_loss: 0.2024 - val_accuracy: 0.9789\n",
            "Epoch 96/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0460 - accuracy: 0.9889 - val_loss: 0.2178 - val_accuracy: 0.9766\n",
            "Epoch 97/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9881 - val_loss: 0.2112 - val_accuracy: 0.9777\n",
            "Epoch 98/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0482 - accuracy: 0.9887 - val_loss: 0.2308 - val_accuracy: 0.9778\n",
            "Epoch 99/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0464 - accuracy: 0.9895 - val_loss: 0.2277 - val_accuracy: 0.9781\n",
            "Epoch 100/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0469 - accuracy: 0.9889 - val_loss: 0.2248 - val_accuracy: 0.9787\n",
            "Epoch 101/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0477 - accuracy: 0.9885 - val_loss: 0.2183 - val_accuracy: 0.9787\n",
            "Epoch 102/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9886 - val_loss: 0.2252 - val_accuracy: 0.9787\n",
            "Epoch 103/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0509 - accuracy: 0.9884 - val_loss: 0.2271 - val_accuracy: 0.9783\n",
            "Epoch 104/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0491 - accuracy: 0.9883 - val_loss: 0.2088 - val_accuracy: 0.9778\n",
            "Epoch 105/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0491 - accuracy: 0.9892 - val_loss: 0.2161 - val_accuracy: 0.9796\n",
            "Epoch 106/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0484 - accuracy: 0.9888 - val_loss: 0.2242 - val_accuracy: 0.9784\n",
            "Epoch 107/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0468 - accuracy: 0.9898 - val_loss: 0.2480 - val_accuracy: 0.9780\n",
            "Epoch 108/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0465 - accuracy: 0.9894 - val_loss: 0.2319 - val_accuracy: 0.9787\n",
            "Epoch 109/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9885 - val_loss: 0.2284 - val_accuracy: 0.9782\n",
            "Epoch 110/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0542 - accuracy: 0.9887 - val_loss: 0.2440 - val_accuracy: 0.9781\n",
            "Epoch 111/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0457 - accuracy: 0.9898 - val_loss: 0.2539 - val_accuracy: 0.9778\n",
            "Epoch 112/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0530 - accuracy: 0.9891 - val_loss: 0.2420 - val_accuracy: 0.9780\n",
            "Epoch 113/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0517 - accuracy: 0.9891 - val_loss: 0.2342 - val_accuracy: 0.9782\n",
            "Epoch 114/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0475 - accuracy: 0.9893 - val_loss: 0.2633 - val_accuracy: 0.9770\n",
            "Epoch 115/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0526 - accuracy: 0.9884 - val_loss: 0.2358 - val_accuracy: 0.9782\n",
            "Epoch 116/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.2522 - val_accuracy: 0.9778\n",
            "Epoch 117/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0453 - accuracy: 0.9896 - val_loss: 0.2461 - val_accuracy: 0.9778\n",
            "Epoch 118/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0502 - accuracy: 0.9884 - val_loss: 0.2590 - val_accuracy: 0.9772\n",
            "Epoch 119/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0503 - accuracy: 0.9892 - val_loss: 0.2569 - val_accuracy: 0.9782\n",
            "Epoch 120/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0478 - accuracy: 0.9893 - val_loss: 0.2417 - val_accuracy: 0.9777\n",
            "Epoch 121/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.2382 - val_accuracy: 0.9785\n",
            "Epoch 122/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0507 - accuracy: 0.9889 - val_loss: 0.2517 - val_accuracy: 0.9778\n",
            "Epoch 123/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0494 - accuracy: 0.9887 - val_loss: 0.2735 - val_accuracy: 0.9790\n",
            "Epoch 124/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9891 - val_loss: 0.2580 - val_accuracy: 0.9767\n",
            "Epoch 125/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0502 - accuracy: 0.9892 - val_loss: 0.2529 - val_accuracy: 0.9772\n",
            "Epoch 126/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0488 - accuracy: 0.9893 - val_loss: 0.2534 - val_accuracy: 0.9777\n",
            "Epoch 127/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0513 - accuracy: 0.9899 - val_loss: 0.2677 - val_accuracy: 0.9778\n",
            "Epoch 128/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0512 - accuracy: 0.9888 - val_loss: 0.2534 - val_accuracy: 0.9772\n",
            "Epoch 129/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0499 - accuracy: 0.9893 - val_loss: 0.2713 - val_accuracy: 0.9783\n",
            "Epoch 130/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0515 - accuracy: 0.9891 - val_loss: 0.2470 - val_accuracy: 0.9785\n",
            "Epoch 131/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0517 - accuracy: 0.9899 - val_loss: 0.2589 - val_accuracy: 0.9778\n",
            "Epoch 132/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0515 - accuracy: 0.9895 - val_loss: 0.2538 - val_accuracy: 0.9778\n",
            "Epoch 133/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0513 - accuracy: 0.9890 - val_loss: 0.2572 - val_accuracy: 0.9783\n",
            "Epoch 134/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0496 - accuracy: 0.9891 - val_loss: 0.2437 - val_accuracy: 0.9787\n",
            "Epoch 135/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0479 - accuracy: 0.9897 - val_loss: 0.2594 - val_accuracy: 0.9778\n",
            "Epoch 136/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0533 - accuracy: 0.9897 - val_loss: 0.2485 - val_accuracy: 0.9783\n",
            "Epoch 137/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0471 - accuracy: 0.9893 - val_loss: 0.2631 - val_accuracy: 0.9778\n",
            "Epoch 138/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0485 - accuracy: 0.9898 - val_loss: 0.2795 - val_accuracy: 0.9766\n",
            "Epoch 139/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0512 - accuracy: 0.9899 - val_loss: 0.2596 - val_accuracy: 0.9787\n",
            "Epoch 140/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0461 - accuracy: 0.9901 - val_loss: 0.2809 - val_accuracy: 0.9785\n",
            "Epoch 141/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0461 - accuracy: 0.9895 - val_loss: 0.2836 - val_accuracy: 0.9779\n",
            "Epoch 142/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0472 - accuracy: 0.9894 - val_loss: 0.2619 - val_accuracy: 0.9783\n",
            "Epoch 143/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0490 - accuracy: 0.9896 - val_loss: 0.2807 - val_accuracy: 0.9786\n",
            "Epoch 144/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0528 - accuracy: 0.9892 - val_loss: 0.2793 - val_accuracy: 0.9787\n",
            "Epoch 145/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0473 - accuracy: 0.9901 - val_loss: 0.2912 - val_accuracy: 0.9778\n",
            "Epoch 146/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0487 - accuracy: 0.9899 - val_loss: 0.2673 - val_accuracy: 0.9770\n",
            "Epoch 147/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0510 - accuracy: 0.9894 - val_loss: 0.2909 - val_accuracy: 0.9781\n",
            "Epoch 148/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0479 - accuracy: 0.9899 - val_loss: 0.2803 - val_accuracy: 0.9772\n",
            "Epoch 149/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0458 - accuracy: 0.9901 - val_loss: 0.2866 - val_accuracy: 0.9770\n",
            "Epoch 150/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0523 - accuracy: 0.9906 - val_loss: 0.2758 - val_accuracy: 0.9778\n",
            "Epoch 151/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0452 - accuracy: 0.9906 - val_loss: 0.2809 - val_accuracy: 0.9775\n",
            "Epoch 152/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0483 - accuracy: 0.9895 - val_loss: 0.2948 - val_accuracy: 0.9779\n",
            "Epoch 153/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0503 - accuracy: 0.9893 - val_loss: 0.3077 - val_accuracy: 0.9772\n",
            "Epoch 154/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0512 - accuracy: 0.9892 - val_loss: 0.2904 - val_accuracy: 0.9756\n",
            "Epoch 155/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0542 - accuracy: 0.9891 - val_loss: 0.2933 - val_accuracy: 0.9772\n",
            "Epoch 156/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0545 - accuracy: 0.9894 - val_loss: 0.2844 - val_accuracy: 0.9772\n",
            "Epoch 157/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0489 - accuracy: 0.9896 - val_loss: 0.3064 - val_accuracy: 0.9776\n",
            "Epoch 158/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0513 - accuracy: 0.9894 - val_loss: 0.2901 - val_accuracy: 0.9771\n",
            "Epoch 159/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0468 - accuracy: 0.9899 - val_loss: 0.3019 - val_accuracy: 0.9774\n",
            "Epoch 160/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0518 - accuracy: 0.9894 - val_loss: 0.2845 - val_accuracy: 0.9765\n",
            "Epoch 161/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0573 - accuracy: 0.9895 - val_loss: 0.2732 - val_accuracy: 0.9775\n",
            "Epoch 162/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0506 - accuracy: 0.9903 - val_loss: 0.2871 - val_accuracy: 0.9773\n",
            "Epoch 163/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0476 - accuracy: 0.9900 - val_loss: 0.2985 - val_accuracy: 0.9783\n",
            "Epoch 164/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0507 - accuracy: 0.9895 - val_loss: 0.2793 - val_accuracy: 0.9779\n",
            "Epoch 165/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0527 - accuracy: 0.9899 - val_loss: 0.2981 - val_accuracy: 0.9773\n",
            "Epoch 166/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0487 - accuracy: 0.9899 - val_loss: 0.2914 - val_accuracy: 0.9766\n",
            "Epoch 167/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0508 - accuracy: 0.9898 - val_loss: 0.2976 - val_accuracy: 0.9772\n",
            "Epoch 168/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0572 - accuracy: 0.9889 - val_loss: 0.2939 - val_accuracy: 0.9787\n",
            "Epoch 169/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0493 - accuracy: 0.9895 - val_loss: 0.3115 - val_accuracy: 0.9775\n",
            "Epoch 170/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0517 - accuracy: 0.9900 - val_loss: 0.3144 - val_accuracy: 0.9772\n",
            "Epoch 171/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0475 - accuracy: 0.9900 - val_loss: 0.2998 - val_accuracy: 0.9777\n",
            "Epoch 172/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0519 - accuracy: 0.9895 - val_loss: 0.2843 - val_accuracy: 0.9773\n",
            "Epoch 173/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0474 - accuracy: 0.9905 - val_loss: 0.3195 - val_accuracy: 0.9784\n",
            "Epoch 174/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0543 - accuracy: 0.9898 - val_loss: 0.3268 - val_accuracy: 0.9774\n",
            "Epoch 175/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0513 - accuracy: 0.9902 - val_loss: 0.2970 - val_accuracy: 0.9774\n",
            "Epoch 176/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0546 - accuracy: 0.9891 - val_loss: 0.2973 - val_accuracy: 0.9779\n",
            "Epoch 177/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0479 - accuracy: 0.9900 - val_loss: 0.3136 - val_accuracy: 0.9770\n",
            "Epoch 178/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0502 - accuracy: 0.9903 - val_loss: 0.3171 - val_accuracy: 0.9773\n",
            "Epoch 179/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0512 - accuracy: 0.9897 - val_loss: 0.3055 - val_accuracy: 0.9783\n",
            "Epoch 180/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0487 - accuracy: 0.9905 - val_loss: 0.3089 - val_accuracy: 0.9771\n",
            "Epoch 181/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0497 - accuracy: 0.9899 - val_loss: 0.3185 - val_accuracy: 0.9772\n",
            "Epoch 182/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0500 - accuracy: 0.9908 - val_loss: 0.3194 - val_accuracy: 0.9772\n",
            "Epoch 183/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0544 - accuracy: 0.9903 - val_loss: 0.3134 - val_accuracy: 0.9780\n",
            "Epoch 184/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0535 - accuracy: 0.9903 - val_loss: 0.3083 - val_accuracy: 0.9783\n",
            "Epoch 185/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0522 - accuracy: 0.9898 - val_loss: 0.3099 - val_accuracy: 0.9776\n",
            "Epoch 186/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0436 - accuracy: 0.9906 - val_loss: 0.3151 - val_accuracy: 0.9779\n",
            "Epoch 187/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0523 - accuracy: 0.9899 - val_loss: 0.3263 - val_accuracy: 0.9782\n",
            "Epoch 188/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0503 - accuracy: 0.9899 - val_loss: 0.3169 - val_accuracy: 0.9779\n",
            "Epoch 189/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0487 - accuracy: 0.9895 - val_loss: 0.3238 - val_accuracy: 0.9783\n",
            "Epoch 190/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0498 - accuracy: 0.9901 - val_loss: 0.3173 - val_accuracy: 0.9786\n",
            "Epoch 191/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0547 - accuracy: 0.9893 - val_loss: 0.3222 - val_accuracy: 0.9771\n",
            "Epoch 192/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0575 - accuracy: 0.9893 - val_loss: 0.3239 - val_accuracy: 0.9778\n",
            "Epoch 193/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0562 - accuracy: 0.9899 - val_loss: 0.3210 - val_accuracy: 0.9772\n",
            "Epoch 194/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0519 - accuracy: 0.9895 - val_loss: 0.3297 - val_accuracy: 0.9774\n",
            "Epoch 195/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0499 - accuracy: 0.9896 - val_loss: 0.3338 - val_accuracy: 0.9776\n",
            "Epoch 196/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0530 - accuracy: 0.9907 - val_loss: 0.3308 - val_accuracy: 0.9781\n",
            "Epoch 197/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0516 - accuracy: 0.9901 - val_loss: 0.3165 - val_accuracy: 0.9774\n",
            "Epoch 198/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9896 - val_loss: 0.3340 - val_accuracy: 0.9768\n",
            "Epoch 199/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0502 - accuracy: 0.9894 - val_loss: 0.3326 - val_accuracy: 0.9780\n",
            "Epoch 200/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0546 - accuracy: 0.9898 - val_loss: 0.3365 - val_accuracy: 0.9769\n",
            "Epoch 201/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0598 - accuracy: 0.9893 - val_loss: 0.3195 - val_accuracy: 0.9777\n",
            "Epoch 202/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0593 - accuracy: 0.9898 - val_loss: 0.3216 - val_accuracy: 0.9774\n",
            "Epoch 203/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9904 - val_loss: 0.3388 - val_accuracy: 0.9791\n",
            "Epoch 204/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0502 - accuracy: 0.9906 - val_loss: 0.3420 - val_accuracy: 0.9787\n",
            "Epoch 205/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0493 - accuracy: 0.9903 - val_loss: 0.3400 - val_accuracy: 0.9786\n",
            "Epoch 206/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0514 - accuracy: 0.9902 - val_loss: 0.3366 - val_accuracy: 0.9776\n",
            "Epoch 207/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0525 - accuracy: 0.9904 - val_loss: 0.3446 - val_accuracy: 0.9776\n",
            "Epoch 208/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0506 - accuracy: 0.9894 - val_loss: 0.3189 - val_accuracy: 0.9786\n",
            "Epoch 209/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0535 - accuracy: 0.9901 - val_loss: 0.3269 - val_accuracy: 0.9782\n",
            "Epoch 210/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0482 - accuracy: 0.9909 - val_loss: 0.3197 - val_accuracy: 0.9788\n",
            "Epoch 211/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0489 - accuracy: 0.9905 - val_loss: 0.3539 - val_accuracy: 0.9776\n",
            "Epoch 212/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0519 - accuracy: 0.9905 - val_loss: 0.3466 - val_accuracy: 0.9772\n",
            "Epoch 213/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0544 - accuracy: 0.9896 - val_loss: 0.3328 - val_accuracy: 0.9780\n",
            "Epoch 214/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0464 - accuracy: 0.9902 - val_loss: 0.3323 - val_accuracy: 0.9768\n",
            "Epoch 215/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0597 - accuracy: 0.9899 - val_loss: 0.3285 - val_accuracy: 0.9783\n",
            "Epoch 216/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0570 - accuracy: 0.9901 - val_loss: 0.3219 - val_accuracy: 0.9778\n",
            "Epoch 217/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0531 - accuracy: 0.9898 - val_loss: 0.3388 - val_accuracy: 0.9776\n",
            "Epoch 218/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0531 - accuracy: 0.9898 - val_loss: 0.3211 - val_accuracy: 0.9772\n",
            "Epoch 219/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0520 - accuracy: 0.9903 - val_loss: 0.3494 - val_accuracy: 0.9772\n",
            "Epoch 220/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0523 - accuracy: 0.9900 - val_loss: 0.3311 - val_accuracy: 0.9766\n",
            "Epoch 221/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0559 - accuracy: 0.9904 - val_loss: 0.3377 - val_accuracy: 0.9780\n",
            "Epoch 222/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0514 - accuracy: 0.9900 - val_loss: 0.3539 - val_accuracy: 0.9781\n",
            "Epoch 223/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0530 - accuracy: 0.9904 - val_loss: 0.3422 - val_accuracy: 0.9773\n",
            "Epoch 224/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0524 - accuracy: 0.9902 - val_loss: 0.3501 - val_accuracy: 0.9756\n",
            "Epoch 225/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0532 - accuracy: 0.9900 - val_loss: 0.3464 - val_accuracy: 0.9778\n",
            "Epoch 226/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0568 - accuracy: 0.9898 - val_loss: 0.3606 - val_accuracy: 0.9780\n",
            "Epoch 227/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0561 - accuracy: 0.9896 - val_loss: 0.3393 - val_accuracy: 0.9770\n",
            "Epoch 228/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0534 - accuracy: 0.9896 - val_loss: 0.3581 - val_accuracy: 0.9775\n",
            "Epoch 229/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0555 - accuracy: 0.9901 - val_loss: 0.3585 - val_accuracy: 0.9766\n",
            "Epoch 230/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0541 - accuracy: 0.9901 - val_loss: 0.3643 - val_accuracy: 0.9765\n",
            "Epoch 231/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0516 - accuracy: 0.9902 - val_loss: 0.3562 - val_accuracy: 0.9784\n",
            "Epoch 232/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0505 - accuracy: 0.9904 - val_loss: 0.3587 - val_accuracy: 0.9777\n",
            "Epoch 233/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0565 - accuracy: 0.9895 - val_loss: 0.3536 - val_accuracy: 0.9774\n",
            "Epoch 234/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0610 - accuracy: 0.9896 - val_loss: 0.3908 - val_accuracy: 0.9760\n",
            "Epoch 235/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0526 - accuracy: 0.9909 - val_loss: 0.3734 - val_accuracy: 0.9776\n",
            "Epoch 236/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0535 - accuracy: 0.9902 - val_loss: 0.3501 - val_accuracy: 0.9775\n",
            "Epoch 237/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0550 - accuracy: 0.9904 - val_loss: 0.3531 - val_accuracy: 0.9766\n",
            "Epoch 238/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0568 - accuracy: 0.9893 - val_loss: 0.3501 - val_accuracy: 0.9772\n",
            "Epoch 239/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0546 - accuracy: 0.9900 - val_loss: 0.3795 - val_accuracy: 0.9770\n",
            "Epoch 240/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0563 - accuracy: 0.9898 - val_loss: 0.3313 - val_accuracy: 0.9780\n",
            "Epoch 241/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0520 - accuracy: 0.9898 - val_loss: 0.3428 - val_accuracy: 0.9780\n",
            "Epoch 242/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0519 - accuracy: 0.9905 - val_loss: 0.3776 - val_accuracy: 0.9786\n",
            "Epoch 243/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0580 - accuracy: 0.9900 - val_loss: 0.3483 - val_accuracy: 0.9768\n",
            "Epoch 244/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0557 - accuracy: 0.9900 - val_loss: 0.3699 - val_accuracy: 0.9762\n",
            "Epoch 245/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0576 - accuracy: 0.9902 - val_loss: 0.3816 - val_accuracy: 0.9781\n",
            "Epoch 246/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0541 - accuracy: 0.9908 - val_loss: 0.3677 - val_accuracy: 0.9778\n",
            "Epoch 247/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0580 - accuracy: 0.9895 - val_loss: 0.3821 - val_accuracy: 0.9776\n",
            "Epoch 248/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0505 - accuracy: 0.9905 - val_loss: 0.3796 - val_accuracy: 0.9779\n",
            "Epoch 249/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0545 - accuracy: 0.9900 - val_loss: 0.3499 - val_accuracy: 0.9787\n",
            "Epoch 250/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0556 - accuracy: 0.9898 - val_loss: 0.3573 - val_accuracy: 0.9774\n",
            "Test accuracy:  0.977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtpnHuzHpaTE"
      },
      "source": [
        "- RMSDropÏùÑ Ïù¥Ïö©Ìï† Îïå ÏóêÌè≠ ÏàòÍ∞Ä Ï¶ùÍ∞ÄÌï† Îïå ÌõàÎ†®Í≥º ÌÖåÏä§Ìä∏ ÏßëÌï©ÏóêÏÑú Ï†ïÌôïÎèÑÍ∞Ä Ïñ¥ÎñªÍ≤å Ï¶ùÍ∞ÄÌïòÎäîÏßÄÎäî Îã§ÏùåÍ≥º Í∞ôÎã§.\n",
        "- -> ÏïΩ 15ÏóêÌè≠ÏóêÏÑú ÏÑúÎ°ú ÎßûÎãøÍ≥† Í∑∏ Ïù¥ÌõÑÏóêÎäî ÌõàÎ†®Ìï† ÌïÑÏöîÍ∞Ä ÏóÜÎã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117957768-58e61d80-b355-11eb-9ee2-9bc8bca20594.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ahhKL_paTE",
        "outputId": "2867dc95-84a6-4fef-eaf0-531801c4432c"
      },
      "source": [
        "# Adam, epochs = 200\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ïã†Í≤ΩÎßùÍ≥º ÌõàÎ†®\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # Ï∂úÎ†• Í∞úÏàò = Ïà´Ïûê Í∞úÏàò\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATIONÏùÑ ÏúÑÌï¥ ÏòàÏïΩÎêú TRAINÏùò Ïñë\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "# Î†àÏù¥Î∏îÏùÄ ÏõêÌï´ Ïù∏ÏΩîÎî©ÏúºÎ°ú ÌëúÌòÑ\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_trainÏùÄ 60000Í∞ú ÌñâÏùò 28*28 Í∞í -> 60000*784 ÌòïÌÉúÎ°ú Î≥ÄÍ≤Ω\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# ÏûÖÎ†•ÏùÑ [0, 1] ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Î™®Îç∏ Íµ¨Ï∂ï\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# Î™®Îç∏ ÏöîÏïΩ\n",
        "model.summary()\n",
        "\n",
        "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
        "# Î≥ÄÍ≤Ω Î∂ÄÎ∂Ñ\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "47488/48000 [============================>.] - ETA: 0s - loss: 0.5206 - accuracy: 0.8404"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.5183 - accuracy: 0.8411 - val_loss: 0.1805 - val_accuracy: 0.9489\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.2319 - accuracy: 0.9307 - val_loss: 0.1401 - val_accuracy: 0.9603\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1802 - accuracy: 0.9454 - val_loss: 0.1153 - val_accuracy: 0.9663\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1468 - accuracy: 0.9564 - val_loss: 0.1036 - val_accuracy: 0.9672\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1286 - accuracy: 0.9615 - val_loss: 0.1025 - val_accuracy: 0.9682\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1173 - accuracy: 0.9644 - val_loss: 0.0934 - val_accuracy: 0.9725\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1081 - accuracy: 0.9673 - val_loss: 0.0840 - val_accuracy: 0.9760\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0983 - accuracy: 0.9693 - val_loss: 0.0861 - val_accuracy: 0.9744\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0890 - accuracy: 0.9720 - val_loss: 0.0847 - val_accuracy: 0.9759\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0834 - accuracy: 0.9741 - val_loss: 0.0838 - val_accuracy: 0.9766\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0795 - accuracy: 0.9751 - val_loss: 0.0791 - val_accuracy: 0.9769\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0732 - accuracy: 0.9776 - val_loss: 0.0806 - val_accuracy: 0.9781\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0708 - accuracy: 0.9775 - val_loss: 0.0784 - val_accuracy: 0.9780\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0673 - accuracy: 0.9787 - val_loss: 0.0827 - val_accuracy: 0.9769\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0634 - accuracy: 0.9791 - val_loss: 0.0788 - val_accuracy: 0.9787\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0616 - accuracy: 0.9801 - val_loss: 0.0841 - val_accuracy: 0.9791\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0590 - accuracy: 0.9799 - val_loss: 0.0791 - val_accuracy: 0.9784\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0568 - accuracy: 0.9818 - val_loss: 0.0815 - val_accuracy: 0.9780\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0531 - accuracy: 0.9823 - val_loss: 0.0798 - val_accuracy: 0.9799\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0543 - accuracy: 0.9820 - val_loss: 0.0779 - val_accuracy: 0.9783\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0540 - accuracy: 0.9820 - val_loss: 0.0815 - val_accuracy: 0.9785\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0492 - accuracy: 0.9839 - val_loss: 0.0820 - val_accuracy: 0.9783\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0499 - accuracy: 0.9833 - val_loss: 0.0816 - val_accuracy: 0.9779\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0476 - accuracy: 0.9840 - val_loss: 0.0811 - val_accuracy: 0.9794\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0449 - accuracy: 0.9856 - val_loss: 0.0810 - val_accuracy: 0.9787\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0446 - accuracy: 0.9848 - val_loss: 0.0814 - val_accuracy: 0.9795\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0427 - accuracy: 0.9850 - val_loss: 0.0828 - val_accuracy: 0.9808\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0434 - accuracy: 0.9859 - val_loss: 0.0843 - val_accuracy: 0.9805\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0421 - accuracy: 0.9859 - val_loss: 0.0844 - val_accuracy: 0.9799\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0418 - accuracy: 0.9863 - val_loss: 0.0848 - val_accuracy: 0.9801\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.0823 - val_accuracy: 0.9803\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0395 - accuracy: 0.9871 - val_loss: 0.0812 - val_accuracy: 0.9806\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0345 - accuracy: 0.9886 - val_loss: 0.0818 - val_accuracy: 0.9808\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0378 - accuracy: 0.9872 - val_loss: 0.0875 - val_accuracy: 0.9791\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0397 - accuracy: 0.9864 - val_loss: 0.0813 - val_accuracy: 0.9797\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.0834 - val_accuracy: 0.9807\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0372 - accuracy: 0.9873 - val_loss: 0.0843 - val_accuracy: 0.9802\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0381 - accuracy: 0.9870 - val_loss: 0.0863 - val_accuracy: 0.9796\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.0900 - val_accuracy: 0.9785\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.0878 - val_accuracy: 0.9793\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0326 - accuracy: 0.9885 - val_loss: 0.0901 - val_accuracy: 0.9802\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.0895 - val_accuracy: 0.9793\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0373 - accuracy: 0.9874 - val_loss: 0.0875 - val_accuracy: 0.9803\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0344 - accuracy: 0.9886 - val_loss: 0.0894 - val_accuracy: 0.9794\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0327 - accuracy: 0.9897 - val_loss: 0.0849 - val_accuracy: 0.9807\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.0887 - val_accuracy: 0.9797\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0308 - accuracy: 0.9894 - val_loss: 0.0886 - val_accuracy: 0.9799\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.0893 - val_accuracy: 0.9807\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.0888 - val_accuracy: 0.9795\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0283 - accuracy: 0.9902 - val_loss: 0.1004 - val_accuracy: 0.9793\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.0968 - val_accuracy: 0.9793\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0296 - accuracy: 0.9899 - val_loss: 0.0911 - val_accuracy: 0.9793\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0306 - accuracy: 0.9901 - val_loss: 0.0991 - val_accuracy: 0.9793\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0341 - accuracy: 0.9888 - val_loss: 0.0937 - val_accuracy: 0.9796\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.0935 - val_accuracy: 0.9809\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0297 - accuracy: 0.9909 - val_loss: 0.0913 - val_accuracy: 0.9809\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.0926 - val_accuracy: 0.9812\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.0955 - val_accuracy: 0.9812\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0299 - accuracy: 0.9896 - val_loss: 0.0911 - val_accuracy: 0.9809\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0292 - accuracy: 0.9905 - val_loss: 0.0941 - val_accuracy: 0.9796\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0267 - accuracy: 0.9911 - val_loss: 0.0960 - val_accuracy: 0.9810\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.1006 - val_accuracy: 0.9798\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.0969 - val_accuracy: 0.9813\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0294 - accuracy: 0.9896 - val_loss: 0.0967 - val_accuracy: 0.9801\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0977 - val_accuracy: 0.9802\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0261 - accuracy: 0.9907 - val_loss: 0.1002 - val_accuracy: 0.9806\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0281 - accuracy: 0.9911 - val_loss: 0.0998 - val_accuracy: 0.9808\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0268 - accuracy: 0.9912 - val_loss: 0.1066 - val_accuracy: 0.9795\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.0959 - val_accuracy: 0.9818\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.1024 - val_accuracy: 0.9802\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.1051 - val_accuracy: 0.9800\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0265 - accuracy: 0.9916 - val_loss: 0.1062 - val_accuracy: 0.9806\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0244 - accuracy: 0.9914 - val_loss: 0.0996 - val_accuracy: 0.9797\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1005 - val_accuracy: 0.9812\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.1072 - val_accuracy: 0.9798\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.1066 - val_accuracy: 0.9807\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0242 - accuracy: 0.9918 - val_loss: 0.1076 - val_accuracy: 0.9803\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.1044 - val_accuracy: 0.9795\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.1000 - val_accuracy: 0.9797\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0247 - accuracy: 0.9919 - val_loss: 0.1026 - val_accuracy: 0.9800\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0231 - accuracy: 0.9924 - val_loss: 0.1036 - val_accuracy: 0.9799\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0216 - accuracy: 0.9924 - val_loss: 0.0995 - val_accuracy: 0.9813\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.1036 - val_accuracy: 0.9806\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.1087 - val_accuracy: 0.9797\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.1023 - val_accuracy: 0.9803\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0221 - accuracy: 0.9927 - val_loss: 0.1127 - val_accuracy: 0.9793\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.1100 - val_accuracy: 0.9803\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1147 - val_accuracy: 0.9793\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.1060 - val_accuracy: 0.9799\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.1038 - val_accuracy: 0.9789\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0249 - accuracy: 0.9924 - val_loss: 0.1024 - val_accuracy: 0.9808\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0221 - accuracy: 0.9926 - val_loss: 0.1122 - val_accuracy: 0.9802\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0199 - accuracy: 0.9932 - val_loss: 0.0970 - val_accuracy: 0.9818\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0235 - accuracy: 0.9925 - val_loss: 0.1063 - val_accuracy: 0.9802\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0257 - accuracy: 0.9921 - val_loss: 0.1075 - val_accuracy: 0.9802\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.1015 - val_accuracy: 0.9819\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0195 - accuracy: 0.9942 - val_loss: 0.1120 - val_accuracy: 0.9798\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0205 - accuracy: 0.9934 - val_loss: 0.1088 - val_accuracy: 0.9797\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1080 - val_accuracy: 0.9796\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.1196 - val_accuracy: 0.9796\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.1121 - val_accuracy: 0.9800\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0204 - accuracy: 0.9934 - val_loss: 0.1186 - val_accuracy: 0.9788\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.1074 - val_accuracy: 0.9808\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.1107 - val_accuracy: 0.9808\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0234 - accuracy: 0.9927 - val_loss: 0.1101 - val_accuracy: 0.9792\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1093 - val_accuracy: 0.9808\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.1131 - val_accuracy: 0.9803\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.1124 - val_accuracy: 0.9803\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.1070 - val_accuracy: 0.9797\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.1114 - val_accuracy: 0.9807\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0228 - accuracy: 0.9927 - val_loss: 0.1111 - val_accuracy: 0.9808\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0206 - accuracy: 0.9933 - val_loss: 0.1150 - val_accuracy: 0.9791\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.1081 - val_accuracy: 0.9804\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0233 - accuracy: 0.9926 - val_loss: 0.1129 - val_accuracy: 0.9808\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.1033 - val_accuracy: 0.9807\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.1081 - val_accuracy: 0.9798\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.1063 - val_accuracy: 0.9810\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.1108 - val_accuracy: 0.9808\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0217 - accuracy: 0.9935 - val_loss: 0.1102 - val_accuracy: 0.9810\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0226 - accuracy: 0.9930 - val_loss: 0.1084 - val_accuracy: 0.9807\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.1074 - val_accuracy: 0.9803\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.1148 - val_accuracy: 0.9795\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.1080 - val_accuracy: 0.9808\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.1205 - val_accuracy: 0.9808\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.1195 - val_accuracy: 0.9800\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.1159 - val_accuracy: 0.9805\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.1167 - val_accuracy: 0.9805\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0184 - accuracy: 0.9940 - val_loss: 0.1193 - val_accuracy: 0.9793\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.1142 - val_accuracy: 0.9795\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.1103 - val_accuracy: 0.9808\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.1141 - val_accuracy: 0.9812\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.1144 - val_accuracy: 0.9803\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.1150 - val_accuracy: 0.9803\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0180 - accuracy: 0.9941 - val_loss: 0.1196 - val_accuracy: 0.9808\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.1203 - val_accuracy: 0.9797\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1145 - val_accuracy: 0.9804\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.1265 - val_accuracy: 0.9797\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.1231 - val_accuracy: 0.9794\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0186 - accuracy: 0.9943 - val_loss: 0.1178 - val_accuracy: 0.9804\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0192 - accuracy: 0.9934 - val_loss: 0.1245 - val_accuracy: 0.9789\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.1087 - val_accuracy: 0.9810\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1091 - val_accuracy: 0.9815\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1180 - val_accuracy: 0.9804\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.1129 - val_accuracy: 0.9807\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0181 - accuracy: 0.9943 - val_loss: 0.1187 - val_accuracy: 0.9797\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.1247 - val_accuracy: 0.9797\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.1181 - val_accuracy: 0.9791\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.1265 - val_accuracy: 0.9799\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.1165 - val_accuracy: 0.9804\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.1196 - val_accuracy: 0.9797\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.1222 - val_accuracy: 0.9797\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.1184 - val_accuracy: 0.9799\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.1211 - val_accuracy: 0.9791\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.1225 - val_accuracy: 0.9799\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0206 - accuracy: 0.9937 - val_loss: 0.1258 - val_accuracy: 0.9792\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.1144 - val_accuracy: 0.9808\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0195 - accuracy: 0.9937 - val_loss: 0.1239 - val_accuracy: 0.9803\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.1260 - val_accuracy: 0.9798\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.1120 - val_accuracy: 0.9803\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.1193 - val_accuracy: 0.9802\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.1279 - val_accuracy: 0.9789\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1243 - val_accuracy: 0.9799\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0177 - accuracy: 0.9943 - val_loss: 0.1224 - val_accuracy: 0.9803\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.1278 - val_accuracy: 0.9794\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.1210 - val_accuracy: 0.9810\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0156 - accuracy: 0.9953 - val_loss: 0.1258 - val_accuracy: 0.9806\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.1326 - val_accuracy: 0.9787\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.1300 - val_accuracy: 0.9792\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.1159 - val_accuracy: 0.9807\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.1244 - val_accuracy: 0.9806\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.1316 - val_accuracy: 0.9795\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0172 - accuracy: 0.9950 - val_loss: 0.1236 - val_accuracy: 0.9795\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.1335 - val_accuracy: 0.9784\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1249 - val_accuracy: 0.9793\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.1244 - val_accuracy: 0.9802\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.1208 - val_accuracy: 0.9811\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0159 - accuracy: 0.9949 - val_loss: 0.1359 - val_accuracy: 0.9797\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.1212 - val_accuracy: 0.9808\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.1368 - val_accuracy: 0.9784\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.1274 - val_accuracy: 0.9791\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.1279 - val_accuracy: 0.9796\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.1282 - val_accuracy: 0.9803\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0166 - accuracy: 0.9950 - val_loss: 0.1307 - val_accuracy: 0.9796\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0159 - accuracy: 0.9952 - val_loss: 0.1359 - val_accuracy: 0.9788\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.1303 - val_accuracy: 0.9801\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.1292 - val_accuracy: 0.9806\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.1277 - val_accuracy: 0.9796\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.1264 - val_accuracy: 0.9805\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.1378 - val_accuracy: 0.9793\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.1303 - val_accuracy: 0.9794\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0181 - accuracy: 0.9947 - val_loss: 0.1247 - val_accuracy: 0.9788\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.1233 - val_accuracy: 0.9797\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1238 - val_accuracy: 0.9788\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.1304 - val_accuracy: 0.9796\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.1291 - val_accuracy: 0.9804\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.1242 - val_accuracy: 0.9795\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.1257 - val_accuracy: 0.9796\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.1314 - val_accuracy: 0.9793\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0167 - accuracy: 0.9951 - val_loss: 0.1368 - val_accuracy: 0.9789\n",
            "Test accuracy:  0.9804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7vlLUxxpaTF"
      },
      "source": [
        "- AdamÏùÑ ÏÇ¨Ïö©Ìï† Îïå ÏóêÌè≠ ÏàòÍ∞Ä Ï¶ùÍ∞ÄÌï† Îïå ÌõàÎ†® Î∞è ÌÖåÏä§Ìä∏ ÏßëÌï©ÏóêÏÑú Ï†ïÌôïÎèÑÏùò Ï¶ùÍ∞ÄÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ ÏñëÏÉÅÏùÑ Î≥¥Ïù∏Îã§.\n",
        "- AdamÏùÑ optimizerÎ°ú ÏÑ†ÌÉùÌïúÎã§Î©¥ ÏïΩ 12ÏóêÌè≠ Îã®Í≥Ñ ÌõÑ Í∑∏ÎßåÎëò Ïàò ÏûàÎã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117958445-f9d4d880-b355-11eb-8310-17b0fb0faa52.png)\n",
        "\n",
        "- Îã§ÏñëÌïú ÎìúÎ°≠ÏïÑÏõÉ Í∞íÏóê ÎåÄÌï¥ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ï†ïÌôïÎèÑÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ ÏñëÏÉÅÏùÑ ÎÇòÌÉÄÎÇ∏Îã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117958546-153fe380-b356-11eb-9a4e-fac431cb454c.png)\n",
        "\n",
        "### ÏóêÌè≠ Ïàò Ï¶ùÍ∞ÄÏãúÌÇ§Í∏∞\n",
        "- ÏóêÌè≠ ÏàòÎ•º 20ÏóêÏÑú 200ÏúºÎ°ú ÎäòÎ†§ÎèÑ Í≥ÑÏÇ∞ ÏãúÍ∞ÑÏùÄ 10Î∞∞ Ï¶ùÍ∞ÄÎêòÏßÄÎßå, Í≤∞Í≥ºÍ∞Ä Í∞úÏÑ†ÎêòÏßÄÎäî ÏïäÎäîÎã§.\n",
        "- -> ÌïôÏäµÏóêÎäî Í≥ÑÏÇ∞Ïóê ÏÜåÏöîÎêú ÏãúÍ∞ÑÎ≥¥Îã§ Ï†ÅÏ†àÌïú Í∏∞Ïà† Ï±ÑÌÉùÏù¥ Îçî Ï§ëÏöîÌïòÎã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117958817-5c2dd900-b356-11eb-9357-385833d6f86e.png)\n",
        "\n",
        "### Optimizer ÌïôÏäµÎ•† Ï°∞Ï†à\n",
        "- ÏÑúÎ°ú Îã§Î•∏ ÌïôÏäµÎ•†Ïóê Îî∞Î•∏ Ï†ïÌôïÎèÑÎäî Îã§ÏùåÍ≥º Í∞ôÎã§. lr=0.1Ïùº Îïå ÏµúÏÉÅÏùò Í∞íÏù¥ÎØÄÎ°ú Ïù¥ Í∞íÏù¥ Í∏∞Î≥∏ ÌïôÏäµ ÏÜçÎèÑÏù¥Îã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117959689-39e88b00-b357-11eb-8c1c-6345c94a0e45.png)\n",
        "\n",
        "### ÎÇ¥Î∂Ä ÏùÄÎãâÏ∏µ Í∞úÏàò Ï¶ùÍ∞Ä\n",
        "- ÏùÄÎãâ Îâ¥Îü∞Ïùò ÏàòÍ∞Ä Ï¶ùÍ∞ÄÌï®Ïóê Îî∞Îùº Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÍ∞Ä Ï¶ùÍ∞ÄÌï¥, ÏµúÏ†ÅÌôîÌï¥Ïïº Ìï† Îß§Í∞úÎ≥ÄÏàòÍ∞Ä Ï†êÏ†ê ÎßéÏïÑÏÑú Ïã§Ìñâ ÏãúÍ∞ÑÏù¥ ÌÅ¨Í≤å Ï¶ùÍ∞ÄÌïúÎã§.\n",
        "- Ïã†Í≤ΩÎßùÏùò ÌÅ¨Í∏∞Î•º Ï¶ùÍ∞ÄÏãúÏºú ÏñªÎäî Ïù¥ÎìùÏùÄ ÎßùÏù¥ Ï¶ùÍ∞ÄÌï®Ïóê Îî∞Îùº Ï†êÏ†ê Í∞êÏÜåÌïúÎã§.\n",
        "- ‚à¥ ÏùÄÎãâ Îâ¥Îü∞Ïùò ÏàòÎ•º ÏùºÏ†ï Ïù¥ÏÉÅ Ï¶ùÍ∞ÄÏãúÌÇ§Î©¥ Ïã†Í≤ΩÎßùÏù¥ ÏùºÎ∞òÌôîÍ∞Ä Ïñ¥Î†§ÏõåÏ†∏ Ï†ïÌôïÎèÑÍ∞Ä Ï†ÄÌïòÎê† Ïàò ÏûàÎã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117960101-9fd51280-b357-11eb-9258-153ad56d2d23.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117960263-c98e3980-b357-11eb-9792-a42ec88b49b8.png)\n",
        "\n",
        "### Î∞∞Ïπò Í≥ÑÏÇ∞ ÌÅ¨Í∏∞ Ï¶ùÍ∞Ä\n",
        "- ÏûêÎ£åÎ•º ÌÜµÌï¥ BATCH_SIZE=64Ïùº Îïå ÏµúÍ≥† Ï†ïÌôïÎèÑÏóê ÎèÑÎã¨Ìï®ÏùÑ Ïïå Ïàò ÏûàÎã§.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117963701-ab2a3d00-b35b-11eb-8c28-fd1ceb06ee7f.png)\n",
        "\n",
        "### ÌïÑÍ∏∞Ï≤¥ Ïù∏Ïãù Ïã§Ìñâ Ï∞®Ìä∏ ÏöîÏïΩ\n",
        "- Î≥ÄÌòïÏùÑ ÌÜµÌï¥ ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÏóàÎã§.\n",
        "    1. TensorFlow 2.0ÏúºÎ°ú Îã®Ïùº Í≥ÑÏ∏µ Ïã†Í≤ΩÎßù Ï†ïÏùò\n",
        "    2. ÏùÄÎãâÏ∏µ Ï∂îÍ∞Ä\n",
        "    3. Ïã†Í≤ΩÎßùÏóê ÏûÑÏùòÏùò ÎìúÎ°≠ÏïÑÏõÉ Ï∂îÍ∞Ä\n",
        "    4. RMSPropÍ≥º AdamÏúºÎ°ú ÌÖåÏä§Ìä∏ ÏßëÌï©Ïùò ÏÑ±Îä• Í∞úÏÑ†\n",
        "    \n",
        "|Î™®Îç∏/Ï†ïÌôïÎèÑ|ÌõàÎ†®|Í≤ÄÏ¶ù|ÌÖåÏä§Ìä∏\n",
        "|-|-|-|-|\n",
        "|Îã®Ïùº|89.96%|90.70%|90.71%|\n",
        "|2 ÏùÄÎãâ(128)|90.81%|91.40%|91.18%|\n",
        "|ÎìúÎ°≠ÏïÑÏõÉ(30%)|91.70%|94.42%|94.15%(200 ÏóêÌè≠)|\n",
        "|RMSProp|97.43%|97.62%|97.64%(10 ÏóêÌè≠)|\n",
        "|Adam|98.94%|97.89%|97.82(10 ÏóêÌè≠)|\n",
        "\n",
        "- Îã§Ïùå Îëê Ïã§ÌóòÏùÄ ÌÅ∞ Í∞úÏÑ†Ï†êÏù¥ ÏóÜÏóàÎã§.\n",
        "    1. ÎÇ¥Î∂Ä Îâ¥Îü∞Ïùò ÏàòÎ•º ÎäòÎ¶¨Î©¥ Îçî Î≥µÏû°Ìïú Î™®Îç∏Ïù¥ ÏÉùÏÑ±ÎêòÍ≥† Îçî ÎßéÏùÄ Í≥ÑÏÇ∞ÎüâÏù¥ ÌïÑÏöîÌïòÏßÄÎßå Í∞úÏÑ†Ïùò Ï†ïÎèÑÍ∞Ä ÎØ∏ÎØ∏ÌïòÎã§.\n",
        "    2. optimizerÏùò BATCH_SIZEÎ•º Î≥ÄÍ≤ΩÌï¥ÎèÑ Í∞úÏÑ†Ïùò Ï†ïÎèÑÍ∞Ä ÎØ∏ÎØ∏ÌïòÎã§.\n",
        "\n",
        "## Ï†ïÍ∑úÌôî\n",
        "### Í≥ºÏ†ÅÌï©ÏùÑ ÌîºÌïòÍ∏∞ ÏúÑÌïú Ï†ïÍ∑úÌôî Ï†ÅÏö©\n",
        "- Ï¢ãÏùÄ Î®∏Ïã†Îü¨Îãù Î™®Îç∏ = ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÎÇÆÏùÄ Ïò§Î•òÏú® <=> Ï£ºÏñ¥ÏßÑ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ Î™®Îç∏Ïùò ÏÜêÏã§ Ìï®Ïàò ÏµúÏÜåÌôî -> min: {ÏÜêÏã§(ÌõàÎ†® Îç∞Ïù¥ÌÑ∞|Î™®Îç∏)}\n",
        "- ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÎÇ¥Ïû¨Îêú Î™®Îì† Í¥ÄÍ≥ÑÎ•º Ìè¨Ï∞©ÌïòÎ†§Îã§ Î™®Îç∏Ïùò Î≥µÏû°ÎèÑ Ï¶ùÍ∞Ä\n",
        "    - Î∂ÄÏ†ïÏ†ÅÏù∏ Í≤∞Í≥º 1. Î≥µÏû°Ìïú Î™®Îç∏ Ïã§ÌñâÏóê ÏÉÅÎãπÌïú ÏãúÍ∞Ñ ÏÜåÏöî\n",
        "    - Î∂ÄÏ†ïÏ†ÅÏù∏ Í≤∞Í≥º 2. ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïö∞ÏàòÌïú ÏÑ±Í≥ºÎ•º Îã¨ÏÑ±Ìï† Ïàò ÏûàÏßÄÎßå, Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÎÇòÏÅú ÏÑ±Í≥ºÎ•º Î≥¥Ïùº Ïàò ÏûàÎã§.\n",
        "        - ‚àµ Î™®Îç∏Ïù¥ ÌõàÎ†®ÏóêÎßå ÌäπÌôîÎêú ÎßéÏùÄ Îß§Í∞úÎ≥ÄÏàòÎ•º Í≥†Î†§Ìï† Ïàú ÏûàÏßÄÎßå, Ïù¥Îäî ÏùºÎ∞òÏ†ÅÏù¥ÏßÄ ÏïäÏùå\n",
        "        - -> **Í≥ºÏ†ÅÌï© overfitting**: ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ ÏûÉÏùÄ Î™®Îç∏\n",
        "        \n",
        "![image](https://user-images.githubusercontent.com/61455647/117966880-64d6dd00-b35f-11eb-9613-096fa71e8b0b.png)\n",
        "\n",
        "- ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑú Ï¥àÍ∏∞ Í∞êÏÜå ÌõÑ Í≤ÄÏ¶ù Îã®Í≥ÑÏóêÏÑú ÏÜêÏã§Ïù¥ Ï¶ùÍ∞ÄÌïúÎã§Î©¥, Î™®Îç∏Ïùò Î≥µÏû°ÏÑ± Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïú Í≤ÉÏù¥Îã§. -> ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ Í≥ºÏ†ÅÌï©\n",
        "- Í≥ºÏ†ÅÌï© Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÎ•º ÌååÏïÖÌï† Ïàò ÏûàÏñ¥Ïïº ÌïúÎã§.\n",
        "    - Î™®Îç∏ = Í∞ÄÏ§ëÏπòÏùò Î≤°ÌÑ∞\n",
        "    - Í∞Å Í∞ÄÏ§ëÏπòÎäî 0Ïù¥ÎÇò 0Ïóê Îß§Ïö∞ Í∞ÄÍπùÏßÄ ÏïäÏúºÎ©¥ Ï∂úÎ†•Ïóê ÏòÅÌñ•ÏùÑ Ï§ÄÎã§.\n",
        "    - ‚à¥ Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÎäî 0Ïù¥ ÏïÑÎãå Í∞ÄÏ§ëÏπòÏùò Í∞úÏàòÎ°ú ÌëúÌòÑÎê† Ïàò ÏûàÎã§. <=> ÏÜêÏã§ Ìï®Ïàò Ï∏°Î©¥ÏóêÏÑú 0Ïù¥ ÏïÑÎãå Í∞ÄÏ§ëÏπòÏùò Í∞úÏàòÍ∞Ä ÏµúÏÜåÏù∏ Í∞ÄÏû• Í∞ÑÎã®Ìïú Î™®Îç∏ ÏÑ†ÌÉù\n",
        "    - Ï¥àÎß§Í∞úÎ≥ÄÏàò Œª >= 0ÏúºÎ°ú Îã®Ïàú Î™®Îç∏Ïùò Ï§ëÏöîÏÑ± Ï°∞Ï†à -> min: {ÏÜêÏã§(ÌõàÎ†® Îç∞Ïù¥ÌÑ∞|Î™®Îç∏)} + Œª * Î≥µÏû°ÎèÑ(Î™®Îç∏)\n",
        "- Ï†ïÍ∑úÌôî Î∞©Î≤ï\n",
        "    - L1 Ï†ïÍ∑úÌôî(LASSO): Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÎäî Í∞ÄÏ§ëÏπò Ï†àÎåìÍ∞íÏùò Ìï©\n",
        "    - L2 Ï†ïÍ∑úÌôî(Ridge): Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÎäî Í∞ÄÏ§ëÏπò Ï†úÍ≥±Ïùò Ìï©\n",
        "    - Elastic Ï†ïÍ∑úÌôî: Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÎäî L1, L2 Ï†ïÍ∑úÌôîÏùò Ï°∞Ìï©\n",
        "- Ï†ïÍ∑úÌôîÎ°ú Í≥ºÏ†ÅÌï©Ïù¥ Î∂ÑÎ™ÖÌï† Îïå Ïã†Í≤ΩÎßùÏùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÎã§.\n",
        "- TensorFlowÎäî L1, L2, ElasticNet Ï†ïÎ•òÌôîÎ•º ÏßÄÏõêÌïúÎã§.\n",
        "```\n",
        "from tf.keras.regularizers import l2, activity_l2\n",
        "model.add(Dense(64, input_dim=64, W_regularizer=l2(0,01), activity_regularizer=activity_l2(0.01)))\n",
        "```\n",
        "\n",
        "### Î∞∞Ïπò Ï†ïÍ∑úÌôî(Batch Normalization)Ïùò Ïù¥Ìï¥\n",
        "- Í≤ΩÏö∞Ïóê Îî∞Îùº ÌõàÎ†® ÏóêÌè≠ÏùÑ Ï†àÎ∞òÏúºÎ°ú Ï§ÑÏó¨ ÌõàÎ†®ÏùÑ Í∞ÄÏÜçÌôîÌï† Ïàò ÏûàÎã§.\n",
        "- Í∏∞Ï°¥Ïùò Î¨∏Ï†úÏ†ê\n",
        "    1. Í∞Å Í≥ÑÏ∏µÏùÄ Î™®Îì† Î∞∞ÏπòÎßàÎã§ Í∞ÄÏ§ëÏπòÎ•º ÏßÄÏÜçÏ†ÅÏúºÎ°ú Îã§Î•∏ Î∂ÑÌè¨Î°ú Ï°∞Ï†ïÌïúÎã§. -> Î™®Îç∏ ÌõàÎ†® ÏÜçÎèÑ ‚Üì => Í∞Å Î∞∞ÏπòÏôÄ Í∞Å ÏóêÌè≠Ïóê ÎåÄÌï¥ Í≥ÑÏ∏µ ÏûÖÎ†•Ïù¥ Ï¢Ä Îçî Ïú†ÏÇ¨Ìïú Î∂ÑÌè¨Î•º Í∞ñÎèÑÎ°ù ÌïòÏûê.\n",
        "    2. ÏãúÍ∑∏Î™®Ïù¥Îìú Ìï®ÏàòÎäî Í∞íÏù¥ 0ÏóêÏÑú ÏÉÅÎãπÌûà Î©ÄÏñ¥ÏßÄÎ©¥ Í≥†Ï∞©ÎêòÏñ¥ Í∞ÄÏ§ëÏπòÍ∞Ä Í∞±Ïã†ÎêòÏßÄ ÏïäÎäîÎã§. => Í≥ÑÏ∏µ Ï∂úÎ†•ÏùÑ 0Ïóê Í∞ÄÍπåÏö¥ Gaussian Î∂ÑÌè¨ Îã®ÏúÑÎ°ú Î≥ÄÌôò\n",
        "- Ìï¥Í≤∞Ï±Ö\n",
        "    - ÌôúÏÑ±Ìôî ÏûÖÎ†• x, Î∞∞Ïπò ÌèâÍ∑† Œº, Î∞∞Ïπò Î∂ÑÏÇ∞ œÉ,  ÏûëÏùÄ Ïàò Œµ, ÏÑ†Ìòï Î≥ÄÌôò y = Œªx + Œ≤\n",
        "    - (x-Œº)Î°ú 0 Ï£ºÏúÑÎ°ú Î™®Ïùå -> (x-Œº)/(œÉ+Œµ)Î°ú Î∂ÑÎ™®Í∞Ä 0Ïù¥ ÎêòÎäî Í≤ÉÏùÑ ÌîºÌï® -> y = Œªx + Œ≤Î°ú ÌõàÎ†® Îã®Í≥ÑÏóêÏÑú Ï†ïÍ∑úÌôî Ìö®Í≥º Ï†ÅÏö©\n",
        "    - -> Îã§Î•∏ Í≥ÑÏ∏µÏóêÏÑúÎèÑ ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑú ŒªÏôÄ Œ≤ Îß§Í∞úÎ≥ÄÏàò ÏµúÏ†ÅÌôî\n",
        "- ÌôúÏÑ±Ìôî ÏÜçÎèÑÍ∞Ä ÎÑàÎ¨¥ ÏûëÏïÑ ÏóÜÏñ¥ÏßÄÍ±∞ÎÇò ÎÑàÎ¨¥ Ïª§ÏßÄÎäî Í≤ÉÏùÑ Î∞©ÏßÄÌïòÎäî Îç∞Ïóê ÎèÑÏõÄ -> ÌõàÎ†® ÏÜçÎèÑÏôÄ Ï†ïÌôïÎèÑ ‚Üë\n",
        "\n",
        "## Íµ¨Í∏Ä Colab ÏÇ¨Ïö©: CPU, GPU, TPU\n",
        "\n",
        "## Í∞êÏ†ï Î∂ÑÏÑù(IMDb Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú Í∞úÎ∞úÎêú Í∞êÏ†ï Î∂ÑÏÑù ÏòàÏãú)\n",
        "- IMDb Îç∞Ïù¥ÌÑ∞ÏÖã: Internet Movie DatabaseÏùò 50000Í∞úÏùò ÏòÅÌôî Î¶¨Î∑∞ ÌÖçÏä§Ìä∏(Í∏çÏ†ï/Î∂ÄÏ†ï) -> 25000Í±¥Ïùò ÌõàÎ†®Í≥º 25000Í±¥Ïùò ÌÖåÏä§Ìä∏ ÏßëÌï©ÏúºÎ°ú ÎÇòÎààÎã§.\n",
        "- Î™©Ìëú: ÌÖçÏä§Ìä∏Î°ú Ïù¥ÏßÑ ÌåêÎã®ÏùÑ ÏòàÏ∏°Ìï† Ïàò ÏûàÎäî Î∂ÑÎ•òÍ∏∞ Íµ¨Ï∂ï\n",
        "- Îç∞Ïù¥ÌÑ∞ÏÖã ÏÑ§Î™Ö\n",
        "  - `tf.keras`Î°ú IMDb Î°úÎìú\n",
        "  - Î¶¨Î∑∞Ïùò Îã®Ïñ¥ ÏãúÌÄÄÏä§Îäî Ï†ïÏàòÏùò ÏãúÌÄÄÏä§Î°ú Î≥ÄÌôò <=> Í∞Å Ï†ïÏàò = ÏÇ¨Ï†ÑÏùò ÌäπÏ†ï Îã®Ïñ¥\n",
        "  - Î¨∏Ïû•ÏùÑ `max_len` Í∏∏Ïù¥Î°ú Ï±ÑÏõå Í∏∏Ïù¥Ïóê ÏÉÅÍ¥Ä ÏóÜÏù¥ Î™®Îì† Î¨∏Ïû•ÏùÑ Ïã†Í≤ΩÎßùÏóê ÏûÖÎ†•\n",
        "  - Í∞Å ÏûÖÎ†• Î≤°ÌÑ∞Îäî Í≥†Ï†ïÎêú ÌÅ¨Í∏∞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXEhmK90paTF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500\n",
        "\n",
        "def load_data():\n",
        "  # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "  (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "  # Î¨∏Ïû•ÏùÑ max_lenÏù¥ ÎêòÎèÑÎ°ù Ï±ÑÏõå ÎÑ£ÎäîÎã§.\n",
        "  X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "  X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "  return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqDNXwZfsaEy"
      },
      "source": [
        "- Î™®Îç∏ ÏÑ§Î™Ö\n",
        "  - `Embedding()` Í≥ÑÏ∏µÏúºÎ°ú Î¶¨Î∑∞Ïóê Ìè¨Ìï®Îêú Îã®Ïñ¥Ïùò Ìù¨ÏÜå Í≥µÍ∞ÑÏùÑ Îçî Ï°∞Î∞ÄÌïú Í≥µÍ∞ÑÏúºÎ°ú Îß§Ìïë -> Í≥ÑÏÇ∞Ïù¥ Ïö©Ïù¥\n",
        "  - `GlobalMaxPooling1D()` Í≥ÑÏ∏µÏúºÎ°ú `n_words`Ïùò ÌäπÏßï Î≤°ÌÑ∞Ïùò ÏµúÎåìÍ∞íÏùÑ ÏñªÏùå\n",
        "  - 2Í∞úÏùò `Dense()` Í≥ÑÏ∏µ\n",
        "  - ÎßàÏßÄÎßâÏùÄ Îã®Ïùº Îâ¥Îü∞: ÏµúÏ¢Ö Ïù¥ÏßÑ Ï∂îÏ†ïÏùÑ ÏúÑÌï¥ ÏãúÍ∑∏Î™®Ïù¥Îìú ÌôúÏÑ±Ìôî Ìï®Ïàò"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbL58WzosXBi",
        "outputId": "5ce35a7b-73d6-42cb-ba23-6fe9de04712c"
      },
      "source": [
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # ÏûÖÎ†•: -eEmbedding Layer\n",
        "  # Î™®Îç∏Ïùò ÏûÖÎ†•: ÌÅ¨Í∏∞Ïùò Ï†ïÏàò ÌñâÎ†¨(batch, input_length), Î™®Îç∏Ïùò Ï∂úÎ†•: Ï∞®Ïõê(input_length, dim_embedding)\n",
        "  # ÏûÖÎ†• Ï§ë Í∞ÄÏû• ÌÅ∞ Ï†ïÏàòÎäî n_wordsÎ≥¥Îã§ ÏûëÍ±∞ÎÇò Í∞ôÎã§.\n",
        "  model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\n",
        "\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  # Í∞Å n_words ÌäπÏßïÏóêÏÑú ÌäπÏßï Î≤°ÌÑ∞Ïùò ÏµúÎåÄÎ•º Ï∑®Ìï®\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "score = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(\"\\nTest score: \", score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.6377"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25000/25000 [==============================] - 24s 962us/sample - loss: 0.6717 - accuracy: 0.6377 - val_loss: 0.6301 - val_accuracy: 0.8304\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.4586 - accuracy: 0.8401 - val_loss: 0.3617 - val_accuracy: 0.8583\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.2806 - accuracy: 0.8868 - val_loss: 0.3059 - val_accuracy: 0.8740\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 24s 945us/sample - loss: 0.2193 - accuracy: 0.9142 - val_loss: 0.2940 - val_accuracy: 0.8764\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.1719 - accuracy: 0.9382 - val_loss: 0.2910 - val_accuracy: 0.8766\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 24s 948us/sample - loss: 0.1346 - accuracy: 0.9541 - val_loss: 0.2934 - val_accuracy: 0.8742\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.1029 - accuracy: 0.9674 - val_loss: 0.3110 - val_accuracy: 0.8690\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 24s 947us/sample - loss: 0.0783 - accuracy: 0.9774 - val_loss: 0.3249 - val_accuracy: 0.8649\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.0584 - accuracy: 0.9846 - val_loss: 0.3388 - val_accuracy: 0.8626\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.0433 - accuracy: 0.9892 - val_loss: 0.3559 - val_accuracy: 0.8603\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 24s 948us/sample - loss: 0.0319 - accuracy: 0.9932 - val_loss: 0.3720 - val_accuracy: 0.8592\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 24s 947us/sample - loss: 0.0255 - accuracy: 0.9945 - val_loss: 0.3883 - val_accuracy: 0.8577\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.0188 - accuracy: 0.9967 - val_loss: 0.4118 - val_accuracy: 0.8541\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 24s 947us/sample - loss: 0.0149 - accuracy: 0.9977 - val_loss: 0.4201 - val_accuracy: 0.8554\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 0.4377 - val_accuracy: 0.8547\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 24s 953us/sample - loss: 0.0099 - accuracy: 0.9986 - val_loss: 0.4575 - val_accuracy: 0.8533\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.4592 - val_accuracy: 0.8544\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 24s 952us/sample - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.4770 - val_accuracy: 0.8529\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 24s 955us/sample - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.4894 - val_accuracy: 0.8521\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 24s 956us/sample - loss: 0.0052 - accuracy: 0.9996 - val_loss: 0.5036 - val_accuracy: 0.8526\n",
            "\n",
            "Test score:  0.5036102217435837\n",
            "Test accuracy:  0.8526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTF7VVDssYmr"
      },
      "source": [
        "## Ï¥àÎß§Í∞úÎ≥ÄÏàò ÌäúÎãùÍ≥º AutoML\n",
        "- **Ï¥àÎß§Í∞úÎ≥ÄÏàò hyperparameter**\n",
        "  - Ï£ºÏñ¥ÏßÑ Ïã†Í≤ΩÎßùÏóê ÎåÄÌï¥ ÏµúÏ†ÅÌôîÌï† Ïàò ÏûàÎäî Ïó¨Îü¨ Îß§Í∞úÎ≥ÄÏàò\n",
        "  - ex. ÏùÄÎãâ Îâ¥Îü∞Ïùò Í∞úÏàò, BATCH_SIZE, ÏóêÌè≠ Ïàò, Îßù ÏûêÏ≤¥Ïùò Î≥µÏû°ÎèÑÏóê Ï¢ÖÏÜçÎêú Îß§Í∞úÎ≥ÄÏàò Îì±\n",
        "  - Ïã†Í≤ΩÎßù ÏûêÏ≤¥Ïùò Îß§Í∞úÎ≥ÄÏàò(Í∞ÄÏ§ëÏπò Î∞è Ìé∏Ìñ• Í∞í)ÏôÄ Íµ¨Î≥ÑÌïòÍ≥†Ïûê Ìï®\n",
        "- Ï¥àÎß§Í∞úÎ≥ÄÏàò ÌäúÎãù(hyperparameter tuning)\n",
        "  - ÎπÑÏö© Ìï®ÏàòÎ•º ÏµúÏÜåÌôîÌïòÎäî Ï¥àÎß§Í∞úÎ≥ÄÏàòÏùò ÏµúÏ†Å Ï°∞Ìï©ÏùÑ Ï∞æÎäî Í≥ºÏ†ï\n",
        "  - nÍ∞úÏùò Ï¥àÎß§Í∞úÎ≥ÄÏàòÍ∞Ä Ïù¥Î£®Îäî nÏ∞®ÏõêÏùò Í≥µÍ∞ÑÏùÑ Ï†ïÏùò, Ïù¥ Í≥µÍ∞ÑÏóêÏÑú ÎπÑÏö© Ìï®ÏàòÏùò ÏµúÏ†Å Í∞íÏùò ÏßÄÏ†ê Ï∞®Í∏∞\n",
        "  - Î∞©Î≤ï: Í≥µÍ∞ÑÏóê Í∑∏Î¶¨ÎìúÎ•º ÎßåÎì§Ïñ¥, Í∞Å Í∑∏Î¶¨Îìú Ï†ïÏ†êÏóê ÎåÄÌïú ÎπÑÏö© Ìï®Ïàò Í∞í ÌôïÏù∏ => Ï¥àÎß§Í∞úÎ≥ÄÏàòÎ•º Î≤ÑÌÇ∑ÏúºÎ°ú ÎÇòÎà† ÏÑúÎ°ú Îã§Î•∏ Ï°∞Ìï©ÏùÑ Î¨¥Ï∞®Î≥Ñ Ï†ëÍ∑º Î∞©ÏãùÏúºÎ°ú ÌôïÏù∏\n",
        "- AutoML: Ï¥àÎß§Í∞úÎ≥ÄÏàòÎ•º ÏûêÎèôÏúºÎ°ú ÌäúÎãùÌïòÍ≥† ÏµúÏ†ÅÏùò Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤òÎ•º ÏûêÎèôÏúºÎ°ú Í≤ÄÏÉâÌïòÎäî Í≤ÉÏù¥ Î™©ÌëúÏù∏ Ïó∞Íµ¨ Í∏∞Î≤ï\n",
        "\n",
        "## Ï∂úÎ†• ÏòàÏ∏°\n",
        "- Ïã†Í≤ΩÎßùÏù¥ ÌõàÎ†®ÎêòÎ©¥ ÏòàÏ∏°Ïóê ÏÇ¨Ïö©Îê† Ïàò ÏûàÎã§.\n",
        "```\n",
        "# ÏòàÏ∏°ÌïòÍ∏∞\n",
        "predictions = model.predict(X)\n",
        "```\n",
        "- Ï£ºÏñ¥ÏßÑ ÏûÖÎ†•Ïóê ÎåÄÌï¥ Ï∂úÎ†• Í≥ÑÏÇ∞\n",
        "  - `model.evaluate()`: ÏÜêÏã§ Í∞í Í≥ÑÏÇ∞\n",
        "  - `model.predict_class()`: Î≤îÏ£º Ï∂úÎ†• Í≥ÑÏÇ∞\n",
        "  - `model.predict_proba()`: Î∂ÄÎ•ò ÌôïÎ•† Í≥ÑÏÇ∞\n",
        "\n",
        "## Ïó≠Ï†ÑÌååÏóê ÎåÄÌïú Ïã§Ïö©Ï†Å Í∞úÍ¥Ñ\n",
        "- Îã§Ï∏µ ÌçºÏÖâÌä∏Î°†ÏùÄ **Ïó≠Ï†ÑÌåå backpropagation** ÌîÑÎ°úÏÑ∏Ïä§Î°ú ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÌïôÏäµ\n",
        "- Í∞Å Ïã†Í≤ΩÎßù Í≥ÑÏ∏µÏóêÎäî Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÏßëÌï©Ïóê ÎåÄÌï¥ Ï∂úÎ†•Í∞íÏùÑ Í≤∞Ï†ïÌïòÎäî Í¥ÄÎ†® Í∞ÄÏ§ëÏπò ÏßëÌï©Ïù¥ ÏûàÎã§.\n",
        "- Ïã†Í≤ΩÎßùÏùÄ Îã§ÏàòÏùò ÏùÄÎãâÏ∏µÏùÑ Í∞ÄÏßà Ïàò ÏûàÎã§.\n",
        "- **ÏàúÏ†ÑÌåå propagate forward**\n",
        "  1. Î™®Îì† Í∞ÄÏ§ëÏπòÏóê ÏûÑÏùòÏùò Í∞í Ìï†Îãπ\n",
        "  2. ÌõàÎ†® ÏßëÌï©Ïùò Í∞Å ÏûÖÎ†•Ïóê ÎåÄÌï¥ Ïã†Í≤ΩÎßù ÌôúÏÑ±Ìôî\n",
        "    - Í∞íÏùÄ ÏûÖÎ†• Îã®Í≥ÑÏóêÏÑú ÏùÄÎãâ Îã®Í≥ÑÎ•º Í±∞Ï≥ê Ï∂úÎ†• Îã®Í≥ÑÎ°ú ÏàúÎ∞©Ìñ•(forward) Ï†ÑÌåå\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/118077076-61d6fd80-b3ee-11eb-82e0-5a6779e96c5c.png)\n",
        "\n",
        "- Ïó≠Ï†ÑÌåå\n",
        "  - ÌõàÎ†® ÏßëÌï©ÏóêÏÑú Ïã§Ï†ú Í¥ÄÏ∏°Í∞íÏùÑ ÏïåÍ∏∞ ÎïåÎ¨∏Ïóê ÏòàÏ∏°ÏóêÏÑú Î∞úÏÉùÌïú Ïò§Ï∞® Í≥ÑÏÇ∞\n",
        "  - GDÏ≤òÎüº Ï†ÅÏ†àÌïú ÏµúÏ†ÅÌôî ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú Ïò§Ï∞®Î•º Ï§ÑÏù¥Î†§Îäî Î™©Ï†Å\n",
        "  - Ïã†Í≤ΩÎßù Í∞ÄÏ§ëÏπòÎ•º Ï°∞Ï†ïÌïòÍ≥†Ïûê Ïò§Ï∞®Î•º Ïó≠ÏúºÎ°ú Ï†ÑÌåå\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/118077218-a2cf1200-b3ee-11eb-9010-37f4441bf587.png)\n",
        "\n",
        "- ÏàúÎ∞©Ìñ• Ï†ÑÌååÏôÄ Ïó≠Î∞©Ìñ• Ï†ÑÌåå ÌîÑÎ°úÏÑ∏Ïä§Îäî Ïò§Ï∞®Í∞Ä ÏÇ¨Ï†Ñ Ï†ïÏùòÎêú ÏûÑÍ≥ÑÍ∞í Ïù¥ÌïòÎ°ú Îñ®Ïñ¥Ïßà ÎïåÍπåÏßÄ Ïó¨Îü¨ Î≤à Î∞òÎ≥µÎêúÎã§.\n",
        "  - ÌäπÏßï: ÏûÖÎ†•\n",
        "  - Î†àÏù¥Î∏î: ÌïôÏäµ Í≥ºÏ†ï ÏßÑÌñâÏóê ÏÇ¨Ïö©\n",
        "  - Î™®Îç∏: ÏÜêÏã§ Ìï®ÏàòÍ∞Ä Ï†êÏßÑÏ†ÅÏúºÎ°ú ÏµúÏÜåÌôîÎêòÎäî Î∞©Ìñ•ÏúºÎ°ú Í∞±Ïã†\n",
        "  - Ïã†Í≤ΩÎßùÏùÄ Ï†ïÌôïÌïòÍ≤å ÏòàÏ∏°Îêú Î†àÏù¥Î∏î ÏàòÎ•º Ï¶ùÍ∞ÄÏãúÌÇ§Îäî Î∞©Ìñ•ÏúºÎ°ú ÎÇ¥Î∂Ä Í∞ÄÏ§ëÏπòÎ•º Ï†êÏßÑÏ†ÅÏúºÎ°ú Ï°∞Ï†ï\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/118077333-dca01880-b3ee-11eb-8c32-61ada962135d.png)\n",
        "\n",
        "## Ï†ïÎ¶¨\n",
        "\n",
        "## Îî•Îü¨Îãù Ï†ëÍ∑ºÎ≤ïÏùÑ Ìñ•Ìï¥\n",
        "- ÌïÑÍ∏∞Ï≤¥ Ïà´Ïûê Ïù∏Ïãù Ï§ë 99% Ï†ïÌôïÎèÑÏóê Í∞ÄÍπåÏõåÏßàÏàòÎ°ù Ï∂îÍ∞Ä Í∞úÏÑ†Ïù¥ Ïñ¥Î†µÎã§Îäî Í≤∞Î°†ÏùÑ ÏñªÏóàÎã§.\n",
        "- Ï†ÅÏö©ÌïòÏßÄ ÏïäÏùÄ Í∞úÏÑ† Í∞ÄÎä•Ìïú Ï†ê: Ïù¥ÎØ∏ÏßÄÏùò Î°úÏª¨ Í≥µÍ∞Ñ Íµ¨Ï°∞Î•º ÌôúÏö©ÌïòÏßÄ ÏïäÏïòÏùå=Í∏∞Î°ùÎêú Í∞Å Ïà´ÏûêÎ•º ÎÇòÌÉÄÎÇ¥Îäî ÎπÑÌä∏ÎßµÏùÑ ÌèâÎ©¥ Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌï¥ Î°úÏª¨ Í≥µÍ∞Ñ Íµ¨Ï°∞(Ïù∏Ï†ëÌïú ÌîΩÏÖÄÏù¥ ÏÑúÎ°ú Îçî Í∞ÄÍπåÏù¥ ÏûàÎã§)Í∞Ä ÏÇ¨ÎùºÏ°åÎã§.\n",
        "- -> CNN(COnvolutional Neural Network)ÏùÄ ÌäπÏ†ï Ïú†ÌòïÏùò Îî•Îü¨Îãù ÎßùÏù¥ Ïù¥ÎØ∏ÏßÄÏùò Î°úÏª¨ Í≥µÍ∞Ñ Íµ¨Ï°∞Î•º Î≥¥Ï°¥ÌïúÎã§+Ï†êÏßÑÏ†Å Ï∂îÏÉÅÌôî Î†àÎ≤®ÏùÑ ÌÜµÌïú ÌïôÏäµÏùÑ Ïù¥Ïö©Ìï¥ Í∞úÎ∞ú"
      ]
    }
  ]
}