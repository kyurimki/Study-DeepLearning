{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "chapter01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-meRTND-paSy"
      },
      "source": [
        "# 01장. 텐서플로 2.0으로 신경망 구현\n",
        "## 텐서플로(TensorFlow, TF)란?\n",
        "- 구글 브레인 팀에서 심층 신경망(deep neural networks)을 위해 개발한 오픈소스 SW 라이브러리\n",
        "- 주요 특징\n",
        "\t- Python, C++, Java, R, Go로 작업 가능\n",
        "\t- Keras는 TensorFlow와 통합된 고급 신경망 API, SW 구성 요소의 상호작용 방식 지정\n",
        "\t- 모델 배치와 생산 과정에서 쉽게 사용 가능\n",
        "\t- TensorFlow 2.0에는 정적 그래프에 기반을 둔 그래프 연산과 즉시 연산(eager computation) 지원 도입\n",
        "\t- 강력한 커뮤니티의 지원: Github, Google Trends에서 인기를 확인할 수 있음\n",
        "    \n",
        "## 케라스(Keras)란?\n",
        "- 딥러닝 모델을 만들고 훈련하고자 기초 구성 요소를 구성하는 API\n",
        "- 여러 딥러닝 엔진(Google의 TensorFlow, MS의 CNTK, Amazon의 MxNet, Theano etc.)\n",
        "- TensorFlow 2.0부터 케라스가 표준 하이레벨 API로 채택 -> 코딩 단순화, 좀 더 직관적인 프로그래밍\n",
        "\n",
        "## TensorFlow 2.0의 가장 중요한 변화\n",
        "- Keras는 TensorFlow의 일부이기 때문에 분리에 의미가 없음\n",
        "- keras vs. tf.keras\n",
        "\t- tf.keras는 Tensorflow 내부에 케라스 구현한 것\n",
        "\t- 다른 TensorFlow API와 더 나은 통합을 원하면 tf.keras 사용\n",
        "- TensorFlow 1.0 설치하기\n",
        "    - CPU만 있을 경우: `pip install tensorflow`\n",
        "    - GPU도 있는 경우: `pip install tensorflow-gpu`\n",
        "- TensorFlow 2.0 설치하기\n",
        "    - CPU만 있을 경우: `pip install tensorflow==2.0.0-alpha0`\n",
        "    - GPU도 있는 경우: `pip install tensorflow-gpu==2.0.0-alpha0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYJYXiKqpjhX"
      },
      "source": [
        "# TensorFlow 1.0에서 신경망을 코딩하는 전통적인 방법(11 lines)\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution() # To avoid RuntimeError: tf.placeholder() is not compatible with eager execution\n",
        "\n",
        "in_a = tf.placeholder(dtype=tf.float32, shape=(2))\n",
        "\n",
        "def model(x):\n",
        "    with tf.variable_scope(\"matmul\"):\n",
        "        W = tf.get_variable(\"W\", initializer=tf.ones(shape=(2, 2)))\n",
        "        b = tf.get_variable(\"b\", initializer=tf.zeros(shape=(2)))\n",
        "        return x * W + b\n",
        "\n",
        "out_a = model(in_a)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    outs = sess.run([out_a], feed_dict={in_a: [1, 0]})"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kJ8l5TPpaS6",
        "outputId": "eb83bfbf-5457-4438-d92e-6fcf796445a5"
      },
      "source": [
        "# TensorFlow 2.0으로 코딩(8 lines)\n",
        "import tensorflow as tf\n",
        "W = tf.Variable(tf.ones(shape=(2, 2)), name=\"W\")\n",
        "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
        "\n",
        "@tf.function\n",
        "def model(x):\n",
        "    return W * x + b\n",
        "\n",
        "out_a = model([1, 0])\n",
        "\n",
        "print(out_a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"StatefulPartitionedCall:0\", shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj8p0GzRpaS8"
      },
      "source": [
        "## 신경망 소개: 인공 신경망(Artificial Neural Networks, nets, ANN)\n",
        "- 포유류의 중추 신경계 연구에서 영감 받은 머신러닝 모델의 일종\n",
        "- 각 신경망은 서로 연결된 많은 뉴런으로 구성\n",
        "- 한 계층(layer)의 뉴런은 특정 상태가 되면 다른 계층으로 메시지를 교환(발화, fire)하고, 신경망은 계산 수행\n",
        "- 변천 과정\n",
        "    - 1950년대 후반: 단순 계산을 위한 두 계층의 퍼셉트론\n",
        "    - 1960년대: 다계층 훈련을 위한 역전파(backpropagation) 알고리즘\n",
        "    - 1980년대 다른 단순 기법들이 더 효과적인 방법이 될 때까지 학문 연구의 주요 주제\n",
        "    - 2000년대 중반: G. Hinton이 제안한 획기적인 빠른 학습 알고리즘과 2011년경 대량 연산이 가능한 GPU의 등장과 훈련에 쓰일 대규모 데이터의 수집 가능 -> deep learning의 발판\n",
        "- 점진적 추상화를 통한 학습은 인간의 두뇌에서 수백만 년 동안 진화해 온 모델과 닮았다.\n",
        "    - 인간의 시각 시스템은 여러 계층으로 구성\n",
        "    - 눈은 시각 피질(Visual Cortex)(V1) 영역과 연결\n",
        "        - 기본적인 특징과 시각적 방향, 공간 주파수, 색상의 작은 변화 구분\n",
        "        - 약 1억 4천만 개의 뉴런으로 구성, 수십억 개의 연결로 구성\n",
        "        - 형태, 얼굴, 동물 등과 같이 좀 더 복잡한 개념을 인식하고 더 복잡한 이미지 처리를 하는 다른 영역(V2, V3, V4, V5, V6)와 연결\n",
        "    - 딥러닝은 인간의 시각 시스템 조직에서 영감을 얻음\n",
        "\n",
        "## 퍼셉트론(Perceptron)\n",
        "- 입력 특징(feature) 또는 간단한 특징인 n개의 크기를 갖는 입력 벡터(x1, x2, ..., xn)이 주어지면 1(True) 또는 0(False)를 출력하는 간단한 알고리즘\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117759330-91570000-b25e-11eb-9dc7-a836525c3d0c.png)\n",
        "\n",
        "- w: 가중치 벡터, wx: dot product, b: 편향(bias)\n",
        "- wx + b: w와 b에 할당된 값에 따라 위치를 변경하는 초평면(hyperplane) 경계 정의\n",
        "    - 초평면: 둘러싼 공간(ambient space)보다 한 차원이 낮은 부공간(subspace)\n",
        "    - 입력이 3개의 특징(빨강, 녹색, 파란색의 양)일 때 퍼셉트론은 색상이 흰색인지 아닌지를 결정\n",
        "![image](https://user-images.githubusercontent.com/61455647/117759499-daa74f80-b25e-11eb-83a2-d65e3c8b50fa.png)\n",
        "\n",
        "- 퍼셉트론은 '아마도'라는 결과를 표현할 수 없다\n",
        "\n",
        "### TensorFlow 2.0 코드 첫 번째 예제\n",
        "- tf.keras로 모델을 작성하는 방법: Sequential API, Functional API, Model Subclassing\n",
        "- `Sequential()` 모델\n",
        "    - 신경망 계층의 선형 파이프라인(=stack)\n",
        "    - 아래의 코드는 784개의 입력 변수(feature)를 취하는 10개의 인공 뉴런을 가진 단일 계층 정의\n",
        "    - 망이 밀집(dense) = 각 계층의 뉴런이 이전 계층에 위한 모든 뉴런과 완전 연결되어 있고, 그 다음 계층에 있는 모든 뉴런과도 완전 연결되어 있다.\n",
        "    - 각 뉴런은 `kernel_initializer` 매개변수를 통해 특정 가중치로 초기화할 수 있다.\n",
        "        - `random_uniform`: 가중치는 -0.05~0.05 사이에서 균등하게 랜덤 분포\n",
        "        - `random_normal`: 가중치는 가우스 분포에 따라 평균이 0이고 작은 표준 편차 0.05로 초기화\n",
        "        - `zero`: 모든 가중치는 0으로 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYnEwNtypaS8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "NB_CLASSES = 10\n",
        "RESHAPED = 784\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(NB_CLASSES, input_shape=(RESHAPED,), kernel_initializer='zeros', name='dense_layer', activation='softmax'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5HGO7yVpaS9"
      },
      "source": [
        "## 다층 퍼셉트론: 신경망 첫 번째 예제\n",
        "- Perceptron은 단일 선형 계층 모델의 이름, 단순한 선형 함수\n",
        "- **다층 퍼셉트론 MLP, Multi-Layer Perceptron**\n",
        "    - 여러 개의 계층이 있는 경우, 여러 개의 단일 계층이 쌓임\n",
        "    - 입력과 출력 계층은 외부에서 볼 수 있지만 중간의 다른 모든 계층은 숨겨져 있음 -> **은닉층 hidden layers**\n",
        "    - 첫 번째 은닉층의 각 노드는 입력 받고 선형 함수에 연계된 값에 따라 발화(fire) -> 첫 번째 은닉층의 출력은 다른 선형 함수가 적용된 두 번째 계층으로 전달 -> 하나의 단일 뉴런으로 구성된 최종 출력 계층으로 전달\n",
        "    \n",
        "![image](https://user-images.githubusercontent.com/61455647/117767999-ac307100-b26c-11eb-9875-5f3ab2391de6.png)\n",
        "\n",
        "### 퍼셉트론 훈련의 문제점과 해결책\n",
        "- 단일 뉴런일 때 가중치 w와 편향 b의 값으로 가장 적합한 것: 이상적으로 일련의 훈련 예를 제공하고 컴퓨터가 출력에서 발생하는 오차를 최소화하는 방식으로 가중치와 편향 조정\n",
        "- ex. 고양이 이미지를 포함한 것과 그렇지 않은 별도의 이미지 집합이 있다고 가정, 각 뉴런은 이미지의 단일 픽셀 값에서 입력을 받는다고 가정\n",
        "- -> 컴퓨터가 이미지를 처리하면서 각 뉴런이 가중치와 편향을 조정해 잘못 인식되는 이미지의 비율이 점차 줄어들기를 원함\n",
        "- -> 출력에 아주 작은 변화만 일으키려면 가중치(or 편향)도 약간만 변경해야 함, 출력에 큰 변화가 생긴다면 점진적인 학습 X\n",
        "- 퍼셉트론은 0과 1이기 때문에 '조금씩'의 작동을 보이지 않음\n",
        "![image](https://user-images.githubusercontent.com/61455647/117768062-c9653f80-b26c-11eb-976f-6b672f59b7a6.png)\n",
        "\n",
        "- -> 불연속(discontinutiy) 없이 0에서 1로 점진적으로 변경되는 함수 필요 <=> 미분 가능한 연속 함수가 필요\n",
        "\n",
        "### 활성화 함수: 시그모이드(sigmoid)\n",
        "- 𝜎(𝑥)= 1/(1+𝑒^(−𝑥))\n",
        "- 입력이 (−∞, ∞)에서 변할 때 출력은 (0, 1)에서 변화\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117768191-fdd8fb80-b26c-11eb-876f-3dc97f54dc0e.png)\n",
        "\n",
        "- 비선형 함수 𝜎(z = w𝑥 + b) 계산에 시그모이드를 사용할 수 있다.\n",
        "    - z = w𝑥 + b가 매우 크고 양수, 𝑒^(−z) -> 0이므로 𝜎(z) -> 1\n",
        "    - z = w𝑥 + b가 매우 크고 음수, 𝑒^(−z) -> ∞이므로 𝜎(z) -> 0\n",
        "- 시그모이드 활성화 함수를 사용한 뉴런의 경우 퍼셉트론과 유사한 작동을 하지만, 그 변화는 점진적이고 출력값도 완전히 유효하다.\n",
        "\n",
        "### 활성화 함수: tanh\n",
        "- tanh(𝑧)=(𝑒^𝑧−𝑒^(−𝑧))/(𝑒^𝑧+𝑒^(−𝑧))\n",
        "- 출력 범위: [-1, 1]\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117768839-e77f6f80-b26d-11eb-9ec6-8bd8f253cce4.png)\n",
        "\n",
        "### 활성화 함수: ReLU(Rectified Linear Unit)\n",
        "- 시그모이드에서 발견된 일부 최적화 문제를 해결하는 데에 도움\n",
        "- f(x) = max(0, x)\n",
        "    - 음수 값에 대해서는 항상 0, 양의 값에 대해 선형으로 증가\n",
        "- 시그모이드에 비해 구현이 매우 간단\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117769160-4b099d00-b26e-11eb-847e-19f163ca5c46.png)\n",
        "\n",
        "### 추가적인 두 개의 활성화 함수: ELU와 LeakyReLU\n",
        "- ELU\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117770589-10a0ff80-b270-11eb-9974-0afcca168f98.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117769881-38dc2e80-b26f-11eb-9fec-5712939b3c60.png)\n",
        "\n",
        "- LeakyReLU\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117770387-cfa8eb00-b26f-11eb-9fa2-92d9e5b9d4af.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117770133-8b1d4f80-b26f-11eb-9798-88d712508eb3.png)\n",
        "\n",
        "- 두 함수 모두 x가 음수일 때 작은 변화를 일으켜 경우에 따라 유용할 수 있다.\n",
        "\n",
        "### 활성화 함수(activation functions)\n",
        "- ('그래디언트 하강' 절에서) 시그모이드와 ReLU 함수가 보여주는 전형적인 점진적인 변화 형태가 신경망에서 오차를 조금씩 줄이며 적응해 나가는 학습 알고리즘을 개발하는 기본 구성 요소임을 알 수 있을 것이다.\n",
        "- 입력 벡터(x1, x2, ..., xm), 가중치 벡터(w1, w2, ..., wm), 편향 b, 합계 Σ일 때 활성화 함수 𝜎는 다음과 같다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117771097-b5bbd800-b270-11eb-8a9b-2fc62d26d68b.png)\n",
        "\n",
        "### 간단히 말해: 결국 신경망이란?\n",
        "- 어떤 입력을 해당 출력으로 매핑하는 함수를 계산하는 방법\n",
        "- 비선형 활성화와 결합해 여러 계층으로 쌓을 경우 거의 모든 것을 학습할 수 있다.\n",
        "- 최적화하려는 적절한 척도(손실함수(loss function)), 학습하기에 충분한 데이터, 충분한 연산 능력 필요\n",
        "- **학습**: 본질적으로 미래의 결과를 예측하고자 확립된 관찰을 일반화하려는 것을 목표로하는 과정\n",
        "\n",
        "## 실제 예제: 필기체 숫자 인식\n",
        "- MNIST 필기체 숫자 데이터베이스 사용\n",
        "- **지도학습 Supervised Learning**\n",
        "    - 머신러닝에서 정답이 있는 데이터셋 사용 -> 신경망을 개선하고자 훈련 예시 사용\n",
        "    - 레이블이 없다고 신경망이 예측을 수행한 후 레이블을 확인해 신경망이 얼마나 숫자를 잘 인식하는지 평가할 수 있다.\n",
        "\n",
        "### 원핫 인코딩(OHE, One-Hot Encoding)\n",
        "- 신경망 내부에 사용될 정보를 인코딩하는 간단한 도구\n",
        "- 범주형 특징을 숫자형 변수로 변환\n",
        "- ex. [0-9]의 값 d를 갖는 범주형 특징 수는 10개의 위치를 가진 이진 벡터로 구성해 d번째 위치만 1로 하고 나머지는 항상 0 값을 갖도록 인코딩\n",
        "- 학습 알고리즘이 수치형 함수를 처리하도록 특화될 경우 사용됨\n",
        "\n",
        "### TensorFlow 2.0으로 단순 신경망 정의\n",
        "- TensorFlow 2.0은 데이터셋을 로드하고 신경망을 미세 조정하는 훈련 집합 `X_train`으로의 분할, 신경망의 성능을 평가하는 데에 사용하는 테스트 집합 `X_test`로 분할하는 적절한 라이브러리 제공\n",
        "- 데이터는 신경망을 훈련할 때 32비트 정밀도를 갖도록 `float32`로 변환, [0, 1] 범위로 정규화\n",
        "- 실제 레이블을 각각 `Y_train`과 `Y_test`에 로드하고 원핫 인코딩 수행\n",
        "- `EPOCH`: 훈련을 얼마나 지속할 것인지\n",
        "- `BATCH_SIZE`: 한 번에 신경망에 입력하는 표본의 수\n",
        "- `VALIDATION`: 훈련 프로세스의 유효성 확인하거나 증명을 위해 남겨둔 데이터의 양"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaNom_pPpaS-",
        "outputId": "ae70480e-e70c-4ae8-d359-fa133c253366"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# 신경망과 훈련 매개변수\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10  # 출력 개수 = 숫자의 개수\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # 검증을 위해 남겨둔 훈련 데이터\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "# 검증\n",
        "# 훈련과 테스트 데이터를 각각 60000개와 10000개로 나눴다.\n",
        "# 레이블에 대한 원핫 인코딩은 자동으로 적용된다.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train은 60000개 행으로 28*28 값을 가진다. -> 60000 * 784 형태로 변환\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 입력을 [0, 1] 사이로 정규화\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# 레이블을 원핫 인코딩\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnMA9SbqpaS_"
      },
      "source": [
        "- 입력 계층에 이미지의 각 픽셀과 연결된 뉴런이 있으며, MNIST 이미지의 각 픽셀마다 하나씩 총 28 * 28 = 784개의 뉴런이 있음\n",
        "- 대개 각 픽셀과 관련된 값은 [0, 1] 범위에서 정규화된다.\n",
        "- 출력은 10자리 부류(Class) 중 하나며, 각 숫자마다 하나의 부류가 있다.\n",
        "- 마지막 계층은 활성화 함수가 소프트맥스(Softmax)인 단일 뉴런으로, 시그모이드 함수를 일반화한 것이다.\n",
        "    - 소프트맥스: 임의의 실수 값의 K차원 벡터를 (0, 1) 범위의 실수 값을 가진 K차원 벡터로 밀어 넣어 총합이 1이 되게 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9ZvA6tspaS_"
      },
      "source": [
        "# 모델 구축\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(NB_CLASSES, input_shape=(RESHAPED,), name='dense_layer', activation='softmax'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG3dBJsKpaS_"
      },
      "source": [
        "- 모델 정의 후 TensorFlow 2.0에서 실행할 수 있도록 모델 컴파일 필요\n",
        "- 컴파일 중 설정 사항\n",
        "    1. 최적화기(optimizer) 선택\n",
        "        - optimizer: 모델을 훈련시키는 동안 가중치를 업데이트하는 데 사용되는 특정 알고리즘\n",
        "    2. 목적 함수(objective function) 선택\n",
        "        - objective function: optimizer가 가중치 공간을 탐색하는 데에 사용\n",
        "        - = 손실 함수(loss function)이나 비용 함수(cost function)\n",
        "        - 최적화 프로세는 손실 최소화 프로세스로 정의\n",
        "        - MSE\n",
        "            - 예측과 실제 값 사이의 평균 제곱 오차\n",
        "            - M𝑆𝐸= (1/n) * ∑(i=1,n)(𝑑−𝑦)^2, d: 예측 벡터, y: n 관측치의 벡터\n",
        "            - 각 예측에서 발생한 모든 오차의 평균\n",
        "            - 예측이 실제 값과 멀수록 제곱 연사에 의해 더욱 뚜렷해진다.\n",
        "            - 제곱을 통해 오차가 양수이든 음수이든 누적 값 증가\n",
        "        - binary_crossentropy\n",
        "            - 로그 손실\n",
        "            - 목표가 c일 때 모델이 p로 예측한 경우: 교차 엔트로피(cross-entropy) L(p, c) = -c * ln(p)-(1-c)ln(1-p)\n",
        "            - 이진 레이블 예측에 적절\n",
        "        - categorical_crossentropy\n",
        "            - 다부류(multiclass) 로그 손실\n",
        "            - 예측 분포를 참 분포와 비교 -> 참 부류에 대한 확률=1로 설정, 나머지는 0으로 설정 -> 원핫 인코딩\n",
        "            - 참 부류가 c인데 y로 예측했다면, L(c, p) = -∑(i) ci * ln(pi)\n",
        "            - 출력이 참 벡터에 가까울수록 손실 ↓\n",
        "            - 다부류 레이블 예측에 적합\n",
        "    3. 훈련된 모델을 평가 by 척도(metrics)\n",
        "        - 정확도(Accuracy): 타깃 대비 정확히 예측한 비율\n",
        "        - 정밀도(Precision): positive으로 예측한 것 중 실제로 참인 것의 비율\n",
        "        - 재현율(Recall): 올바르게 예측한 것(참은 positive, 거짓은 negative으로 예측) 중 positive으로 예측한 것이 실제로 참인 비율\n",
        "        - 모델 평가에만 사용, 신경망의 성능 판단"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaY04XMXpaTA"
      },
      "source": [
        "# 모델 컴파일\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bAFTBCZpaTA"
      },
      "source": [
        "- 확률적 그래디언트 하강(SGD, Stochastic Gradient Descent)\n",
        "    - 최적화 알고리즘의 특별한 종류\n",
        "    - 각 훈련 에폭(epoch)마다 신경망의 오차를 줄이고자 사용\n",
        "- 모델 컴파일 후 `fit()` 메소드로 훈련할 수 있고, 이때 매개변수 명시 가능\n",
        "    - `epochs`\n",
        "        - 모델이 훈련 집합에 노출된 횟수\n",
        "        - 각 반복에서 optimizer는 목표 함수가 최소가 되도록 가중치를 조정\n",
        "    - `batch_size`\n",
        "        - optimizer가 가중치 갱신을 수행하기 전에 관찰한 훈련 인스턴스의 수\n",
        "        - 한 에폭당 여러 배치가 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YWnvVuIpaTA",
        "outputId": "dd6386d7-73a5-4fa9-fac4-84d3d2e6a099"
      },
      "source": [
        "# 모델 훈련\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 1.3597 - accuracy: 0.6863 - val_loss: 0.8869 - val_accuracy: 0.8321\n",
            "Epoch 2/200\n",
            " 3840/48000 [=>............................] - ETA: 0s - loss: 0.8951 - accuracy: 0.8266"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.7853 - accuracy: 0.8351 - val_loss: 0.6525 - val_accuracy: 0.8603\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.6389 - accuracy: 0.8544 - val_loss: 0.5588 - val_accuracy: 0.8706\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5680 - accuracy: 0.8644 - val_loss: 0.5070 - val_accuracy: 0.8772\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5248 - accuracy: 0.8704 - val_loss: 0.4731 - val_accuracy: 0.8831\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4950 - accuracy: 0.8746 - val_loss: 0.4491 - val_accuracy: 0.8871\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4730 - accuracy: 0.8784 - val_loss: 0.4315 - val_accuracy: 0.8911\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4558 - accuracy: 0.8809 - val_loss: 0.4175 - val_accuracy: 0.8938\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4420 - accuracy: 0.8837 - val_loss: 0.4060 - val_accuracy: 0.8950\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4305 - accuracy: 0.8863 - val_loss: 0.3964 - val_accuracy: 0.8965\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4209 - accuracy: 0.8881 - val_loss: 0.3885 - val_accuracy: 0.8982\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4125 - accuracy: 0.8895 - val_loss: 0.3815 - val_accuracy: 0.8998\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4052 - accuracy: 0.8914 - val_loss: 0.3755 - val_accuracy: 0.9007\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3987 - accuracy: 0.8918 - val_loss: 0.3702 - val_accuracy: 0.9018\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3930 - accuracy: 0.8935 - val_loss: 0.3654 - val_accuracy: 0.9029\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3879 - accuracy: 0.8946 - val_loss: 0.3610 - val_accuracy: 0.9038\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3832 - accuracy: 0.8956 - val_loss: 0.3573 - val_accuracy: 0.9050\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3789 - accuracy: 0.8963 - val_loss: 0.3537 - val_accuracy: 0.9057\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3750 - accuracy: 0.8974 - val_loss: 0.3506 - val_accuracy: 0.9061\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3714 - accuracy: 0.8981 - val_loss: 0.3476 - val_accuracy: 0.9066\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3681 - accuracy: 0.8987 - val_loss: 0.3449 - val_accuracy: 0.9068\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3650 - accuracy: 0.8996 - val_loss: 0.3423 - val_accuracy: 0.9080\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3621 - accuracy: 0.9001 - val_loss: 0.3400 - val_accuracy: 0.9080\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3594 - accuracy: 0.9009 - val_loss: 0.3378 - val_accuracy: 0.9089\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3568 - accuracy: 0.9012 - val_loss: 0.3359 - val_accuracy: 0.9087\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3545 - accuracy: 0.9019 - val_loss: 0.3337 - val_accuracy: 0.9096\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3522 - accuracy: 0.9024 - val_loss: 0.3320 - val_accuracy: 0.9098\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3501 - accuracy: 0.9031 - val_loss: 0.3303 - val_accuracy: 0.9103\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3481 - accuracy: 0.9036 - val_loss: 0.3285 - val_accuracy: 0.9103\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3462 - accuracy: 0.9040 - val_loss: 0.3270 - val_accuracy: 0.9118\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3444 - accuracy: 0.9043 - val_loss: 0.3256 - val_accuracy: 0.9114\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3426 - accuracy: 0.9052 - val_loss: 0.3243 - val_accuracy: 0.9115\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3409 - accuracy: 0.9054 - val_loss: 0.3229 - val_accuracy: 0.9120\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3394 - accuracy: 0.9060 - val_loss: 0.3216 - val_accuracy: 0.9127\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3379 - accuracy: 0.9064 - val_loss: 0.3204 - val_accuracy: 0.9130\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3364 - accuracy: 0.9068 - val_loss: 0.3193 - val_accuracy: 0.9128\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3350 - accuracy: 0.9070 - val_loss: 0.3183 - val_accuracy: 0.9132\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3337 - accuracy: 0.9074 - val_loss: 0.3170 - val_accuracy: 0.9140\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3324 - accuracy: 0.9079 - val_loss: 0.3161 - val_accuracy: 0.9136\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3312 - accuracy: 0.9083 - val_loss: 0.3151 - val_accuracy: 0.9126\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3300 - accuracy: 0.9086 - val_loss: 0.3142 - val_accuracy: 0.9135\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3289 - accuracy: 0.9090 - val_loss: 0.3132 - val_accuracy: 0.9141\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3277 - accuracy: 0.9093 - val_loss: 0.3124 - val_accuracy: 0.9150\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3267 - accuracy: 0.9097 - val_loss: 0.3115 - val_accuracy: 0.9147\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3256 - accuracy: 0.9097 - val_loss: 0.3107 - val_accuracy: 0.9149\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3246 - accuracy: 0.9102 - val_loss: 0.3100 - val_accuracy: 0.9152\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3237 - accuracy: 0.9106 - val_loss: 0.3092 - val_accuracy: 0.9149\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3227 - accuracy: 0.9103 - val_loss: 0.3084 - val_accuracy: 0.9155\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3218 - accuracy: 0.9110 - val_loss: 0.3078 - val_accuracy: 0.9152\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3209 - accuracy: 0.9113 - val_loss: 0.3071 - val_accuracy: 0.9155\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3201 - accuracy: 0.9115 - val_loss: 0.3064 - val_accuracy: 0.9156\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3192 - accuracy: 0.9115 - val_loss: 0.3057 - val_accuracy: 0.9157\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3184 - accuracy: 0.9121 - val_loss: 0.3051 - val_accuracy: 0.9157\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3176 - accuracy: 0.9121 - val_loss: 0.3044 - val_accuracy: 0.9158\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3169 - accuracy: 0.9126 - val_loss: 0.3039 - val_accuracy: 0.9156\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3161 - accuracy: 0.9124 - val_loss: 0.3034 - val_accuracy: 0.9159\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3154 - accuracy: 0.9129 - val_loss: 0.3029 - val_accuracy: 0.9158\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3147 - accuracy: 0.9131 - val_loss: 0.3022 - val_accuracy: 0.9160\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3140 - accuracy: 0.9130 - val_loss: 0.3017 - val_accuracy: 0.9161\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3133 - accuracy: 0.9131 - val_loss: 0.3011 - val_accuracy: 0.9164\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3127 - accuracy: 0.9134 - val_loss: 0.3007 - val_accuracy: 0.9162\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3120 - accuracy: 0.9134 - val_loss: 0.3002 - val_accuracy: 0.9164\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3114 - accuracy: 0.9138 - val_loss: 0.2998 - val_accuracy: 0.9163\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3108 - accuracy: 0.9140 - val_loss: 0.2993 - val_accuracy: 0.9167\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3102 - accuracy: 0.9141 - val_loss: 0.2988 - val_accuracy: 0.9168\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3096 - accuracy: 0.9144 - val_loss: 0.2985 - val_accuracy: 0.9169\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3090 - accuracy: 0.9144 - val_loss: 0.2979 - val_accuracy: 0.9174\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3084 - accuracy: 0.9146 - val_loss: 0.2974 - val_accuracy: 0.9172\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3079 - accuracy: 0.9145 - val_loss: 0.2972 - val_accuracy: 0.9172\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3074 - accuracy: 0.9149 - val_loss: 0.2968 - val_accuracy: 0.9176\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3068 - accuracy: 0.9150 - val_loss: 0.2963 - val_accuracy: 0.9178\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3063 - accuracy: 0.9148 - val_loss: 0.2960 - val_accuracy: 0.9183\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3058 - accuracy: 0.9150 - val_loss: 0.2955 - val_accuracy: 0.9177\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3053 - accuracy: 0.9152 - val_loss: 0.2952 - val_accuracy: 0.9183\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3049 - accuracy: 0.9153 - val_loss: 0.2948 - val_accuracy: 0.9181\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3044 - accuracy: 0.9153 - val_loss: 0.2945 - val_accuracy: 0.9183\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3039 - accuracy: 0.9159 - val_loss: 0.2941 - val_accuracy: 0.9184\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3034 - accuracy: 0.9157 - val_loss: 0.2938 - val_accuracy: 0.9185\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3029 - accuracy: 0.9157 - val_loss: 0.2935 - val_accuracy: 0.9183\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3025 - accuracy: 0.9159 - val_loss: 0.2931 - val_accuracy: 0.9180\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3021 - accuracy: 0.9159 - val_loss: 0.2929 - val_accuracy: 0.9187\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3017 - accuracy: 0.9160 - val_loss: 0.2925 - val_accuracy: 0.9184\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3013 - accuracy: 0.9164 - val_loss: 0.2923 - val_accuracy: 0.9182\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3009 - accuracy: 0.9161 - val_loss: 0.2919 - val_accuracy: 0.9186\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3004 - accuracy: 0.9166 - val_loss: 0.2917 - val_accuracy: 0.9181\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3000 - accuracy: 0.9163 - val_loss: 0.2915 - val_accuracy: 0.9189\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2996 - accuracy: 0.9164 - val_loss: 0.2911 - val_accuracy: 0.9192\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2993 - accuracy: 0.9168 - val_loss: 0.2908 - val_accuracy: 0.9191\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2989 - accuracy: 0.9169 - val_loss: 0.2905 - val_accuracy: 0.9193\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2985 - accuracy: 0.9170 - val_loss: 0.2902 - val_accuracy: 0.9194\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2981 - accuracy: 0.9171 - val_loss: 0.2901 - val_accuracy: 0.9197\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2978 - accuracy: 0.9169 - val_loss: 0.2897 - val_accuracy: 0.9192\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2974 - accuracy: 0.9173 - val_loss: 0.2895 - val_accuracy: 0.9196\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2971 - accuracy: 0.9171 - val_loss: 0.2891 - val_accuracy: 0.9193\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2967 - accuracy: 0.9174 - val_loss: 0.2890 - val_accuracy: 0.9196\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2964 - accuracy: 0.9174 - val_loss: 0.2887 - val_accuracy: 0.9197\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2961 - accuracy: 0.9177 - val_loss: 0.2885 - val_accuracy: 0.9195\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2957 - accuracy: 0.9176 - val_loss: 0.2882 - val_accuracy: 0.9203\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2954 - accuracy: 0.9175 - val_loss: 0.2880 - val_accuracy: 0.9200\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2951 - accuracy: 0.9179 - val_loss: 0.2878 - val_accuracy: 0.9202\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2948 - accuracy: 0.9181 - val_loss: 0.2876 - val_accuracy: 0.9206\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2944 - accuracy: 0.9179 - val_loss: 0.2873 - val_accuracy: 0.9202\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2941 - accuracy: 0.9179 - val_loss: 0.2871 - val_accuracy: 0.9203\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2939 - accuracy: 0.9185 - val_loss: 0.2869 - val_accuracy: 0.9210\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2936 - accuracy: 0.9185 - val_loss: 0.2867 - val_accuracy: 0.9206\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2933 - accuracy: 0.9185 - val_loss: 0.2865 - val_accuracy: 0.9208\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2930 - accuracy: 0.9185 - val_loss: 0.2863 - val_accuracy: 0.9207\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2926 - accuracy: 0.9184 - val_loss: 0.2862 - val_accuracy: 0.9203\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2924 - accuracy: 0.9189 - val_loss: 0.2859 - val_accuracy: 0.9206\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2921 - accuracy: 0.9189 - val_loss: 0.2857 - val_accuracy: 0.9208\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2919 - accuracy: 0.9189 - val_loss: 0.2855 - val_accuracy: 0.9204\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2916 - accuracy: 0.9191 - val_loss: 0.2854 - val_accuracy: 0.9211\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2913 - accuracy: 0.9187 - val_loss: 0.2851 - val_accuracy: 0.9205\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2911 - accuracy: 0.9191 - val_loss: 0.2849 - val_accuracy: 0.9208\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2908 - accuracy: 0.9193 - val_loss: 0.2847 - val_accuracy: 0.9209\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2905 - accuracy: 0.9193 - val_loss: 0.2846 - val_accuracy: 0.9209\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2902 - accuracy: 0.9192 - val_loss: 0.2845 - val_accuracy: 0.9208\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2900 - accuracy: 0.9193 - val_loss: 0.2843 - val_accuracy: 0.9210\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2898 - accuracy: 0.9193 - val_loss: 0.2841 - val_accuracy: 0.9210\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2895 - accuracy: 0.9194 - val_loss: 0.2840 - val_accuracy: 0.9210\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2893 - accuracy: 0.9192 - val_loss: 0.2838 - val_accuracy: 0.9207\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2890 - accuracy: 0.9196 - val_loss: 0.2837 - val_accuracy: 0.9212\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2888 - accuracy: 0.9196 - val_loss: 0.2835 - val_accuracy: 0.9212\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2886 - accuracy: 0.9194 - val_loss: 0.2833 - val_accuracy: 0.9212\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2883 - accuracy: 0.9197 - val_loss: 0.2832 - val_accuracy: 0.9211\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2881 - accuracy: 0.9202 - val_loss: 0.2831 - val_accuracy: 0.9204\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2878 - accuracy: 0.9196 - val_loss: 0.2829 - val_accuracy: 0.9212\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2876 - accuracy: 0.9199 - val_loss: 0.2827 - val_accuracy: 0.9212\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2874 - accuracy: 0.9196 - val_loss: 0.2826 - val_accuracy: 0.9212\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2872 - accuracy: 0.9197 - val_loss: 0.2824 - val_accuracy: 0.9211\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2870 - accuracy: 0.9201 - val_loss: 0.2823 - val_accuracy: 0.9213\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2868 - accuracy: 0.9200 - val_loss: 0.2821 - val_accuracy: 0.9215\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2866 - accuracy: 0.9200 - val_loss: 0.2819 - val_accuracy: 0.9212\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2863 - accuracy: 0.9201 - val_loss: 0.2818 - val_accuracy: 0.9216\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2861 - accuracy: 0.9202 - val_loss: 0.2818 - val_accuracy: 0.9208\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2859 - accuracy: 0.9203 - val_loss: 0.2816 - val_accuracy: 0.9217\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2857 - accuracy: 0.9201 - val_loss: 0.2815 - val_accuracy: 0.9218\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2855 - accuracy: 0.9203 - val_loss: 0.2813 - val_accuracy: 0.9213\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2853 - accuracy: 0.9203 - val_loss: 0.2811 - val_accuracy: 0.9211\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2851 - accuracy: 0.9204 - val_loss: 0.2810 - val_accuracy: 0.9212\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2849 - accuracy: 0.9203 - val_loss: 0.2809 - val_accuracy: 0.9221\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2847 - accuracy: 0.9204 - val_loss: 0.2808 - val_accuracy: 0.9212\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2845 - accuracy: 0.9204 - val_loss: 0.2807 - val_accuracy: 0.9219\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2844 - accuracy: 0.9203 - val_loss: 0.2805 - val_accuracy: 0.9218\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2842 - accuracy: 0.9207 - val_loss: 0.2804 - val_accuracy: 0.9220\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2840 - accuracy: 0.9206 - val_loss: 0.2803 - val_accuracy: 0.9220\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2838 - accuracy: 0.9207 - val_loss: 0.2802 - val_accuracy: 0.9223\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2836 - accuracy: 0.9206 - val_loss: 0.2801 - val_accuracy: 0.9221\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2834 - accuracy: 0.9209 - val_loss: 0.2800 - val_accuracy: 0.9217\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2832 - accuracy: 0.9206 - val_loss: 0.2798 - val_accuracy: 0.9218\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2831 - accuracy: 0.9208 - val_loss: 0.2797 - val_accuracy: 0.9222\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2829 - accuracy: 0.9209 - val_loss: 0.2797 - val_accuracy: 0.9222\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2827 - accuracy: 0.9208 - val_loss: 0.2795 - val_accuracy: 0.9220\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2825 - accuracy: 0.9210 - val_loss: 0.2794 - val_accuracy: 0.9223\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2824 - accuracy: 0.9211 - val_loss: 0.2792 - val_accuracy: 0.9222\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2822 - accuracy: 0.9211 - val_loss: 0.2791 - val_accuracy: 0.9224\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2820 - accuracy: 0.9212 - val_loss: 0.2790 - val_accuracy: 0.9227\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2819 - accuracy: 0.9214 - val_loss: 0.2789 - val_accuracy: 0.9224\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2817 - accuracy: 0.9214 - val_loss: 0.2790 - val_accuracy: 0.9220\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2815 - accuracy: 0.9212 - val_loss: 0.2788 - val_accuracy: 0.9222\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2814 - accuracy: 0.9213 - val_loss: 0.2787 - val_accuracy: 0.9221\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2812 - accuracy: 0.9215 - val_loss: 0.2787 - val_accuracy: 0.9227\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2811 - accuracy: 0.9213 - val_loss: 0.2784 - val_accuracy: 0.9225\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2809 - accuracy: 0.9215 - val_loss: 0.2783 - val_accuracy: 0.9224\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2807 - accuracy: 0.9216 - val_loss: 0.2783 - val_accuracy: 0.9224\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2805 - accuracy: 0.9216 - val_loss: 0.2784 - val_accuracy: 0.9229\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2805 - accuracy: 0.9217 - val_loss: 0.2781 - val_accuracy: 0.9221\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2803 - accuracy: 0.9216 - val_loss: 0.2780 - val_accuracy: 0.9229\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2801 - accuracy: 0.9216 - val_loss: 0.2779 - val_accuracy: 0.9226\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2800 - accuracy: 0.9217 - val_loss: 0.2779 - val_accuracy: 0.9227\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2798 - accuracy: 0.9217 - val_loss: 0.2777 - val_accuracy: 0.9225\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2797 - accuracy: 0.9217 - val_loss: 0.2777 - val_accuracy: 0.9228\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2795 - accuracy: 0.9218 - val_loss: 0.2775 - val_accuracy: 0.9227\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2794 - accuracy: 0.9218 - val_loss: 0.2775 - val_accuracy: 0.9225\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2792 - accuracy: 0.9220 - val_loss: 0.2773 - val_accuracy: 0.9225\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2791 - accuracy: 0.9219 - val_loss: 0.2773 - val_accuracy: 0.9224\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2790 - accuracy: 0.9221 - val_loss: 0.2772 - val_accuracy: 0.9225\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2788 - accuracy: 0.9221 - val_loss: 0.2771 - val_accuracy: 0.9230\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2787 - accuracy: 0.9218 - val_loss: 0.2770 - val_accuracy: 0.9232\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2785 - accuracy: 0.9219 - val_loss: 0.2769 - val_accuracy: 0.9232\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2784 - accuracy: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.9227\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2782 - accuracy: 0.9222 - val_loss: 0.2768 - val_accuracy: 0.9227\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2781 - accuracy: 0.9224 - val_loss: 0.2767 - val_accuracy: 0.9224\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2780 - accuracy: 0.9221 - val_loss: 0.2766 - val_accuracy: 0.9227\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2778 - accuracy: 0.9227 - val_loss: 0.2766 - val_accuracy: 0.9231\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2777 - accuracy: 0.9224 - val_loss: 0.2765 - val_accuracy: 0.9226\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2776 - accuracy: 0.9224 - val_loss: 0.2763 - val_accuracy: 0.9232\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2774 - accuracy: 0.9223 - val_loss: 0.2763 - val_accuracy: 0.9226\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2773 - accuracy: 0.9224 - val_loss: 0.2762 - val_accuracy: 0.9229\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2772 - accuracy: 0.9226 - val_loss: 0.2762 - val_accuracy: 0.9235\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2770 - accuracy: 0.9225 - val_loss: 0.2760 - val_accuracy: 0.9235\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2769 - accuracy: 0.9225 - val_loss: 0.2760 - val_accuracy: 0.9234\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2768 - accuracy: 0.9228 - val_loss: 0.2759 - val_accuracy: 0.9232\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2767 - accuracy: 0.9229 - val_loss: 0.2758 - val_accuracy: 0.9234\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2765 - accuracy: 0.9227 - val_loss: 0.2758 - val_accuracy: 0.9237\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2764 - accuracy: 0.9229 - val_loss: 0.2757 - val_accuracy: 0.9242\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2763 - accuracy: 0.9227 - val_loss: 0.2756 - val_accuracy: 0.9236\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2762 - accuracy: 0.9229 - val_loss: 0.2755 - val_accuracy: 0.9232\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2761 - accuracy: 0.9228 - val_loss: 0.2755 - val_accuracy: 0.9233\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2759 - accuracy: 0.9229 - val_loss: 0.2754 - val_accuracy: 0.9237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9b234b0c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HVBmjiYpaTB"
      },
      "source": [
        "- 훈련하는 동안 유효성 성능을 측정하고자 훈련 데이터의 일부를 남겨둔다.\n",
        "- 모델 훈련 후 훈련 과정에서 모델이 한 번도 본 적 없는 새로운 예시의 테스트 집합으로 평가\n",
        "- `evaluate(X_test, Y_test)`로 `test_loss`와 `test_acc` 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxNnkjd8paTB",
        "outputId": "8e288db0-c804-4e2c-adbf-703a969545f7"
      },
      "source": [
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUv8k5RzpaTB"
      },
      "source": [
        "### 단순 TensorFlow 2.0 신경망 실행과 베이스라인 구축\n",
        "- 신경망의 아키텍처가 출력되고, 사용된 여러 계층의 유형, 출력 형태, 최적화해야 할 매개변수 개수(= 가중치 수)와 연결 방식 확인\n",
        "- 신경망은 48000개의 표본으로 훈련, 12000개의 표본은 검증을 위해 사용됨\n",
        "- 훈련 후에 테스트 집합에서 모델을 테스트\n",
        "\n",
        "### TensorFlow 2.0의 단순 신경망을 은닉층으로 개선\n",
        "- 개선법: 신경망에 계층 추가\n",
        "    - ∵ 추가 뉴런은 훈련 데이터에서 좀 더 복잡한 패턴을 학습하는 데 도움\n",
        "    - -> 계층 추가로 매개변수가 추가돼 모델이 더 복잡한 패턴을 기억할 수 있게 된다.\n",
        "    - -> 변경: 입력 계층 -> N_HIDDEN 뉴런과 활성화 함수 ReLU로 첫 번째 밀집 계층 추가(= hidden) -> N_HIDDEN 뉴런을 가진 2번째 은닉층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzMaFY9LpaTB",
        "outputId": "69ce5e08-63ad-48c7-df12-91832003623b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 신경망과 훈련\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10 # 출력 개수 = 숫자 개수\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # 검증에 남겨둘 훈련 집합 부분\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "# 레이블은 원핫 표기로 되어 있다.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train은 60000개 행의 28*28 값 -> 60000*784 형태로 변경\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 입력을 [0, 1] 사이로 정규화\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# 모델 구축\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/50\n",
            "48000/48000 [==============================] - 1s 29us/sample - loss: 1.4358 - accuracy: 0.6379 - val_loss: 0.7136 - val_accuracy: 0.8407\n",
            "Epoch 2/50\n",
            "  128/48000 [..............................] - ETA: 1s - loss: 0.8927 - accuracy: 0.7656"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.5795 - accuracy: 0.8541 - val_loss: 0.4430 - val_accuracy: 0.8852\n",
            "Epoch 3/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.4318 - accuracy: 0.8834 - val_loss: 0.3700 - val_accuracy: 0.8986\n",
            "Epoch 4/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3765 - accuracy: 0.8949 - val_loss: 0.3373 - val_accuracy: 0.9047\n",
            "Epoch 5/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3449 - accuracy: 0.9024 - val_loss: 0.3124 - val_accuracy: 0.9112\n",
            "Epoch 6/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3231 - accuracy: 0.9084 - val_loss: 0.2969 - val_accuracy: 0.9153\n",
            "Epoch 7/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3062 - accuracy: 0.9132 - val_loss: 0.2834 - val_accuracy: 0.9198\n",
            "Epoch 8/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2924 - accuracy: 0.9172 - val_loss: 0.2714 - val_accuracy: 0.9237\n",
            "Epoch 9/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2805 - accuracy: 0.9205 - val_loss: 0.2621 - val_accuracy: 0.9248\n",
            "Epoch 10/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2695 - accuracy: 0.9236 - val_loss: 0.2560 - val_accuracy: 0.9290\n",
            "Epoch 11/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2598 - accuracy: 0.9271 - val_loss: 0.2473 - val_accuracy: 0.9308\n",
            "Epoch 12/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2510 - accuracy: 0.9289 - val_loss: 0.2388 - val_accuracy: 0.9312\n",
            "Epoch 13/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2428 - accuracy: 0.9310 - val_loss: 0.2319 - val_accuracy: 0.9349\n",
            "Epoch 14/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2350 - accuracy: 0.9334 - val_loss: 0.2271 - val_accuracy: 0.9350\n",
            "Epoch 15/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.2275 - accuracy: 0.9351 - val_loss: 0.2200 - val_accuracy: 0.9384\n",
            "Epoch 16/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2207 - accuracy: 0.9373 - val_loss: 0.2143 - val_accuracy: 0.9396\n",
            "Epoch 17/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2143 - accuracy: 0.9396 - val_loss: 0.2080 - val_accuracy: 0.9414\n",
            "Epoch 18/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2078 - accuracy: 0.9412 - val_loss: 0.2032 - val_accuracy: 0.9428\n",
            "Epoch 19/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2018 - accuracy: 0.9433 - val_loss: 0.1980 - val_accuracy: 0.9451\n",
            "Epoch 20/50\n",
            "48000/48000 [==============================] - 1s 29us/sample - loss: 0.1961 - accuracy: 0.9445 - val_loss: 0.1941 - val_accuracy: 0.9454\n",
            "Epoch 21/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1907 - accuracy: 0.9467 - val_loss: 0.1890 - val_accuracy: 0.9473\n",
            "Epoch 22/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1855 - accuracy: 0.9477 - val_loss: 0.1845 - val_accuracy: 0.9485\n",
            "Epoch 23/50\n",
            "48000/48000 [==============================] - 1s 30us/sample - loss: 0.1805 - accuracy: 0.9487 - val_loss: 0.1808 - val_accuracy: 0.9503\n",
            "Epoch 24/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1758 - accuracy: 0.9501 - val_loss: 0.1777 - val_accuracy: 0.9507\n",
            "Epoch 25/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1714 - accuracy: 0.9519 - val_loss: 0.1733 - val_accuracy: 0.9531\n",
            "Epoch 26/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1673 - accuracy: 0.9521 - val_loss: 0.1713 - val_accuracy: 0.9522\n",
            "Epoch 27/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1630 - accuracy: 0.9538 - val_loss: 0.1683 - val_accuracy: 0.9532\n",
            "Epoch 28/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1591 - accuracy: 0.9547 - val_loss: 0.1642 - val_accuracy: 0.9553\n",
            "Epoch 29/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1555 - accuracy: 0.9558 - val_loss: 0.1612 - val_accuracy: 0.9565\n",
            "Epoch 30/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1518 - accuracy: 0.9566 - val_loss: 0.1583 - val_accuracy: 0.9575\n",
            "Epoch 31/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1485 - accuracy: 0.9578 - val_loss: 0.1559 - val_accuracy: 0.9575\n",
            "Epoch 32/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1448 - accuracy: 0.9591 - val_loss: 0.1537 - val_accuracy: 0.9585\n",
            "Epoch 33/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1419 - accuracy: 0.9597 - val_loss: 0.1519 - val_accuracy: 0.9582\n",
            "Epoch 34/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1387 - accuracy: 0.9607 - val_loss: 0.1499 - val_accuracy: 0.9582\n",
            "Epoch 35/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1358 - accuracy: 0.9618 - val_loss: 0.1472 - val_accuracy: 0.9595\n",
            "Epoch 36/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1329 - accuracy: 0.9623 - val_loss: 0.1452 - val_accuracy: 0.9597\n",
            "Epoch 37/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1302 - accuracy: 0.9635 - val_loss: 0.1426 - val_accuracy: 0.9599\n",
            "Epoch 38/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1275 - accuracy: 0.9643 - val_loss: 0.1410 - val_accuracy: 0.9604\n",
            "Epoch 39/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1251 - accuracy: 0.9650 - val_loss: 0.1399 - val_accuracy: 0.9606\n",
            "Epoch 40/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1226 - accuracy: 0.9658 - val_loss: 0.1373 - val_accuracy: 0.9620\n",
            "Epoch 41/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1202 - accuracy: 0.9667 - val_loss: 0.1357 - val_accuracy: 0.9620\n",
            "Epoch 42/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1179 - accuracy: 0.9671 - val_loss: 0.1349 - val_accuracy: 0.9624\n",
            "Epoch 43/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1157 - accuracy: 0.9674 - val_loss: 0.1330 - val_accuracy: 0.9620\n",
            "Epoch 44/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1133 - accuracy: 0.9685 - val_loss: 0.1321 - val_accuracy: 0.9628\n",
            "Epoch 45/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1114 - accuracy: 0.9690 - val_loss: 0.1296 - val_accuracy: 0.9634\n",
            "Epoch 46/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1094 - accuracy: 0.9693 - val_loss: 0.1288 - val_accuracy: 0.9638\n",
            "Epoch 47/50\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.1074 - accuracy: 0.9703 - val_loss: 0.1273 - val_accuracy: 0.9638\n",
            "Epoch 48/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1054 - accuracy: 0.9709 - val_loss: 0.1258 - val_accuracy: 0.9647\n",
            "Epoch 49/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1035 - accuracy: 0.9713 - val_loss: 0.1247 - val_accuracy: 0.9647\n",
            "Epoch 50/50\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.1017 - accuracy: 0.9720 - val_loss: 0.1232 - val_accuracy: 0.9653\n",
            "Test accuracy:  0.9639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UiroRMMpaTC"
      },
      "source": [
        "- `to_categorical(Y_train, NB_CLASSES)`: 배열 `Y_train`을 부류 개수만큼 열을 가진 행렬로 변환\n",
        "- 1번째 학습 vs. 2번째 학습(은닉층 2개 추가): 테스트 정확도 ↑, 반복 횟수 200 -> 50\n",
        "- **수렴 convergence**: 일정 에폭을 넘어서면 개선의 효과가 미미함\n",
        "\n",
        "### TensorFlow에서 드롭아웃(Dropout)으로 단순망 개선\n",
        "- 훈련 중에 은닉층 내부 밀집 신경망에 전파된 값 중 일부를 무작위로 제거 -> 성능 향상\n",
        "- **무작위 드롭아웃 Random Dropout**: 신경망의 일반화를 향상시키는 데 도움 되는 유용한 중복 패턴 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXxovtKLpaTC",
        "outputId": "9d4cb67f-2461-4399-8b0a-25495cbf61f8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# 신경망과 훈련\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # 출력 개수 = 숫자 개수\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATION을 위해 예약된 TRAIN의 양\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "# 레이블은 원핫 인코딩으로 표현\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train은 60000개 행의 28*28 값 -> 60000*784 형태로 변경\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 입력을 [0, 1] 사이로 정규화\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# 모델 구축\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 1.6911 - accuracy: 0.4684 - val_loss: 0.8806 - val_accuracy: 0.8147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.9110 - accuracy: 0.7175 - val_loss: 0.5224 - val_accuracy: 0.8698\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.6921 - accuracy: 0.7885 - val_loss: 0.4122 - val_accuracy: 0.8927\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.5934 - accuracy: 0.8190 - val_loss: 0.3643 - val_accuracy: 0.9025\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.5299 - accuracy: 0.8396 - val_loss: 0.3309 - val_accuracy: 0.9090\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4858 - accuracy: 0.8556 - val_loss: 0.3089 - val_accuracy: 0.9141\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4519 - accuracy: 0.8647 - val_loss: 0.2909 - val_accuracy: 0.9171\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.4302 - accuracy: 0.8730 - val_loss: 0.2760 - val_accuracy: 0.9213\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4098 - accuracy: 0.8779 - val_loss: 0.2646 - val_accuracy: 0.9237\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3930 - accuracy: 0.8839 - val_loss: 0.2537 - val_accuracy: 0.9256\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3764 - accuracy: 0.8891 - val_loss: 0.2455 - val_accuracy: 0.9273\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3643 - accuracy: 0.8935 - val_loss: 0.2352 - val_accuracy: 0.9308\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3491 - accuracy: 0.8971 - val_loss: 0.2278 - val_accuracy: 0.9324\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3373 - accuracy: 0.9002 - val_loss: 0.2206 - val_accuracy: 0.9339\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3280 - accuracy: 0.9037 - val_loss: 0.2145 - val_accuracy: 0.9374\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3198 - accuracy: 0.9066 - val_loss: 0.2088 - val_accuracy: 0.9389\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.3094 - accuracy: 0.9086 - val_loss: 0.2025 - val_accuracy: 0.9398\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.3041 - accuracy: 0.9098 - val_loss: 0.1969 - val_accuracy: 0.9419\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.2965 - accuracy: 0.9131 - val_loss: 0.1927 - val_accuracy: 0.9438\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2819 - accuracy: 0.9172 - val_loss: 0.1879 - val_accuracy: 0.9457\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2789 - accuracy: 0.9187 - val_loss: 0.1836 - val_accuracy: 0.9463\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.2723 - accuracy: 0.9193 - val_loss: 0.1807 - val_accuracy: 0.9468\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2709 - accuracy: 0.9193 - val_loss: 0.1762 - val_accuracy: 0.9488\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2632 - accuracy: 0.9227 - val_loss: 0.1726 - val_accuracy: 0.9495\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2581 - accuracy: 0.9246 - val_loss: 0.1696 - val_accuracy: 0.9499\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2500 - accuracy: 0.9259 - val_loss: 0.1656 - val_accuracy: 0.9513\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2480 - accuracy: 0.9284 - val_loss: 0.1625 - val_accuracy: 0.9524\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2409 - accuracy: 0.9288 - val_loss: 0.1604 - val_accuracy: 0.9531\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2435 - accuracy: 0.9284 - val_loss: 0.1582 - val_accuracy: 0.9535\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2370 - accuracy: 0.9307 - val_loss: 0.1549 - val_accuracy: 0.9545\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2351 - accuracy: 0.9317 - val_loss: 0.1538 - val_accuracy: 0.9547\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2274 - accuracy: 0.9337 - val_loss: 0.1507 - val_accuracy: 0.9557\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2264 - accuracy: 0.9334 - val_loss: 0.1495 - val_accuracy: 0.9562\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2204 - accuracy: 0.9346 - val_loss: 0.1463 - val_accuracy: 0.9569\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2201 - accuracy: 0.9357 - val_loss: 0.1443 - val_accuracy: 0.9572\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2147 - accuracy: 0.9361 - val_loss: 0.1423 - val_accuracy: 0.9582\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2124 - accuracy: 0.9375 - val_loss: 0.1403 - val_accuracy: 0.9577\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2078 - accuracy: 0.9384 - val_loss: 0.1389 - val_accuracy: 0.9581\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.2092 - accuracy: 0.9383 - val_loss: 0.1370 - val_accuracy: 0.9589\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2020 - accuracy: 0.9395 - val_loss: 0.1354 - val_accuracy: 0.9597\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2012 - accuracy: 0.9404 - val_loss: 0.1341 - val_accuracy: 0.9603\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1980 - accuracy: 0.9417 - val_loss: 0.1317 - val_accuracy: 0.9603\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1965 - accuracy: 0.9432 - val_loss: 0.1316 - val_accuracy: 0.9607\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1930 - accuracy: 0.9435 - val_loss: 0.1300 - val_accuracy: 0.9612\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1894 - accuracy: 0.9442 - val_loss: 0.1289 - val_accuracy: 0.9619\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1859 - accuracy: 0.9452 - val_loss: 0.1277 - val_accuracy: 0.9624\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1851 - accuracy: 0.9435 - val_loss: 0.1260 - val_accuracy: 0.9617\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1832 - accuracy: 0.9464 - val_loss: 0.1248 - val_accuracy: 0.9624\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1835 - accuracy: 0.9457 - val_loss: 0.1236 - val_accuracy: 0.9628\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1792 - accuracy: 0.9465 - val_loss: 0.1228 - val_accuracy: 0.9628\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1790 - accuracy: 0.9471 - val_loss: 0.1213 - val_accuracy: 0.9638\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1751 - accuracy: 0.9480 - val_loss: 0.1205 - val_accuracy: 0.9638\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1721 - accuracy: 0.9492 - val_loss: 0.1196 - val_accuracy: 0.9635\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1686 - accuracy: 0.9498 - val_loss: 0.1184 - val_accuracy: 0.9652\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1723 - accuracy: 0.9501 - val_loss: 0.1172 - val_accuracy: 0.9654\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1720 - accuracy: 0.9487 - val_loss: 0.1164 - val_accuracy: 0.9651\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1667 - accuracy: 0.9511 - val_loss: 0.1167 - val_accuracy: 0.9648\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1664 - accuracy: 0.9503 - val_loss: 0.1144 - val_accuracy: 0.9659\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1655 - accuracy: 0.9504 - val_loss: 0.1137 - val_accuracy: 0.9659\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1617 - accuracy: 0.9520 - val_loss: 0.1130 - val_accuracy: 0.9657\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1602 - accuracy: 0.9533 - val_loss: 0.1128 - val_accuracy: 0.9660\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1585 - accuracy: 0.9535 - val_loss: 0.1117 - val_accuracy: 0.9659\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1586 - accuracy: 0.9536 - val_loss: 0.1112 - val_accuracy: 0.9664\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1565 - accuracy: 0.9531 - val_loss: 0.1108 - val_accuracy: 0.9668\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1563 - accuracy: 0.9538 - val_loss: 0.1093 - val_accuracy: 0.9670\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1536 - accuracy: 0.9546 - val_loss: 0.1087 - val_accuracy: 0.9679\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1516 - accuracy: 0.9545 - val_loss: 0.1078 - val_accuracy: 0.9674\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1497 - accuracy: 0.9554 - val_loss: 0.1077 - val_accuracy: 0.9672\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1477 - accuracy: 0.9563 - val_loss: 0.1075 - val_accuracy: 0.9674\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1488 - accuracy: 0.9557 - val_loss: 0.1061 - val_accuracy: 0.9682\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1468 - accuracy: 0.9556 - val_loss: 0.1062 - val_accuracy: 0.9678\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1442 - accuracy: 0.9568 - val_loss: 0.1048 - val_accuracy: 0.9688\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1449 - accuracy: 0.9563 - val_loss: 0.1045 - val_accuracy: 0.9688\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1442 - accuracy: 0.9578 - val_loss: 0.1039 - val_accuracy: 0.9686\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1443 - accuracy: 0.9571 - val_loss: 0.1028 - val_accuracy: 0.9688\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1423 - accuracy: 0.9564 - val_loss: 0.1022 - val_accuracy: 0.9691\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1411 - accuracy: 0.9581 - val_loss: 0.1029 - val_accuracy: 0.9691\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1418 - accuracy: 0.9575 - val_loss: 0.1021 - val_accuracy: 0.9697\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1372 - accuracy: 0.9592 - val_loss: 0.1016 - val_accuracy: 0.9695\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1377 - accuracy: 0.9590 - val_loss: 0.1013 - val_accuracy: 0.9686\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1360 - accuracy: 0.9595 - val_loss: 0.1003 - val_accuracy: 0.9698\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1359 - accuracy: 0.9587 - val_loss: 0.1004 - val_accuracy: 0.9699\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1336 - accuracy: 0.9601 - val_loss: 0.0991 - val_accuracy: 0.9700\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1324 - accuracy: 0.9604 - val_loss: 0.0991 - val_accuracy: 0.9706\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1303 - accuracy: 0.9610 - val_loss: 0.0982 - val_accuracy: 0.9712\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1308 - accuracy: 0.9613 - val_loss: 0.0979 - val_accuracy: 0.9703\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1300 - accuracy: 0.9617 - val_loss: 0.0976 - val_accuracy: 0.9703\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1297 - accuracy: 0.9618 - val_loss: 0.0967 - val_accuracy: 0.9707\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1271 - accuracy: 0.9626 - val_loss: 0.0968 - val_accuracy: 0.9706\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1289 - accuracy: 0.9615 - val_loss: 0.0973 - val_accuracy: 0.9705\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1261 - accuracy: 0.9620 - val_loss: 0.0955 - val_accuracy: 0.9709\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1270 - accuracy: 0.9623 - val_loss: 0.0954 - val_accuracy: 0.9710\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1250 - accuracy: 0.9636 - val_loss: 0.0950 - val_accuracy: 0.9708\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1269 - accuracy: 0.9622 - val_loss: 0.0948 - val_accuracy: 0.9710\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1230 - accuracy: 0.9635 - val_loss: 0.0945 - val_accuracy: 0.9712\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1226 - accuracy: 0.9627 - val_loss: 0.0943 - val_accuracy: 0.9719\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1211 - accuracy: 0.9644 - val_loss: 0.0935 - val_accuracy: 0.9716\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1199 - accuracy: 0.9644 - val_loss: 0.0934 - val_accuracy: 0.9724\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1225 - accuracy: 0.9640 - val_loss: 0.0926 - val_accuracy: 0.9722\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1191 - accuracy: 0.9640 - val_loss: 0.0930 - val_accuracy: 0.9717\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1182 - accuracy: 0.9649 - val_loss: 0.0928 - val_accuracy: 0.9719\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1176 - accuracy: 0.9642 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1192 - accuracy: 0.9634 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1191 - accuracy: 0.9645 - val_loss: 0.0913 - val_accuracy: 0.9721\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1150 - accuracy: 0.9656 - val_loss: 0.0912 - val_accuracy: 0.9725\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1147 - accuracy: 0.9663 - val_loss: 0.0909 - val_accuracy: 0.9725\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1161 - accuracy: 0.9647 - val_loss: 0.0911 - val_accuracy: 0.9722\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1132 - accuracy: 0.9664 - val_loss: 0.0914 - val_accuracy: 0.9729\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1116 - accuracy: 0.9664 - val_loss: 0.0905 - val_accuracy: 0.9728\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1128 - accuracy: 0.9665 - val_loss: 0.0900 - val_accuracy: 0.9728\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1118 - accuracy: 0.9665 - val_loss: 0.0897 - val_accuracy: 0.9733\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1130 - accuracy: 0.9664 - val_loss: 0.0888 - val_accuracy: 0.9731\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1106 - accuracy: 0.9671 - val_loss: 0.0891 - val_accuracy: 0.9732\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1100 - accuracy: 0.9671 - val_loss: 0.0892 - val_accuracy: 0.9733\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1100 - accuracy: 0.9678 - val_loss: 0.0887 - val_accuracy: 0.9735\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1113 - accuracy: 0.9674 - val_loss: 0.0881 - val_accuracy: 0.9736\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1079 - accuracy: 0.9671 - val_loss: 0.0890 - val_accuracy: 0.9737\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1076 - accuracy: 0.9675 - val_loss: 0.0881 - val_accuracy: 0.9736\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1115 - accuracy: 0.9662 - val_loss: 0.0875 - val_accuracy: 0.9739\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1096 - accuracy: 0.9670 - val_loss: 0.0877 - val_accuracy: 0.9742\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1063 - accuracy: 0.9682 - val_loss: 0.0868 - val_accuracy: 0.9738\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1048 - accuracy: 0.9686 - val_loss: 0.0872 - val_accuracy: 0.9738\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1041 - accuracy: 0.9687 - val_loss: 0.0869 - val_accuracy: 0.9740\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1052 - accuracy: 0.9687 - val_loss: 0.0867 - val_accuracy: 0.9738\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1040 - accuracy: 0.9694 - val_loss: 0.0861 - val_accuracy: 0.9737\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1046 - accuracy: 0.9684 - val_loss: 0.0864 - val_accuracy: 0.9744\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1030 - accuracy: 0.9692 - val_loss: 0.0856 - val_accuracy: 0.9737\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1034 - accuracy: 0.9682 - val_loss: 0.0855 - val_accuracy: 0.9742\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1022 - accuracy: 0.9699 - val_loss: 0.0863 - val_accuracy: 0.9745\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0997 - accuracy: 0.9698 - val_loss: 0.0856 - val_accuracy: 0.9746\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1019 - accuracy: 0.9693 - val_loss: 0.0848 - val_accuracy: 0.9747\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0994 - accuracy: 0.9703 - val_loss: 0.0852 - val_accuracy: 0.9747\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0995 - accuracy: 0.9697 - val_loss: 0.0849 - val_accuracy: 0.9740\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0984 - accuracy: 0.9701 - val_loss: 0.0845 - val_accuracy: 0.9746\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0975 - accuracy: 0.9706 - val_loss: 0.0847 - val_accuracy: 0.9742\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0976 - accuracy: 0.9710 - val_loss: 0.0845 - val_accuracy: 0.9742\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0975 - accuracy: 0.9705 - val_loss: 0.0846 - val_accuracy: 0.9748\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0955 - accuracy: 0.9712 - val_loss: 0.0840 - val_accuracy: 0.9753\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0963 - accuracy: 0.9705 - val_loss: 0.0838 - val_accuracy: 0.9743\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0962 - accuracy: 0.9704 - val_loss: 0.0835 - val_accuracy: 0.9747\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0941 - accuracy: 0.9725 - val_loss: 0.0841 - val_accuracy: 0.9750\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0946 - accuracy: 0.9712 - val_loss: 0.0832 - val_accuracy: 0.9752\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0975 - accuracy: 0.9697 - val_loss: 0.0831 - val_accuracy: 0.9753\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0921 - accuracy: 0.9718 - val_loss: 0.0827 - val_accuracy: 0.9756\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0952 - accuracy: 0.9706 - val_loss: 0.0827 - val_accuracy: 0.9745\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0948 - accuracy: 0.9715 - val_loss: 0.0831 - val_accuracy: 0.9747\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0931 - accuracy: 0.9716 - val_loss: 0.0827 - val_accuracy: 0.9748\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0918 - accuracy: 0.9722 - val_loss: 0.0828 - val_accuracy: 0.9749\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0914 - accuracy: 0.9734 - val_loss: 0.0831 - val_accuracy: 0.9751\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0907 - accuracy: 0.9722 - val_loss: 0.0825 - val_accuracy: 0.9754\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0902 - accuracy: 0.9722 - val_loss: 0.0828 - val_accuracy: 0.9753\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0901 - accuracy: 0.9721 - val_loss: 0.0820 - val_accuracy: 0.9753\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0921 - accuracy: 0.9721 - val_loss: 0.0816 - val_accuracy: 0.9754\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0886 - accuracy: 0.9730 - val_loss: 0.0824 - val_accuracy: 0.9759\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0888 - accuracy: 0.9724 - val_loss: 0.0819 - val_accuracy: 0.9751\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0889 - accuracy: 0.9728 - val_loss: 0.0817 - val_accuracy: 0.9755\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0907 - accuracy: 0.9726 - val_loss: 0.0814 - val_accuracy: 0.9753\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0883 - accuracy: 0.9731 - val_loss: 0.0810 - val_accuracy: 0.9758\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0896 - accuracy: 0.9729 - val_loss: 0.0815 - val_accuracy: 0.9753\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0883 - accuracy: 0.9729 - val_loss: 0.0812 - val_accuracy: 0.9756\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0889 - accuracy: 0.9730 - val_loss: 0.0811 - val_accuracy: 0.9760\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0892 - accuracy: 0.9728 - val_loss: 0.0807 - val_accuracy: 0.9760\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0866 - accuracy: 0.9745 - val_loss: 0.0807 - val_accuracy: 0.9761\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0858 - accuracy: 0.9736 - val_loss: 0.0811 - val_accuracy: 0.9762\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0857 - accuracy: 0.9741 - val_loss: 0.0802 - val_accuracy: 0.9758\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0849 - accuracy: 0.9738 - val_loss: 0.0805 - val_accuracy: 0.9768\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0858 - accuracy: 0.9734 - val_loss: 0.0802 - val_accuracy: 0.9761\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0865 - accuracy: 0.9738 - val_loss: 0.0805 - val_accuracy: 0.9763\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0834 - accuracy: 0.9740 - val_loss: 0.0804 - val_accuracy: 0.9758\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0853 - accuracy: 0.9736 - val_loss: 0.0798 - val_accuracy: 0.9765\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0832 - accuracy: 0.9751 - val_loss: 0.0804 - val_accuracy: 0.9774\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0835 - accuracy: 0.9745 - val_loss: 0.0795 - val_accuracy: 0.9765\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0850 - accuracy: 0.9741 - val_loss: 0.0795 - val_accuracy: 0.9768\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0833 - accuracy: 0.9740 - val_loss: 0.0801 - val_accuracy: 0.9770\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0815 - accuracy: 0.9750 - val_loss: 0.0804 - val_accuracy: 0.9768\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0820 - accuracy: 0.9750 - val_loss: 0.0792 - val_accuracy: 0.9764\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0808 - accuracy: 0.9752 - val_loss: 0.0790 - val_accuracy: 0.9764\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0803 - accuracy: 0.9760 - val_loss: 0.0798 - val_accuracy: 0.9768\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0814 - accuracy: 0.9749 - val_loss: 0.0793 - val_accuracy: 0.9769\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0796 - accuracy: 0.9756 - val_loss: 0.0790 - val_accuracy: 0.9772\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0829 - accuracy: 0.9741 - val_loss: 0.0784 - val_accuracy: 0.9767\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0792 - accuracy: 0.9758 - val_loss: 0.0789 - val_accuracy: 0.9763\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0828 - accuracy: 0.9750 - val_loss: 0.0784 - val_accuracy: 0.9775\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0793 - accuracy: 0.9754 - val_loss: 0.0791 - val_accuracy: 0.9770\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0789 - accuracy: 0.9758 - val_loss: 0.0791 - val_accuracy: 0.9774\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0794 - accuracy: 0.9761 - val_loss: 0.0790 - val_accuracy: 0.9764\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0803 - accuracy: 0.9750 - val_loss: 0.0790 - val_accuracy: 0.9770\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0772 - accuracy: 0.9760 - val_loss: 0.0784 - val_accuracy: 0.9770\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0790 - accuracy: 0.9749 - val_loss: 0.0790 - val_accuracy: 0.9774\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0769 - accuracy: 0.9768 - val_loss: 0.0781 - val_accuracy: 0.9778\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0790 - accuracy: 0.9752 - val_loss: 0.0779 - val_accuracy: 0.9774\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0765 - accuracy: 0.9766 - val_loss: 0.0784 - val_accuracy: 0.9775\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0796 - accuracy: 0.9754 - val_loss: 0.0782 - val_accuracy: 0.9775\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0762 - accuracy: 0.9772 - val_loss: 0.0776 - val_accuracy: 0.9771\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0763 - accuracy: 0.9768 - val_loss: 0.0779 - val_accuracy: 0.9778\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0749 - accuracy: 0.9778 - val_loss: 0.0778 - val_accuracy: 0.9781\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0765 - accuracy: 0.9759 - val_loss: 0.0771 - val_accuracy: 0.9778\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0750 - accuracy: 0.9765 - val_loss: 0.0769 - val_accuracy: 0.9782\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0745 - accuracy: 0.9771 - val_loss: 0.0773 - val_accuracy: 0.9781\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0776 - accuracy: 0.9758 - val_loss: 0.0770 - val_accuracy: 0.9776\n",
            "Test accuracy:  0.9765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAm4FirpaTD"
      },
      "source": [
        "- 내부 은닉층에서 무작위로 드롭아웃하는 신경망이 테스트 집합에 포함된 낯선(unseen) 예시를 잘 '일반화'한다.\n",
        "    - ∵ 각각의 뉴런이 자기 이웃에 의존할 수 없다는 것을 인식, 중복된 방식으로 정보가 저장되도록 강제\n",
        "- 테스트 중에는 드롭아웃이 없으므로, 모든 뉴런이 사용된다.\n",
        "- 훈련 정확도는 테스트 정확도보다 높아야 한다. 그렇지 않다면 에폭의 수를 더 늘려야 한다.\n",
        "\n",
        "### TensorFlow 2.0에서 여러 Optimizer 테스트\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117948802-72369c00-b34c-11eb-93ae-9d2cd74a3a09.png)\n",
        "\n",
        "- 그래디언트 하강(Gradient Descent, GD)\n",
        "    - 하나의 단일 변수 w에 대해 일반적인 비용 함수 C(w)가 있다고 가정하자.\n",
        "    - 목표: 출발점 w0에서 아주 조금씩 움직이면서 경사(함수 C)면을 따라 내려가 도랑(최소 Cmin) 찾기\n",
        "    - 각 단계 r에서 gradient = 최대 증가 방향 -> 단계 r에서 도달한 지점 wr에서 계산된 편미분 값 ∂C/∂w\n",
        "    - => 반대 방향인 -(∂C/∂w)(wr)을 택하면 도랑을 향해 나아갈 수 있다.\n",
        "    - 학습률 η(>= 0): 다음 단계의 보폭\n",
        "        - η이 너무 작다면 천천히 이동\n",
        "        - η이 너무 크다면 도랑을 지나칠 가능성이 있다.\n",
        "    - sigmoid function\n",
        "        - 연속이고 미분 가능\n",
        "        - 𝜎(𝑥)= 1/(1+𝑒^(−𝑥))이면, d𝜎(𝑥)/d(x) = 𝜎(𝑥)(1-𝜎(𝑥))\n",
        "    - ReLU function\n",
        "        - 0에서 미분 불가능\n",
        "        - -> 0에서 미분 값을 0이나 1로 임의 지정하면 전체 범위로 확장 가능\n",
        "        - y = max(0, x)의 부분 미분 dy/dx = 0(x <= 0), 1(x > 0) -> GD를 통해 망을 최적화할 수 있다.\n",
        "- TensorFlow에는 GD의 속도를 높이기 위한 변형인 SGD와 RMSProp, Adam 등의 최적화 기술이 제공된다.\n",
        "    - RMSProp, Adam은 SGD의 가속 구성 요소 외에도 momentum(속도 요소) 개념 포함 -> 많은 계산을 통한 더 빠른 수렴\n",
        "        - momentum을 통해 SGD를 유관 방향으로 가속화하고 이동하는 것을 줄일 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQGFnHKwpaTD",
        "outputId": "aad34f91-f4f0-4084-9273-4436171bf30c"
      },
      "source": [
        "# RMSProp, epochs = 200\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# 신경망과 훈련\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # 출력 개수 = 숫자 개수\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATION을 위해 예약된 TRAIN의 양\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "# 레이블은 원핫 인코딩으로 표현\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train은 60000개 행의 28*28 값 -> 60000*784 형태로 변경\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 입력을 [0, 1] 사이로 정규화\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# 모델 구축\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 모델 컴파일\n",
        "# 변경 부분\n",
        "model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "47616/48000 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.8571"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.4723 - accuracy: 0.8575 - val_loss: 0.1786 - val_accuracy: 0.9469\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.2224 - accuracy: 0.9338 - val_loss: 0.1324 - val_accuracy: 0.9599\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1737 - accuracy: 0.9481 - val_loss: 0.1204 - val_accuracy: 0.9639\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1505 - accuracy: 0.9559 - val_loss: 0.1072 - val_accuracy: 0.9688\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1321 - accuracy: 0.9600 - val_loss: 0.1118 - val_accuracy: 0.9670\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1254 - accuracy: 0.9624 - val_loss: 0.1066 - val_accuracy: 0.9718\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1102 - accuracy: 0.9681 - val_loss: 0.1005 - val_accuracy: 0.9723\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1046 - accuracy: 0.9694 - val_loss: 0.1041 - val_accuracy: 0.9717\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0995 - accuracy: 0.9709 - val_loss: 0.0965 - val_accuracy: 0.9750\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0918 - accuracy: 0.9721 - val_loss: 0.1023 - val_accuracy: 0.9751\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0884 - accuracy: 0.9736 - val_loss: 0.0995 - val_accuracy: 0.9742\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0854 - accuracy: 0.9743 - val_loss: 0.0984 - val_accuracy: 0.9747\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0851 - accuracy: 0.9751 - val_loss: 0.1023 - val_accuracy: 0.9747\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0751 - accuracy: 0.9771 - val_loss: 0.1057 - val_accuracy: 0.9745\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0801 - accuracy: 0.9763 - val_loss: 0.1002 - val_accuracy: 0.9774\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0754 - accuracy: 0.9778 - val_loss: 0.1086 - val_accuracy: 0.9771\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0738 - accuracy: 0.9789 - val_loss: 0.1078 - val_accuracy: 0.9774\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0719 - accuracy: 0.9787 - val_loss: 0.1068 - val_accuracy: 0.9769\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0697 - accuracy: 0.9793 - val_loss: 0.1090 - val_accuracy: 0.9773\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0680 - accuracy: 0.9799 - val_loss: 0.1120 - val_accuracy: 0.9776\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0679 - accuracy: 0.9803 - val_loss: 0.1129 - val_accuracy: 0.9780\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0646 - accuracy: 0.9807 - val_loss: 0.1113 - val_accuracy: 0.9783\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0676 - accuracy: 0.9812 - val_loss: 0.1100 - val_accuracy: 0.9764\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0673 - accuracy: 0.9802 - val_loss: 0.1139 - val_accuracy: 0.9791\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0630 - accuracy: 0.9814 - val_loss: 0.1281 - val_accuracy: 0.9774\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0627 - accuracy: 0.9819 - val_loss: 0.1309 - val_accuracy: 0.9770\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0628 - accuracy: 0.9825 - val_loss: 0.1261 - val_accuracy: 0.9781\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0636 - accuracy: 0.9821 - val_loss: 0.1204 - val_accuracy: 0.9773\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0631 - accuracy: 0.9825 - val_loss: 0.1234 - val_accuracy: 0.9790\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0593 - accuracy: 0.9822 - val_loss: 0.1249 - val_accuracy: 0.9779\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0610 - accuracy: 0.9831 - val_loss: 0.1303 - val_accuracy: 0.9776\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.1346 - val_accuracy: 0.9771\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0547 - accuracy: 0.9841 - val_loss: 0.1349 - val_accuracy: 0.9772\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0561 - accuracy: 0.9844 - val_loss: 0.1368 - val_accuracy: 0.9768\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0559 - accuracy: 0.9851 - val_loss: 0.1411 - val_accuracy: 0.9774\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0588 - accuracy: 0.9834 - val_loss: 0.1344 - val_accuracy: 0.9780\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0573 - accuracy: 0.9841 - val_loss: 0.1329 - val_accuracy: 0.9775\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0578 - accuracy: 0.9841 - val_loss: 0.1492 - val_accuracy: 0.9771\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0554 - accuracy: 0.9848 - val_loss: 0.1458 - val_accuracy: 0.9780\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0564 - accuracy: 0.9846 - val_loss: 0.1449 - val_accuracy: 0.9774\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0538 - accuracy: 0.9856 - val_loss: 0.1438 - val_accuracy: 0.9778\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0584 - accuracy: 0.9846 - val_loss: 0.1451 - val_accuracy: 0.9765\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0554 - accuracy: 0.9852 - val_loss: 0.1484 - val_accuracy: 0.9776\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0577 - accuracy: 0.9852 - val_loss: 0.1518 - val_accuracy: 0.9759\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0541 - accuracy: 0.9850 - val_loss: 0.1567 - val_accuracy: 0.9775\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0537 - accuracy: 0.9853 - val_loss: 0.1578 - val_accuracy: 0.9772\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0560 - accuracy: 0.9858 - val_loss: 0.1593 - val_accuracy: 0.9768\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0548 - accuracy: 0.9858 - val_loss: 0.1586 - val_accuracy: 0.9763\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0553 - accuracy: 0.9849 - val_loss: 0.1549 - val_accuracy: 0.9786\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0513 - accuracy: 0.9862 - val_loss: 0.1599 - val_accuracy: 0.9780\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0549 - accuracy: 0.9859 - val_loss: 0.1631 - val_accuracy: 0.9767\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0533 - accuracy: 0.9858 - val_loss: 0.1876 - val_accuracy: 0.9761\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0540 - accuracy: 0.9860 - val_loss: 0.1739 - val_accuracy: 0.9772\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0521 - accuracy: 0.9859 - val_loss: 0.1717 - val_accuracy: 0.9769\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0530 - accuracy: 0.9858 - val_loss: 0.1698 - val_accuracy: 0.9770\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0534 - accuracy: 0.9864 - val_loss: 0.1802 - val_accuracy: 0.9766\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0521 - accuracy: 0.9865 - val_loss: 0.1695 - val_accuracy: 0.9786\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0556 - accuracy: 0.9861 - val_loss: 0.1826 - val_accuracy: 0.9774\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0506 - accuracy: 0.9864 - val_loss: 0.1841 - val_accuracy: 0.9773\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0500 - accuracy: 0.9870 - val_loss: 0.1787 - val_accuracy: 0.9781\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0518 - accuracy: 0.9873 - val_loss: 0.1805 - val_accuracy: 0.9778\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9871 - val_loss: 0.1776 - val_accuracy: 0.9766\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9870 - val_loss: 0.1951 - val_accuracy: 0.9768\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0539 - accuracy: 0.9865 - val_loss: 0.1959 - val_accuracy: 0.9763\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0505 - accuracy: 0.9869 - val_loss: 0.1922 - val_accuracy: 0.9773\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0500 - accuracy: 0.9869 - val_loss: 0.1804 - val_accuracy: 0.9773\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0513 - accuracy: 0.9870 - val_loss: 0.2048 - val_accuracy: 0.9775\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0519 - accuracy: 0.9871 - val_loss: 0.1994 - val_accuracy: 0.9764\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0529 - accuracy: 0.9868 - val_loss: 0.1835 - val_accuracy: 0.9762\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0528 - accuracy: 0.9873 - val_loss: 0.1819 - val_accuracy: 0.9764\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0523 - accuracy: 0.9875 - val_loss: 0.1916 - val_accuracy: 0.9778\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0494 - accuracy: 0.9877 - val_loss: 0.1999 - val_accuracy: 0.9771\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9872 - val_loss: 0.1916 - val_accuracy: 0.9785\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0520 - accuracy: 0.9875 - val_loss: 0.2013 - val_accuracy: 0.9780\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0510 - accuracy: 0.9880 - val_loss: 0.1948 - val_accuracy: 0.9772\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0487 - accuracy: 0.9875 - val_loss: 0.2008 - val_accuracy: 0.9767\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0459 - accuracy: 0.9880 - val_loss: 0.2065 - val_accuracy: 0.9770\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0548 - accuracy: 0.9870 - val_loss: 0.1937 - val_accuracy: 0.9785\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0523 - accuracy: 0.9874 - val_loss: 0.1925 - val_accuracy: 0.9776\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0462 - accuracy: 0.9877 - val_loss: 0.2057 - val_accuracy: 0.9776\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0499 - accuracy: 0.9876 - val_loss: 0.2029 - val_accuracy: 0.9775\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0535 - accuracy: 0.9869 - val_loss: 0.2133 - val_accuracy: 0.9774\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0518 - accuracy: 0.9867 - val_loss: 0.2031 - val_accuracy: 0.9762\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0515 - accuracy: 0.9875 - val_loss: 0.2103 - val_accuracy: 0.9766\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0499 - accuracy: 0.9875 - val_loss: 0.2284 - val_accuracy: 0.9760\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0526 - accuracy: 0.9875 - val_loss: 0.2078 - val_accuracy: 0.9765\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0505 - accuracy: 0.9883 - val_loss: 0.2024 - val_accuracy: 0.9772\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0480 - accuracy: 0.9877 - val_loss: 0.2097 - val_accuracy: 0.9783\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0518 - accuracy: 0.9878 - val_loss: 0.2207 - val_accuracy: 0.9768\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0492 - accuracy: 0.9883 - val_loss: 0.2254 - val_accuracy: 0.9778\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0465 - accuracy: 0.9884 - val_loss: 0.2266 - val_accuracy: 0.9786\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0490 - accuracy: 0.9880 - val_loss: 0.2309 - val_accuracy: 0.9763\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0504 - accuracy: 0.9885 - val_loss: 0.2244 - val_accuracy: 0.9769\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0507 - accuracy: 0.9879 - val_loss: 0.2380 - val_accuracy: 0.9772\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0484 - accuracy: 0.9879 - val_loss: 0.2130 - val_accuracy: 0.9786\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0476 - accuracy: 0.9883 - val_loss: 0.2252 - val_accuracy: 0.9783\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0485 - accuracy: 0.9891 - val_loss: 0.2284 - val_accuracy: 0.9768\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0570 - accuracy: 0.9887 - val_loss: 0.2245 - val_accuracy: 0.9773\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0496 - accuracy: 0.9879 - val_loss: 0.2328 - val_accuracy: 0.9766\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0521 - accuracy: 0.9883 - val_loss: 0.2206 - val_accuracy: 0.9779\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0488 - accuracy: 0.9884 - val_loss: 0.2203 - val_accuracy: 0.9779\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9890 - val_loss: 0.2403 - val_accuracy: 0.9769\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0526 - accuracy: 0.9880 - val_loss: 0.2337 - val_accuracy: 0.9776\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0498 - accuracy: 0.9884 - val_loss: 0.2330 - val_accuracy: 0.9772\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0514 - accuracy: 0.9892 - val_loss: 0.2311 - val_accuracy: 0.9781\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0464 - accuracy: 0.9884 - val_loss: 0.2366 - val_accuracy: 0.9773\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0490 - accuracy: 0.9891 - val_loss: 0.2471 - val_accuracy: 0.9769\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0539 - accuracy: 0.9885 - val_loss: 0.2245 - val_accuracy: 0.9776\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9881 - val_loss: 0.2361 - val_accuracy: 0.9772\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0531 - accuracy: 0.9882 - val_loss: 0.2303 - val_accuracy: 0.9770\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0477 - accuracy: 0.9891 - val_loss: 0.2599 - val_accuracy: 0.9768\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0510 - accuracy: 0.9891 - val_loss: 0.2431 - val_accuracy: 0.9775\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0483 - accuracy: 0.9894 - val_loss: 0.2431 - val_accuracy: 0.9775\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9886 - val_loss: 0.2583 - val_accuracy: 0.9770\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9884 - val_loss: 0.2599 - val_accuracy: 0.9764\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0514 - accuracy: 0.9886 - val_loss: 0.2663 - val_accuracy: 0.9773\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0511 - accuracy: 0.9888 - val_loss: 0.2345 - val_accuracy: 0.9773\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0525 - accuracy: 0.9884 - val_loss: 0.2443 - val_accuracy: 0.9768\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0555 - accuracy: 0.9883 - val_loss: 0.2585 - val_accuracy: 0.9766\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0534 - accuracy: 0.9881 - val_loss: 0.2496 - val_accuracy: 0.9758\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0532 - accuracy: 0.9881 - val_loss: 0.2380 - val_accuracy: 0.9767\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0504 - accuracy: 0.9887 - val_loss: 0.2562 - val_accuracy: 0.9752\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0482 - accuracy: 0.9889 - val_loss: 0.2593 - val_accuracy: 0.9762\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0489 - accuracy: 0.9890 - val_loss: 0.2554 - val_accuracy: 0.9780\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0542 - accuracy: 0.9882 - val_loss: 0.2513 - val_accuracy: 0.9776\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0510 - accuracy: 0.9891 - val_loss: 0.2508 - val_accuracy: 0.9768\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.2654 - val_accuracy: 0.9776\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0492 - accuracy: 0.9889 - val_loss: 0.2510 - val_accuracy: 0.9778\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0524 - accuracy: 0.9893 - val_loss: 0.2682 - val_accuracy: 0.9766\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0507 - accuracy: 0.9891 - val_loss: 0.2596 - val_accuracy: 0.9759\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0498 - accuracy: 0.9888 - val_loss: 0.2701 - val_accuracy: 0.9762\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0503 - accuracy: 0.9895 - val_loss: 0.2750 - val_accuracy: 0.9787\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0495 - accuracy: 0.9897 - val_loss: 0.2833 - val_accuracy: 0.9770\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9890 - val_loss: 0.2544 - val_accuracy: 0.9778\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0535 - accuracy: 0.9892 - val_loss: 0.2746 - val_accuracy: 0.9758\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.2558 - val_accuracy: 0.9770\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0518 - accuracy: 0.9891 - val_loss: 0.2413 - val_accuracy: 0.9767\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0490 - accuracy: 0.9891 - val_loss: 0.2599 - val_accuracy: 0.9766\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0469 - accuracy: 0.9891 - val_loss: 0.2760 - val_accuracy: 0.9766\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0537 - accuracy: 0.9887 - val_loss: 0.2527 - val_accuracy: 0.9762\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0576 - accuracy: 0.9877 - val_loss: 0.2660 - val_accuracy: 0.9784\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0554 - accuracy: 0.9880 - val_loss: 0.2590 - val_accuracy: 0.9766\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0525 - accuracy: 0.9889 - val_loss: 0.2492 - val_accuracy: 0.9777\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9890 - val_loss: 0.2751 - val_accuracy: 0.9762\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0485 - accuracy: 0.9902 - val_loss: 0.2582 - val_accuracy: 0.9778\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0486 - accuracy: 0.9898 - val_loss: 0.2713 - val_accuracy: 0.9778\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0553 - accuracy: 0.9887 - val_loss: 0.2713 - val_accuracy: 0.9774\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0508 - accuracy: 0.9894 - val_loss: 0.2732 - val_accuracy: 0.9776\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0496 - accuracy: 0.9896 - val_loss: 0.2710 - val_accuracy: 0.9772\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0521 - accuracy: 0.9892 - val_loss: 0.2877 - val_accuracy: 0.9768\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0584 - accuracy: 0.9883 - val_loss: 0.2810 - val_accuracy: 0.9769\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0555 - accuracy: 0.9890 - val_loss: 0.2774 - val_accuracy: 0.9780\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0513 - accuracy: 0.9899 - val_loss: 0.2661 - val_accuracy: 0.9778\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0524 - accuracy: 0.9889 - val_loss: 0.2632 - val_accuracy: 0.9768\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0488 - accuracy: 0.9897 - val_loss: 0.2788 - val_accuracy: 0.9774\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0486 - accuracy: 0.9894 - val_loss: 0.2663 - val_accuracy: 0.9772\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9892 - val_loss: 0.2853 - val_accuracy: 0.9773\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0529 - accuracy: 0.9889 - val_loss: 0.2806 - val_accuracy: 0.9772\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0461 - accuracy: 0.9902 - val_loss: 0.3113 - val_accuracy: 0.9770\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0505 - accuracy: 0.9896 - val_loss: 0.2833 - val_accuracy: 0.9767\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0494 - accuracy: 0.9898 - val_loss: 0.2740 - val_accuracy: 0.9777\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0500 - accuracy: 0.9890 - val_loss: 0.2863 - val_accuracy: 0.9762\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9888 - val_loss: 0.2765 - val_accuracy: 0.9780\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0484 - accuracy: 0.9902 - val_loss: 0.2756 - val_accuracy: 0.9775\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0528 - accuracy: 0.9899 - val_loss: 0.2811 - val_accuracy: 0.9766\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0543 - accuracy: 0.9894 - val_loss: 0.2936 - val_accuracy: 0.9776\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0516 - accuracy: 0.9892 - val_loss: 0.2924 - val_accuracy: 0.9768\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0465 - accuracy: 0.9899 - val_loss: 0.2957 - val_accuracy: 0.9771\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0497 - accuracy: 0.9893 - val_loss: 0.2952 - val_accuracy: 0.9768\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0499 - accuracy: 0.9898 - val_loss: 0.2890 - val_accuracy: 0.9770\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0507 - accuracy: 0.9892 - val_loss: 0.2843 - val_accuracy: 0.9776\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0511 - accuracy: 0.9897 - val_loss: 0.3011 - val_accuracy: 0.9779\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0472 - accuracy: 0.9902 - val_loss: 0.2973 - val_accuracy: 0.9770\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0524 - accuracy: 0.9899 - val_loss: 0.2974 - val_accuracy: 0.9772\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0540 - accuracy: 0.9889 - val_loss: 0.2888 - val_accuracy: 0.9775\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0545 - accuracy: 0.9895 - val_loss: 0.2828 - val_accuracy: 0.9767\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0537 - accuracy: 0.9895 - val_loss: 0.2980 - val_accuracy: 0.9779\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0525 - accuracy: 0.9898 - val_loss: 0.3004 - val_accuracy: 0.9769\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0546 - accuracy: 0.9893 - val_loss: 0.3089 - val_accuracy: 0.9773\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0514 - accuracy: 0.9900 - val_loss: 0.3222 - val_accuracy: 0.9769\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0560 - accuracy: 0.9893 - val_loss: 0.2967 - val_accuracy: 0.9772\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0611 - accuracy: 0.9887 - val_loss: 0.2962 - val_accuracy: 0.9768\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0523 - accuracy: 0.9891 - val_loss: 0.2949 - val_accuracy: 0.9768\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0498 - accuracy: 0.9900 - val_loss: 0.2801 - val_accuracy: 0.9779\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0483 - accuracy: 0.9898 - val_loss: 0.2906 - val_accuracy: 0.9776\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9895 - val_loss: 0.3337 - val_accuracy: 0.9768\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0545 - accuracy: 0.9891 - val_loss: 0.3137 - val_accuracy: 0.9771\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0476 - accuracy: 0.9903 - val_loss: 0.3081 - val_accuracy: 0.9781\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0525 - accuracy: 0.9897 - val_loss: 0.2887 - val_accuracy: 0.9777\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0536 - accuracy: 0.9893 - val_loss: 0.3169 - val_accuracy: 0.9782\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0550 - accuracy: 0.9897 - val_loss: 0.3196 - val_accuracy: 0.9761\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0507 - accuracy: 0.9896 - val_loss: 0.3055 - val_accuracy: 0.9779\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0482 - accuracy: 0.9900 - val_loss: 0.3178 - val_accuracy: 0.9768\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0552 - accuracy: 0.9898 - val_loss: 0.3192 - val_accuracy: 0.9778\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0577 - accuracy: 0.9889 - val_loss: 0.3191 - val_accuracy: 0.9766\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0546 - accuracy: 0.9892 - val_loss: 0.3116 - val_accuracy: 0.9777\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0576 - accuracy: 0.9885 - val_loss: 0.2998 - val_accuracy: 0.9769\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0494 - accuracy: 0.9903 - val_loss: 0.3199 - val_accuracy: 0.9769\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0504 - accuracy: 0.9898 - val_loss: 0.3293 - val_accuracy: 0.9772\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0530 - accuracy: 0.9902 - val_loss: 0.2981 - val_accuracy: 0.9771\n",
            "Test accuracy:  0.9781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt0UTVu4paTE",
        "outputId": "d876325c-7dc4-4047-b1c6-1a3200315ef3"
      },
      "source": [
        "# RMSProp, epochs = 250\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# 신경망과 훈련\n",
        "EPOCHS = 250\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # 출력 개수 = 숫자 개수\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATION을 위해 예약된 TRAIN의 양\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "# 레이블은 원핫 인코딩으로 표현\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train은 60000개 행의 28*28 값 -> 60000*784 형태로 변경\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 입력을 [0, 1] 사이로 정규화\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# 모델 구축\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 모델 컴파일\n",
        "# 변경 부분\n",
        "model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/250\n",
            "47104/48000 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.8567"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.4700 - accuracy: 0.8581 - val_loss: 0.1716 - val_accuracy: 0.9508\n",
            "Epoch 2/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.2192 - accuracy: 0.9351 - val_loss: 0.1384 - val_accuracy: 0.9590\n",
            "Epoch 3/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.1678 - accuracy: 0.9506 - val_loss: 0.1162 - val_accuracy: 0.9668\n",
            "Epoch 4/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1441 - accuracy: 0.9578 - val_loss: 0.1024 - val_accuracy: 0.9706\n",
            "Epoch 5/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1266 - accuracy: 0.9621 - val_loss: 0.1021 - val_accuracy: 0.9722\n",
            "Epoch 6/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1164 - accuracy: 0.9653 - val_loss: 0.0982 - val_accuracy: 0.9712\n",
            "Epoch 7/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.1077 - accuracy: 0.9672 - val_loss: 0.0923 - val_accuracy: 0.9746\n",
            "Epoch 8/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1011 - accuracy: 0.9698 - val_loss: 0.0929 - val_accuracy: 0.9750\n",
            "Epoch 9/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0943 - accuracy: 0.9722 - val_loss: 0.0977 - val_accuracy: 0.9748\n",
            "Epoch 10/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0896 - accuracy: 0.9729 - val_loss: 0.0949 - val_accuracy: 0.9762\n",
            "Epoch 11/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0854 - accuracy: 0.9744 - val_loss: 0.0953 - val_accuracy: 0.9753\n",
            "Epoch 12/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0847 - accuracy: 0.9750 - val_loss: 0.0892 - val_accuracy: 0.9770\n",
            "Epoch 13/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0781 - accuracy: 0.9765 - val_loss: 0.1001 - val_accuracy: 0.9747\n",
            "Epoch 14/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0762 - accuracy: 0.9774 - val_loss: 0.1066 - val_accuracy: 0.9750\n",
            "Epoch 15/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0784 - accuracy: 0.9768 - val_loss: 0.0969 - val_accuracy: 0.9775\n",
            "Epoch 16/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0730 - accuracy: 0.9788 - val_loss: 0.1024 - val_accuracy: 0.9766\n",
            "Epoch 17/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0700 - accuracy: 0.9794 - val_loss: 0.1058 - val_accuracy: 0.9778\n",
            "Epoch 18/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0719 - accuracy: 0.9791 - val_loss: 0.1057 - val_accuracy: 0.9771\n",
            "Epoch 19/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0677 - accuracy: 0.9803 - val_loss: 0.1024 - val_accuracy: 0.9764\n",
            "Epoch 20/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0660 - accuracy: 0.9803 - val_loss: 0.1109 - val_accuracy: 0.9765\n",
            "Epoch 21/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.1099 - val_accuracy: 0.9776\n",
            "Epoch 22/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0641 - accuracy: 0.9815 - val_loss: 0.1115 - val_accuracy: 0.9780\n",
            "Epoch 23/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0652 - accuracy: 0.9811 - val_loss: 0.1107 - val_accuracy: 0.9775\n",
            "Epoch 24/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0634 - accuracy: 0.9821 - val_loss: 0.1167 - val_accuracy: 0.9778\n",
            "Epoch 25/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0615 - accuracy: 0.9820 - val_loss: 0.1144 - val_accuracy: 0.9787\n",
            "Epoch 26/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0591 - accuracy: 0.9831 - val_loss: 0.1255 - val_accuracy: 0.9768\n",
            "Epoch 27/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0594 - accuracy: 0.9828 - val_loss: 0.1147 - val_accuracy: 0.9785\n",
            "Epoch 28/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0612 - accuracy: 0.9825 - val_loss: 0.1177 - val_accuracy: 0.9772\n",
            "Epoch 29/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.1212 - val_accuracy: 0.9789\n",
            "Epoch 30/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0613 - accuracy: 0.9834 - val_loss: 0.1225 - val_accuracy: 0.9793\n",
            "Epoch 31/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0563 - accuracy: 0.9844 - val_loss: 0.1271 - val_accuracy: 0.9778\n",
            "Epoch 32/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0587 - accuracy: 0.9831 - val_loss: 0.1324 - val_accuracy: 0.9775\n",
            "Epoch 33/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0608 - accuracy: 0.9830 - val_loss: 0.1312 - val_accuracy: 0.9776\n",
            "Epoch 34/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0537 - accuracy: 0.9845 - val_loss: 0.1320 - val_accuracy: 0.9777\n",
            "Epoch 35/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0552 - accuracy: 0.9849 - val_loss: 0.1392 - val_accuracy: 0.9784\n",
            "Epoch 36/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0567 - accuracy: 0.9847 - val_loss: 0.1315 - val_accuracy: 0.9776\n",
            "Epoch 37/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0574 - accuracy: 0.9844 - val_loss: 0.1346 - val_accuracy: 0.9783\n",
            "Epoch 38/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0536 - accuracy: 0.9850 - val_loss: 0.1373 - val_accuracy: 0.9774\n",
            "Epoch 39/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0548 - accuracy: 0.9843 - val_loss: 0.1409 - val_accuracy: 0.9777\n",
            "Epoch 40/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0501 - accuracy: 0.9857 - val_loss: 0.1442 - val_accuracy: 0.9772\n",
            "Epoch 41/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0495 - accuracy: 0.9860 - val_loss: 0.1464 - val_accuracy: 0.9778\n",
            "Epoch 42/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0536 - accuracy: 0.9857 - val_loss: 0.1454 - val_accuracy: 0.9769\n",
            "Epoch 43/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0528 - accuracy: 0.9858 - val_loss: 0.1444 - val_accuracy: 0.9771\n",
            "Epoch 44/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0535 - accuracy: 0.9859 - val_loss: 0.1394 - val_accuracy: 0.9786\n",
            "Epoch 45/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0537 - accuracy: 0.9853 - val_loss: 0.1436 - val_accuracy: 0.9773\n",
            "Epoch 46/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0529 - accuracy: 0.9860 - val_loss: 0.1508 - val_accuracy: 0.9770\n",
            "Epoch 47/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0520 - accuracy: 0.9861 - val_loss: 0.1481 - val_accuracy: 0.9788\n",
            "Epoch 48/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0533 - accuracy: 0.9865 - val_loss: 0.1596 - val_accuracy: 0.9780\n",
            "Epoch 49/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0525 - accuracy: 0.9864 - val_loss: 0.1542 - val_accuracy: 0.9770\n",
            "Epoch 50/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0530 - accuracy: 0.9859 - val_loss: 0.1546 - val_accuracy: 0.9773\n",
            "Epoch 51/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0486 - accuracy: 0.9867 - val_loss: 0.1591 - val_accuracy: 0.9770\n",
            "Epoch 52/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0509 - accuracy: 0.9867 - val_loss: 0.1695 - val_accuracy: 0.9786\n",
            "Epoch 53/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0517 - accuracy: 0.9869 - val_loss: 0.1701 - val_accuracy: 0.9776\n",
            "Epoch 54/250\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0512 - accuracy: 0.9868 - val_loss: 0.1718 - val_accuracy: 0.9785\n",
            "Epoch 55/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0518 - accuracy: 0.9862 - val_loss: 0.1691 - val_accuracy: 0.9783\n",
            "Epoch 56/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0519 - accuracy: 0.9868 - val_loss: 0.1650 - val_accuracy: 0.9787\n",
            "Epoch 57/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0494 - accuracy: 0.9870 - val_loss: 0.1722 - val_accuracy: 0.9774\n",
            "Epoch 58/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0488 - accuracy: 0.9874 - val_loss: 0.1675 - val_accuracy: 0.9765\n",
            "Epoch 59/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0532 - accuracy: 0.9867 - val_loss: 0.1755 - val_accuracy: 0.9763\n",
            "Epoch 60/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0498 - accuracy: 0.9874 - val_loss: 0.1667 - val_accuracy: 0.9774\n",
            "Epoch 61/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0515 - accuracy: 0.9871 - val_loss: 0.1746 - val_accuracy: 0.9778\n",
            "Epoch 62/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0544 - accuracy: 0.9870 - val_loss: 0.1843 - val_accuracy: 0.9776\n",
            "Epoch 63/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0517 - accuracy: 0.9872 - val_loss: 0.1711 - val_accuracy: 0.9780\n",
            "Epoch 64/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0524 - accuracy: 0.9872 - val_loss: 0.1784 - val_accuracy: 0.9782\n",
            "Epoch 65/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0479 - accuracy: 0.9883 - val_loss: 0.1772 - val_accuracy: 0.9777\n",
            "Epoch 66/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0543 - accuracy: 0.9867 - val_loss: 0.1742 - val_accuracy: 0.9792\n",
            "Epoch 67/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0472 - accuracy: 0.9875 - val_loss: 0.1713 - val_accuracy: 0.9778\n",
            "Epoch 68/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0490 - accuracy: 0.9876 - val_loss: 0.1752 - val_accuracy: 0.9783\n",
            "Epoch 69/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0500 - accuracy: 0.9876 - val_loss: 0.1817 - val_accuracy: 0.9770\n",
            "Epoch 70/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0508 - accuracy: 0.9871 - val_loss: 0.1845 - val_accuracy: 0.9783\n",
            "Epoch 71/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0502 - accuracy: 0.9874 - val_loss: 0.1841 - val_accuracy: 0.9790\n",
            "Epoch 72/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0499 - accuracy: 0.9870 - val_loss: 0.1892 - val_accuracy: 0.9786\n",
            "Epoch 73/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0495 - accuracy: 0.9881 - val_loss: 0.1906 - val_accuracy: 0.9783\n",
            "Epoch 74/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0504 - accuracy: 0.9885 - val_loss: 0.1891 - val_accuracy: 0.9781\n",
            "Epoch 75/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0495 - accuracy: 0.9884 - val_loss: 0.1931 - val_accuracy: 0.9783\n",
            "Epoch 76/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0541 - accuracy: 0.9875 - val_loss: 0.2012 - val_accuracy: 0.9783\n",
            "Epoch 77/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0515 - accuracy: 0.9873 - val_loss: 0.1937 - val_accuracy: 0.9786\n",
            "Epoch 78/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0492 - accuracy: 0.9879 - val_loss: 0.1891 - val_accuracy: 0.9785\n",
            "Epoch 79/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0487 - accuracy: 0.9880 - val_loss: 0.1896 - val_accuracy: 0.9785\n",
            "Epoch 80/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0488 - accuracy: 0.9877 - val_loss: 0.1936 - val_accuracy: 0.9787\n",
            "Epoch 81/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0487 - accuracy: 0.9881 - val_loss: 0.2039 - val_accuracy: 0.9771\n",
            "Epoch 82/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0484 - accuracy: 0.9883 - val_loss: 0.2027 - val_accuracy: 0.9778\n",
            "Epoch 83/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0502 - accuracy: 0.9871 - val_loss: 0.2116 - val_accuracy: 0.9760\n",
            "Epoch 84/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0451 - accuracy: 0.9896 - val_loss: 0.2022 - val_accuracy: 0.9783\n",
            "Epoch 85/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0471 - accuracy: 0.9885 - val_loss: 0.2204 - val_accuracy: 0.9771\n",
            "Epoch 86/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0513 - accuracy: 0.9888 - val_loss: 0.2009 - val_accuracy: 0.9784\n",
            "Epoch 87/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0492 - accuracy: 0.9892 - val_loss: 0.2179 - val_accuracy: 0.9768\n",
            "Epoch 88/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0526 - accuracy: 0.9880 - val_loss: 0.2050 - val_accuracy: 0.9779\n",
            "Epoch 89/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0486 - accuracy: 0.9883 - val_loss: 0.2084 - val_accuracy: 0.9793\n",
            "Epoch 90/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0486 - accuracy: 0.9885 - val_loss: 0.2157 - val_accuracy: 0.9778\n",
            "Epoch 91/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0508 - accuracy: 0.9889 - val_loss: 0.2119 - val_accuracy: 0.9790\n",
            "Epoch 92/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.2166 - val_accuracy: 0.9782\n",
            "Epoch 93/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0473 - accuracy: 0.9891 - val_loss: 0.2086 - val_accuracy: 0.9781\n",
            "Epoch 94/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0514 - accuracy: 0.9889 - val_loss: 0.2229 - val_accuracy: 0.9783\n",
            "Epoch 95/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0465 - accuracy: 0.9887 - val_loss: 0.2024 - val_accuracy: 0.9789\n",
            "Epoch 96/250\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0460 - accuracy: 0.9889 - val_loss: 0.2178 - val_accuracy: 0.9766\n",
            "Epoch 97/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9881 - val_loss: 0.2112 - val_accuracy: 0.9777\n",
            "Epoch 98/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0482 - accuracy: 0.9887 - val_loss: 0.2308 - val_accuracy: 0.9778\n",
            "Epoch 99/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0464 - accuracy: 0.9895 - val_loss: 0.2277 - val_accuracy: 0.9781\n",
            "Epoch 100/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0469 - accuracy: 0.9889 - val_loss: 0.2248 - val_accuracy: 0.9787\n",
            "Epoch 101/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0477 - accuracy: 0.9885 - val_loss: 0.2183 - val_accuracy: 0.9787\n",
            "Epoch 102/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9886 - val_loss: 0.2252 - val_accuracy: 0.9787\n",
            "Epoch 103/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0509 - accuracy: 0.9884 - val_loss: 0.2271 - val_accuracy: 0.9783\n",
            "Epoch 104/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0491 - accuracy: 0.9883 - val_loss: 0.2088 - val_accuracy: 0.9778\n",
            "Epoch 105/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0491 - accuracy: 0.9892 - val_loss: 0.2161 - val_accuracy: 0.9796\n",
            "Epoch 106/250\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0484 - accuracy: 0.9888 - val_loss: 0.2242 - val_accuracy: 0.9784\n",
            "Epoch 107/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0468 - accuracy: 0.9898 - val_loss: 0.2480 - val_accuracy: 0.9780\n",
            "Epoch 108/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0465 - accuracy: 0.9894 - val_loss: 0.2319 - val_accuracy: 0.9787\n",
            "Epoch 109/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0494 - accuracy: 0.9885 - val_loss: 0.2284 - val_accuracy: 0.9782\n",
            "Epoch 110/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0542 - accuracy: 0.9887 - val_loss: 0.2440 - val_accuracy: 0.9781\n",
            "Epoch 111/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0457 - accuracy: 0.9898 - val_loss: 0.2539 - val_accuracy: 0.9778\n",
            "Epoch 112/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0530 - accuracy: 0.9891 - val_loss: 0.2420 - val_accuracy: 0.9780\n",
            "Epoch 113/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0517 - accuracy: 0.9891 - val_loss: 0.2342 - val_accuracy: 0.9782\n",
            "Epoch 114/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0475 - accuracy: 0.9893 - val_loss: 0.2633 - val_accuracy: 0.9770\n",
            "Epoch 115/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0526 - accuracy: 0.9884 - val_loss: 0.2358 - val_accuracy: 0.9782\n",
            "Epoch 116/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.2522 - val_accuracy: 0.9778\n",
            "Epoch 117/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0453 - accuracy: 0.9896 - val_loss: 0.2461 - val_accuracy: 0.9778\n",
            "Epoch 118/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0502 - accuracy: 0.9884 - val_loss: 0.2590 - val_accuracy: 0.9772\n",
            "Epoch 119/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0503 - accuracy: 0.9892 - val_loss: 0.2569 - val_accuracy: 0.9782\n",
            "Epoch 120/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0478 - accuracy: 0.9893 - val_loss: 0.2417 - val_accuracy: 0.9777\n",
            "Epoch 121/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.2382 - val_accuracy: 0.9785\n",
            "Epoch 122/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0507 - accuracy: 0.9889 - val_loss: 0.2517 - val_accuracy: 0.9778\n",
            "Epoch 123/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0494 - accuracy: 0.9887 - val_loss: 0.2735 - val_accuracy: 0.9790\n",
            "Epoch 124/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9891 - val_loss: 0.2580 - val_accuracy: 0.9767\n",
            "Epoch 125/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0502 - accuracy: 0.9892 - val_loss: 0.2529 - val_accuracy: 0.9772\n",
            "Epoch 126/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0488 - accuracy: 0.9893 - val_loss: 0.2534 - val_accuracy: 0.9777\n",
            "Epoch 127/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0513 - accuracy: 0.9899 - val_loss: 0.2677 - val_accuracy: 0.9778\n",
            "Epoch 128/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0512 - accuracy: 0.9888 - val_loss: 0.2534 - val_accuracy: 0.9772\n",
            "Epoch 129/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0499 - accuracy: 0.9893 - val_loss: 0.2713 - val_accuracy: 0.9783\n",
            "Epoch 130/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0515 - accuracy: 0.9891 - val_loss: 0.2470 - val_accuracy: 0.9785\n",
            "Epoch 131/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0517 - accuracy: 0.9899 - val_loss: 0.2589 - val_accuracy: 0.9778\n",
            "Epoch 132/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0515 - accuracy: 0.9895 - val_loss: 0.2538 - val_accuracy: 0.9778\n",
            "Epoch 133/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0513 - accuracy: 0.9890 - val_loss: 0.2572 - val_accuracy: 0.9783\n",
            "Epoch 134/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0496 - accuracy: 0.9891 - val_loss: 0.2437 - val_accuracy: 0.9787\n",
            "Epoch 135/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0479 - accuracy: 0.9897 - val_loss: 0.2594 - val_accuracy: 0.9778\n",
            "Epoch 136/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0533 - accuracy: 0.9897 - val_loss: 0.2485 - val_accuracy: 0.9783\n",
            "Epoch 137/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0471 - accuracy: 0.9893 - val_loss: 0.2631 - val_accuracy: 0.9778\n",
            "Epoch 138/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0485 - accuracy: 0.9898 - val_loss: 0.2795 - val_accuracy: 0.9766\n",
            "Epoch 139/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0512 - accuracy: 0.9899 - val_loss: 0.2596 - val_accuracy: 0.9787\n",
            "Epoch 140/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0461 - accuracy: 0.9901 - val_loss: 0.2809 - val_accuracy: 0.9785\n",
            "Epoch 141/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0461 - accuracy: 0.9895 - val_loss: 0.2836 - val_accuracy: 0.9779\n",
            "Epoch 142/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0472 - accuracy: 0.9894 - val_loss: 0.2619 - val_accuracy: 0.9783\n",
            "Epoch 143/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0490 - accuracy: 0.9896 - val_loss: 0.2807 - val_accuracy: 0.9786\n",
            "Epoch 144/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0528 - accuracy: 0.9892 - val_loss: 0.2793 - val_accuracy: 0.9787\n",
            "Epoch 145/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0473 - accuracy: 0.9901 - val_loss: 0.2912 - val_accuracy: 0.9778\n",
            "Epoch 146/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0487 - accuracy: 0.9899 - val_loss: 0.2673 - val_accuracy: 0.9770\n",
            "Epoch 147/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0510 - accuracy: 0.9894 - val_loss: 0.2909 - val_accuracy: 0.9781\n",
            "Epoch 148/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0479 - accuracy: 0.9899 - val_loss: 0.2803 - val_accuracy: 0.9772\n",
            "Epoch 149/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0458 - accuracy: 0.9901 - val_loss: 0.2866 - val_accuracy: 0.9770\n",
            "Epoch 150/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0523 - accuracy: 0.9906 - val_loss: 0.2758 - val_accuracy: 0.9778\n",
            "Epoch 151/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0452 - accuracy: 0.9906 - val_loss: 0.2809 - val_accuracy: 0.9775\n",
            "Epoch 152/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0483 - accuracy: 0.9895 - val_loss: 0.2948 - val_accuracy: 0.9779\n",
            "Epoch 153/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0503 - accuracy: 0.9893 - val_loss: 0.3077 - val_accuracy: 0.9772\n",
            "Epoch 154/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0512 - accuracy: 0.9892 - val_loss: 0.2904 - val_accuracy: 0.9756\n",
            "Epoch 155/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0542 - accuracy: 0.9891 - val_loss: 0.2933 - val_accuracy: 0.9772\n",
            "Epoch 156/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0545 - accuracy: 0.9894 - val_loss: 0.2844 - val_accuracy: 0.9772\n",
            "Epoch 157/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0489 - accuracy: 0.9896 - val_loss: 0.3064 - val_accuracy: 0.9776\n",
            "Epoch 158/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0513 - accuracy: 0.9894 - val_loss: 0.2901 - val_accuracy: 0.9771\n",
            "Epoch 159/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0468 - accuracy: 0.9899 - val_loss: 0.3019 - val_accuracy: 0.9774\n",
            "Epoch 160/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0518 - accuracy: 0.9894 - val_loss: 0.2845 - val_accuracy: 0.9765\n",
            "Epoch 161/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0573 - accuracy: 0.9895 - val_loss: 0.2732 - val_accuracy: 0.9775\n",
            "Epoch 162/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0506 - accuracy: 0.9903 - val_loss: 0.2871 - val_accuracy: 0.9773\n",
            "Epoch 163/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0476 - accuracy: 0.9900 - val_loss: 0.2985 - val_accuracy: 0.9783\n",
            "Epoch 164/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0507 - accuracy: 0.9895 - val_loss: 0.2793 - val_accuracy: 0.9779\n",
            "Epoch 165/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0527 - accuracy: 0.9899 - val_loss: 0.2981 - val_accuracy: 0.9773\n",
            "Epoch 166/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0487 - accuracy: 0.9899 - val_loss: 0.2914 - val_accuracy: 0.9766\n",
            "Epoch 167/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0508 - accuracy: 0.9898 - val_loss: 0.2976 - val_accuracy: 0.9772\n",
            "Epoch 168/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0572 - accuracy: 0.9889 - val_loss: 0.2939 - val_accuracy: 0.9787\n",
            "Epoch 169/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0493 - accuracy: 0.9895 - val_loss: 0.3115 - val_accuracy: 0.9775\n",
            "Epoch 170/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0517 - accuracy: 0.9900 - val_loss: 0.3144 - val_accuracy: 0.9772\n",
            "Epoch 171/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0475 - accuracy: 0.9900 - val_loss: 0.2998 - val_accuracy: 0.9777\n",
            "Epoch 172/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0519 - accuracy: 0.9895 - val_loss: 0.2843 - val_accuracy: 0.9773\n",
            "Epoch 173/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0474 - accuracy: 0.9905 - val_loss: 0.3195 - val_accuracy: 0.9784\n",
            "Epoch 174/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0543 - accuracy: 0.9898 - val_loss: 0.3268 - val_accuracy: 0.9774\n",
            "Epoch 175/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0513 - accuracy: 0.9902 - val_loss: 0.2970 - val_accuracy: 0.9774\n",
            "Epoch 176/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0546 - accuracy: 0.9891 - val_loss: 0.2973 - val_accuracy: 0.9779\n",
            "Epoch 177/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0479 - accuracy: 0.9900 - val_loss: 0.3136 - val_accuracy: 0.9770\n",
            "Epoch 178/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0502 - accuracy: 0.9903 - val_loss: 0.3171 - val_accuracy: 0.9773\n",
            "Epoch 179/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0512 - accuracy: 0.9897 - val_loss: 0.3055 - val_accuracy: 0.9783\n",
            "Epoch 180/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0487 - accuracy: 0.9905 - val_loss: 0.3089 - val_accuracy: 0.9771\n",
            "Epoch 181/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0497 - accuracy: 0.9899 - val_loss: 0.3185 - val_accuracy: 0.9772\n",
            "Epoch 182/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0500 - accuracy: 0.9908 - val_loss: 0.3194 - val_accuracy: 0.9772\n",
            "Epoch 183/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0544 - accuracy: 0.9903 - val_loss: 0.3134 - val_accuracy: 0.9780\n",
            "Epoch 184/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0535 - accuracy: 0.9903 - val_loss: 0.3083 - val_accuracy: 0.9783\n",
            "Epoch 185/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0522 - accuracy: 0.9898 - val_loss: 0.3099 - val_accuracy: 0.9776\n",
            "Epoch 186/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0436 - accuracy: 0.9906 - val_loss: 0.3151 - val_accuracy: 0.9779\n",
            "Epoch 187/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0523 - accuracy: 0.9899 - val_loss: 0.3263 - val_accuracy: 0.9782\n",
            "Epoch 188/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0503 - accuracy: 0.9899 - val_loss: 0.3169 - val_accuracy: 0.9779\n",
            "Epoch 189/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0487 - accuracy: 0.9895 - val_loss: 0.3238 - val_accuracy: 0.9783\n",
            "Epoch 190/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0498 - accuracy: 0.9901 - val_loss: 0.3173 - val_accuracy: 0.9786\n",
            "Epoch 191/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0547 - accuracy: 0.9893 - val_loss: 0.3222 - val_accuracy: 0.9771\n",
            "Epoch 192/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0575 - accuracy: 0.9893 - val_loss: 0.3239 - val_accuracy: 0.9778\n",
            "Epoch 193/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0562 - accuracy: 0.9899 - val_loss: 0.3210 - val_accuracy: 0.9772\n",
            "Epoch 194/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0519 - accuracy: 0.9895 - val_loss: 0.3297 - val_accuracy: 0.9774\n",
            "Epoch 195/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0499 - accuracy: 0.9896 - val_loss: 0.3338 - val_accuracy: 0.9776\n",
            "Epoch 196/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0530 - accuracy: 0.9907 - val_loss: 0.3308 - val_accuracy: 0.9781\n",
            "Epoch 197/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0516 - accuracy: 0.9901 - val_loss: 0.3165 - val_accuracy: 0.9774\n",
            "Epoch 198/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0530 - accuracy: 0.9896 - val_loss: 0.3340 - val_accuracy: 0.9768\n",
            "Epoch 199/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0502 - accuracy: 0.9894 - val_loss: 0.3326 - val_accuracy: 0.9780\n",
            "Epoch 200/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0546 - accuracy: 0.9898 - val_loss: 0.3365 - val_accuracy: 0.9769\n",
            "Epoch 201/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0598 - accuracy: 0.9893 - val_loss: 0.3195 - val_accuracy: 0.9777\n",
            "Epoch 202/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0593 - accuracy: 0.9898 - val_loss: 0.3216 - val_accuracy: 0.9774\n",
            "Epoch 203/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9904 - val_loss: 0.3388 - val_accuracy: 0.9791\n",
            "Epoch 204/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0502 - accuracy: 0.9906 - val_loss: 0.3420 - val_accuracy: 0.9787\n",
            "Epoch 205/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0493 - accuracy: 0.9903 - val_loss: 0.3400 - val_accuracy: 0.9786\n",
            "Epoch 206/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0514 - accuracy: 0.9902 - val_loss: 0.3366 - val_accuracy: 0.9776\n",
            "Epoch 207/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0525 - accuracy: 0.9904 - val_loss: 0.3446 - val_accuracy: 0.9776\n",
            "Epoch 208/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0506 - accuracy: 0.9894 - val_loss: 0.3189 - val_accuracy: 0.9786\n",
            "Epoch 209/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0535 - accuracy: 0.9901 - val_loss: 0.3269 - val_accuracy: 0.9782\n",
            "Epoch 210/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0482 - accuracy: 0.9909 - val_loss: 0.3197 - val_accuracy: 0.9788\n",
            "Epoch 211/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0489 - accuracy: 0.9905 - val_loss: 0.3539 - val_accuracy: 0.9776\n",
            "Epoch 212/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0519 - accuracy: 0.9905 - val_loss: 0.3466 - val_accuracy: 0.9772\n",
            "Epoch 213/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0544 - accuracy: 0.9896 - val_loss: 0.3328 - val_accuracy: 0.9780\n",
            "Epoch 214/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0464 - accuracy: 0.9902 - val_loss: 0.3323 - val_accuracy: 0.9768\n",
            "Epoch 215/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0597 - accuracy: 0.9899 - val_loss: 0.3285 - val_accuracy: 0.9783\n",
            "Epoch 216/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0570 - accuracy: 0.9901 - val_loss: 0.3219 - val_accuracy: 0.9778\n",
            "Epoch 217/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0531 - accuracy: 0.9898 - val_loss: 0.3388 - val_accuracy: 0.9776\n",
            "Epoch 218/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0531 - accuracy: 0.9898 - val_loss: 0.3211 - val_accuracy: 0.9772\n",
            "Epoch 219/250\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0520 - accuracy: 0.9903 - val_loss: 0.3494 - val_accuracy: 0.9772\n",
            "Epoch 220/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0523 - accuracy: 0.9900 - val_loss: 0.3311 - val_accuracy: 0.9766\n",
            "Epoch 221/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0559 - accuracy: 0.9904 - val_loss: 0.3377 - val_accuracy: 0.9780\n",
            "Epoch 222/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0514 - accuracy: 0.9900 - val_loss: 0.3539 - val_accuracy: 0.9781\n",
            "Epoch 223/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0530 - accuracy: 0.9904 - val_loss: 0.3422 - val_accuracy: 0.9773\n",
            "Epoch 224/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0524 - accuracy: 0.9902 - val_loss: 0.3501 - val_accuracy: 0.9756\n",
            "Epoch 225/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0532 - accuracy: 0.9900 - val_loss: 0.3464 - val_accuracy: 0.9778\n",
            "Epoch 226/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0568 - accuracy: 0.9898 - val_loss: 0.3606 - val_accuracy: 0.9780\n",
            "Epoch 227/250\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0561 - accuracy: 0.9896 - val_loss: 0.3393 - val_accuracy: 0.9770\n",
            "Epoch 228/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0534 - accuracy: 0.9896 - val_loss: 0.3581 - val_accuracy: 0.9775\n",
            "Epoch 229/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0555 - accuracy: 0.9901 - val_loss: 0.3585 - val_accuracy: 0.9766\n",
            "Epoch 230/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0541 - accuracy: 0.9901 - val_loss: 0.3643 - val_accuracy: 0.9765\n",
            "Epoch 231/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0516 - accuracy: 0.9902 - val_loss: 0.3562 - val_accuracy: 0.9784\n",
            "Epoch 232/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0505 - accuracy: 0.9904 - val_loss: 0.3587 - val_accuracy: 0.9777\n",
            "Epoch 233/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0565 - accuracy: 0.9895 - val_loss: 0.3536 - val_accuracy: 0.9774\n",
            "Epoch 234/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0610 - accuracy: 0.9896 - val_loss: 0.3908 - val_accuracy: 0.9760\n",
            "Epoch 235/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0526 - accuracy: 0.9909 - val_loss: 0.3734 - val_accuracy: 0.9776\n",
            "Epoch 236/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0535 - accuracy: 0.9902 - val_loss: 0.3501 - val_accuracy: 0.9775\n",
            "Epoch 237/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0550 - accuracy: 0.9904 - val_loss: 0.3531 - val_accuracy: 0.9766\n",
            "Epoch 238/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0568 - accuracy: 0.9893 - val_loss: 0.3501 - val_accuracy: 0.9772\n",
            "Epoch 239/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0546 - accuracy: 0.9900 - val_loss: 0.3795 - val_accuracy: 0.9770\n",
            "Epoch 240/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0563 - accuracy: 0.9898 - val_loss: 0.3313 - val_accuracy: 0.9780\n",
            "Epoch 241/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0520 - accuracy: 0.9898 - val_loss: 0.3428 - val_accuracy: 0.9780\n",
            "Epoch 242/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0519 - accuracy: 0.9905 - val_loss: 0.3776 - val_accuracy: 0.9786\n",
            "Epoch 243/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0580 - accuracy: 0.9900 - val_loss: 0.3483 - val_accuracy: 0.9768\n",
            "Epoch 244/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0557 - accuracy: 0.9900 - val_loss: 0.3699 - val_accuracy: 0.9762\n",
            "Epoch 245/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0576 - accuracy: 0.9902 - val_loss: 0.3816 - val_accuracy: 0.9781\n",
            "Epoch 246/250\n",
            "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0541 - accuracy: 0.9908 - val_loss: 0.3677 - val_accuracy: 0.9778\n",
            "Epoch 247/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0580 - accuracy: 0.9895 - val_loss: 0.3821 - val_accuracy: 0.9776\n",
            "Epoch 248/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0505 - accuracy: 0.9905 - val_loss: 0.3796 - val_accuracy: 0.9779\n",
            "Epoch 249/250\n",
            "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0545 - accuracy: 0.9900 - val_loss: 0.3499 - val_accuracy: 0.9787\n",
            "Epoch 250/250\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0556 - accuracy: 0.9898 - val_loss: 0.3573 - val_accuracy: 0.9774\n",
            "Test accuracy:  0.977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtpnHuzHpaTE"
      },
      "source": [
        "- RMSDrop을 이용할 때 에폭 수가 증가할 때 훈련과 테스트 집합에서 정확도가 어떻게 증가하는지는 다음과 같다.\n",
        "- -> 약 15에폭에서 서로 맞닿고 그 이후에는 훈련할 필요가 없다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117957768-58e61d80-b355-11eb-9ee2-9bc8bca20594.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ahhKL_paTE",
        "outputId": "2867dc95-84a6-4fef-eaf0-531801c4432c"
      },
      "source": [
        "# Adam, epochs = 200\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# 신경망과 훈련\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VEBOSE = 1\n",
        "NB_CLASSES = 10  # 출력 개수 = 숫자 개수\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2  # VALIDATION을 위해 예약된 TRAIN의 양\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "# 레이블은 원핫 인코딩으로 표현\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train은 60000개 행의 28*28 값 -> 60000*784 형태로 변경\n",
        "RESHAPED = 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 입력을 [0, 1] 사이로 정규화\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'teset samples')\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# 모델 구축\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 모델 컴파일\n",
        "# 변경 부분\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 teset samples\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "47488/48000 [============================>.] - ETA: 0s - loss: 0.5206 - accuracy: 0.8404"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.5183 - accuracy: 0.8411 - val_loss: 0.1805 - val_accuracy: 0.9489\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.2319 - accuracy: 0.9307 - val_loss: 0.1401 - val_accuracy: 0.9603\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1802 - accuracy: 0.9454 - val_loss: 0.1153 - val_accuracy: 0.9663\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1468 - accuracy: 0.9564 - val_loss: 0.1036 - val_accuracy: 0.9672\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1286 - accuracy: 0.9615 - val_loss: 0.1025 - val_accuracy: 0.9682\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1173 - accuracy: 0.9644 - val_loss: 0.0934 - val_accuracy: 0.9725\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1081 - accuracy: 0.9673 - val_loss: 0.0840 - val_accuracy: 0.9760\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0983 - accuracy: 0.9693 - val_loss: 0.0861 - val_accuracy: 0.9744\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0890 - accuracy: 0.9720 - val_loss: 0.0847 - val_accuracy: 0.9759\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0834 - accuracy: 0.9741 - val_loss: 0.0838 - val_accuracy: 0.9766\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0795 - accuracy: 0.9751 - val_loss: 0.0791 - val_accuracy: 0.9769\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0732 - accuracy: 0.9776 - val_loss: 0.0806 - val_accuracy: 0.9781\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0708 - accuracy: 0.9775 - val_loss: 0.0784 - val_accuracy: 0.9780\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0673 - accuracy: 0.9787 - val_loss: 0.0827 - val_accuracy: 0.9769\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0634 - accuracy: 0.9791 - val_loss: 0.0788 - val_accuracy: 0.9787\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0616 - accuracy: 0.9801 - val_loss: 0.0841 - val_accuracy: 0.9791\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0590 - accuracy: 0.9799 - val_loss: 0.0791 - val_accuracy: 0.9784\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0568 - accuracy: 0.9818 - val_loss: 0.0815 - val_accuracy: 0.9780\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0531 - accuracy: 0.9823 - val_loss: 0.0798 - val_accuracy: 0.9799\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0543 - accuracy: 0.9820 - val_loss: 0.0779 - val_accuracy: 0.9783\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0540 - accuracy: 0.9820 - val_loss: 0.0815 - val_accuracy: 0.9785\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0492 - accuracy: 0.9839 - val_loss: 0.0820 - val_accuracy: 0.9783\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0499 - accuracy: 0.9833 - val_loss: 0.0816 - val_accuracy: 0.9779\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0476 - accuracy: 0.9840 - val_loss: 0.0811 - val_accuracy: 0.9794\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0449 - accuracy: 0.9856 - val_loss: 0.0810 - val_accuracy: 0.9787\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0446 - accuracy: 0.9848 - val_loss: 0.0814 - val_accuracy: 0.9795\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0427 - accuracy: 0.9850 - val_loss: 0.0828 - val_accuracy: 0.9808\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0434 - accuracy: 0.9859 - val_loss: 0.0843 - val_accuracy: 0.9805\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0421 - accuracy: 0.9859 - val_loss: 0.0844 - val_accuracy: 0.9799\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0418 - accuracy: 0.9863 - val_loss: 0.0848 - val_accuracy: 0.9801\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.0823 - val_accuracy: 0.9803\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0395 - accuracy: 0.9871 - val_loss: 0.0812 - val_accuracy: 0.9806\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0345 - accuracy: 0.9886 - val_loss: 0.0818 - val_accuracy: 0.9808\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0378 - accuracy: 0.9872 - val_loss: 0.0875 - val_accuracy: 0.9791\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0397 - accuracy: 0.9864 - val_loss: 0.0813 - val_accuracy: 0.9797\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.0834 - val_accuracy: 0.9807\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0372 - accuracy: 0.9873 - val_loss: 0.0843 - val_accuracy: 0.9802\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0381 - accuracy: 0.9870 - val_loss: 0.0863 - val_accuracy: 0.9796\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.0900 - val_accuracy: 0.9785\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.0878 - val_accuracy: 0.9793\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0326 - accuracy: 0.9885 - val_loss: 0.0901 - val_accuracy: 0.9802\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.0895 - val_accuracy: 0.9793\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0373 - accuracy: 0.9874 - val_loss: 0.0875 - val_accuracy: 0.9803\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0344 - accuracy: 0.9886 - val_loss: 0.0894 - val_accuracy: 0.9794\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0327 - accuracy: 0.9897 - val_loss: 0.0849 - val_accuracy: 0.9807\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.0887 - val_accuracy: 0.9797\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0308 - accuracy: 0.9894 - val_loss: 0.0886 - val_accuracy: 0.9799\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.0893 - val_accuracy: 0.9807\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.0888 - val_accuracy: 0.9795\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0283 - accuracy: 0.9902 - val_loss: 0.1004 - val_accuracy: 0.9793\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.0968 - val_accuracy: 0.9793\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0296 - accuracy: 0.9899 - val_loss: 0.0911 - val_accuracy: 0.9793\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0306 - accuracy: 0.9901 - val_loss: 0.0991 - val_accuracy: 0.9793\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0341 - accuracy: 0.9888 - val_loss: 0.0937 - val_accuracy: 0.9796\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.0935 - val_accuracy: 0.9809\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0297 - accuracy: 0.9909 - val_loss: 0.0913 - val_accuracy: 0.9809\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.0926 - val_accuracy: 0.9812\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.0955 - val_accuracy: 0.9812\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0299 - accuracy: 0.9896 - val_loss: 0.0911 - val_accuracy: 0.9809\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0292 - accuracy: 0.9905 - val_loss: 0.0941 - val_accuracy: 0.9796\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0267 - accuracy: 0.9911 - val_loss: 0.0960 - val_accuracy: 0.9810\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.1006 - val_accuracy: 0.9798\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.0969 - val_accuracy: 0.9813\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0294 - accuracy: 0.9896 - val_loss: 0.0967 - val_accuracy: 0.9801\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0977 - val_accuracy: 0.9802\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0261 - accuracy: 0.9907 - val_loss: 0.1002 - val_accuracy: 0.9806\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0281 - accuracy: 0.9911 - val_loss: 0.0998 - val_accuracy: 0.9808\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0268 - accuracy: 0.9912 - val_loss: 0.1066 - val_accuracy: 0.9795\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.0959 - val_accuracy: 0.9818\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.1024 - val_accuracy: 0.9802\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.1051 - val_accuracy: 0.9800\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0265 - accuracy: 0.9916 - val_loss: 0.1062 - val_accuracy: 0.9806\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0244 - accuracy: 0.9914 - val_loss: 0.0996 - val_accuracy: 0.9797\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1005 - val_accuracy: 0.9812\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.1072 - val_accuracy: 0.9798\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.1066 - val_accuracy: 0.9807\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0242 - accuracy: 0.9918 - val_loss: 0.1076 - val_accuracy: 0.9803\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.1044 - val_accuracy: 0.9795\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.1000 - val_accuracy: 0.9797\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0247 - accuracy: 0.9919 - val_loss: 0.1026 - val_accuracy: 0.9800\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0231 - accuracy: 0.9924 - val_loss: 0.1036 - val_accuracy: 0.9799\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0216 - accuracy: 0.9924 - val_loss: 0.0995 - val_accuracy: 0.9813\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.1036 - val_accuracy: 0.9806\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.1087 - val_accuracy: 0.9797\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.1023 - val_accuracy: 0.9803\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0221 - accuracy: 0.9927 - val_loss: 0.1127 - val_accuracy: 0.9793\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.1100 - val_accuracy: 0.9803\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1147 - val_accuracy: 0.9793\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.1060 - val_accuracy: 0.9799\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.1038 - val_accuracy: 0.9789\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0249 - accuracy: 0.9924 - val_loss: 0.1024 - val_accuracy: 0.9808\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0221 - accuracy: 0.9926 - val_loss: 0.1122 - val_accuracy: 0.9802\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0199 - accuracy: 0.9932 - val_loss: 0.0970 - val_accuracy: 0.9818\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0235 - accuracy: 0.9925 - val_loss: 0.1063 - val_accuracy: 0.9802\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0257 - accuracy: 0.9921 - val_loss: 0.1075 - val_accuracy: 0.9802\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.1015 - val_accuracy: 0.9819\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0195 - accuracy: 0.9942 - val_loss: 0.1120 - val_accuracy: 0.9798\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0205 - accuracy: 0.9934 - val_loss: 0.1088 - val_accuracy: 0.9797\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1080 - val_accuracy: 0.9796\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.1196 - val_accuracy: 0.9796\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.1121 - val_accuracy: 0.9800\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0204 - accuracy: 0.9934 - val_loss: 0.1186 - val_accuracy: 0.9788\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.1074 - val_accuracy: 0.9808\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.1107 - val_accuracy: 0.9808\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0234 - accuracy: 0.9927 - val_loss: 0.1101 - val_accuracy: 0.9792\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1093 - val_accuracy: 0.9808\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.1131 - val_accuracy: 0.9803\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.1124 - val_accuracy: 0.9803\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.1070 - val_accuracy: 0.9797\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.1114 - val_accuracy: 0.9807\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0228 - accuracy: 0.9927 - val_loss: 0.1111 - val_accuracy: 0.9808\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0206 - accuracy: 0.9933 - val_loss: 0.1150 - val_accuracy: 0.9791\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.1081 - val_accuracy: 0.9804\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0233 - accuracy: 0.9926 - val_loss: 0.1129 - val_accuracy: 0.9808\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.1033 - val_accuracy: 0.9807\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.1081 - val_accuracy: 0.9798\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.1063 - val_accuracy: 0.9810\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.1108 - val_accuracy: 0.9808\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0217 - accuracy: 0.9935 - val_loss: 0.1102 - val_accuracy: 0.9810\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0226 - accuracy: 0.9930 - val_loss: 0.1084 - val_accuracy: 0.9807\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.1074 - val_accuracy: 0.9803\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.1148 - val_accuracy: 0.9795\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.1080 - val_accuracy: 0.9808\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.1205 - val_accuracy: 0.9808\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.1195 - val_accuracy: 0.9800\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.1159 - val_accuracy: 0.9805\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.1167 - val_accuracy: 0.9805\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0184 - accuracy: 0.9940 - val_loss: 0.1193 - val_accuracy: 0.9793\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.1142 - val_accuracy: 0.9795\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.1103 - val_accuracy: 0.9808\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.1141 - val_accuracy: 0.9812\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.1144 - val_accuracy: 0.9803\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.1150 - val_accuracy: 0.9803\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0180 - accuracy: 0.9941 - val_loss: 0.1196 - val_accuracy: 0.9808\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.1203 - val_accuracy: 0.9797\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1145 - val_accuracy: 0.9804\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.1265 - val_accuracy: 0.9797\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.1231 - val_accuracy: 0.9794\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0186 - accuracy: 0.9943 - val_loss: 0.1178 - val_accuracy: 0.9804\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0192 - accuracy: 0.9934 - val_loss: 0.1245 - val_accuracy: 0.9789\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.1087 - val_accuracy: 0.9810\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1091 - val_accuracy: 0.9815\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1180 - val_accuracy: 0.9804\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.1129 - val_accuracy: 0.9807\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0181 - accuracy: 0.9943 - val_loss: 0.1187 - val_accuracy: 0.9797\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.1247 - val_accuracy: 0.9797\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.1181 - val_accuracy: 0.9791\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.1265 - val_accuracy: 0.9799\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.1165 - val_accuracy: 0.9804\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.1196 - val_accuracy: 0.9797\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.1222 - val_accuracy: 0.9797\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.1184 - val_accuracy: 0.9799\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.1211 - val_accuracy: 0.9791\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.1225 - val_accuracy: 0.9799\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0206 - accuracy: 0.9937 - val_loss: 0.1258 - val_accuracy: 0.9792\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.1144 - val_accuracy: 0.9808\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0195 - accuracy: 0.9937 - val_loss: 0.1239 - val_accuracy: 0.9803\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.1260 - val_accuracy: 0.9798\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.1120 - val_accuracy: 0.9803\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.1193 - val_accuracy: 0.9802\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.1279 - val_accuracy: 0.9789\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1243 - val_accuracy: 0.9799\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0177 - accuracy: 0.9943 - val_loss: 0.1224 - val_accuracy: 0.9803\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.1278 - val_accuracy: 0.9794\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.1210 - val_accuracy: 0.9810\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0156 - accuracy: 0.9953 - val_loss: 0.1258 - val_accuracy: 0.9806\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.1326 - val_accuracy: 0.9787\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.1300 - val_accuracy: 0.9792\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.1159 - val_accuracy: 0.9807\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.1244 - val_accuracy: 0.9806\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.1316 - val_accuracy: 0.9795\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0172 - accuracy: 0.9950 - val_loss: 0.1236 - val_accuracy: 0.9795\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.1335 - val_accuracy: 0.9784\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1249 - val_accuracy: 0.9793\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.1244 - val_accuracy: 0.9802\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.1208 - val_accuracy: 0.9811\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0159 - accuracy: 0.9949 - val_loss: 0.1359 - val_accuracy: 0.9797\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.1212 - val_accuracy: 0.9808\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.1368 - val_accuracy: 0.9784\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.1274 - val_accuracy: 0.9791\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.1279 - val_accuracy: 0.9796\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.1282 - val_accuracy: 0.9803\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0166 - accuracy: 0.9950 - val_loss: 0.1307 - val_accuracy: 0.9796\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0159 - accuracy: 0.9952 - val_loss: 0.1359 - val_accuracy: 0.9788\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.1303 - val_accuracy: 0.9801\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.1292 - val_accuracy: 0.9806\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.1277 - val_accuracy: 0.9796\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.1264 - val_accuracy: 0.9805\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.1378 - val_accuracy: 0.9793\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.1303 - val_accuracy: 0.9794\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0181 - accuracy: 0.9947 - val_loss: 0.1247 - val_accuracy: 0.9788\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.1233 - val_accuracy: 0.9797\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1238 - val_accuracy: 0.9788\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.1304 - val_accuracy: 0.9796\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.1291 - val_accuracy: 0.9804\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.1242 - val_accuracy: 0.9795\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.1257 - val_accuracy: 0.9796\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.1314 - val_accuracy: 0.9793\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0167 - accuracy: 0.9951 - val_loss: 0.1368 - val_accuracy: 0.9789\n",
            "Test accuracy:  0.9804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7vlLUxxpaTF"
      },
      "source": [
        "- Adam을 사용할 때 에폭 수가 증가할 때 훈련 및 테스트 집합에서 정확도의 증가는 다음과 같은 양상을 보인다.\n",
        "- Adam을 optimizer로 선택한다면 약 12에폭 단계 후 그만둘 수 있다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117958445-f9d4d880-b355-11eb-8310-17b0fb0faa52.png)\n",
        "\n",
        "- 다양한 드롭아웃 값에 대해 테스트 데이터셋의 정확도는 다음과 같은 양상을 나타낸다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117958546-153fe380-b356-11eb-9a4e-fac431cb454c.png)\n",
        "\n",
        "### 에폭 수 증가시키기\n",
        "- 에폭 수를 20에서 200으로 늘려도 계산 시간은 10배 증가되지만, 결과가 개선되지는 않는다.\n",
        "- -> 학습에는 계산에 소요된 시간보다 적절한 기술 채택이 더 중요하다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117958817-5c2dd900-b356-11eb-9357-385833d6f86e.png)\n",
        "\n",
        "### Optimizer 학습률 조절\n",
        "- 서로 다른 학습률에 따른 정확도는 다음과 같다. lr=0.1일 때 최상의 값이므로 이 값이 기본 학습 속도이다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117959689-39e88b00-b357-11eb-8c1c-6345c94a0e45.png)\n",
        "\n",
        "### 내부 은닉층 개수 증가\n",
        "- 은닉 뉴런의 수가 증가함에 따라 모델의 복잡도가 증가해, 최적화해야 할 매개변수가 점점 많아서 실행 시간이 크게 증가한다.\n",
        "- 신경망의 크기를 증가시켜 얻는 이득은 망이 증가함에 따라 점점 감소한다.\n",
        "- ∴ 은닉 뉴런의 수를 일정 이상 증가시키면 신경망이 일반화가 어려워져 정확도가 저하될 수 있다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117960101-9fd51280-b357-11eb-9258-153ad56d2d23.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117960263-c98e3980-b357-11eb-9792-a42ec88b49b8.png)\n",
        "\n",
        "### 배치 계산 크기 증가\n",
        "- 자료를 통해 BATCH_SIZE=64일 때 최고 정확도에 도달함을 알 수 있다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/117963701-ab2a3d00-b35b-11eb-8c28-fd1ceb06ee7f.png)\n",
        "\n",
        "### 필기체 인식 실행 차트 요약\n",
        "- 변형을 통해 성능을 향상시킬 수 있었다.\n",
        "    1. TensorFlow 2.0으로 단일 계층 신경망 정의\n",
        "    2. 은닉층 추가\n",
        "    3. 신경망에 임의의 드롭아웃 추가\n",
        "    4. RMSProp과 Adam으로 테스트 집합의 성능 개선\n",
        "    \n",
        "|모델/정확도|훈련|검증|테스트\n",
        "|-|-|-|-|\n",
        "|단일|89.96%|90.70%|90.71%|\n",
        "|2 은닉(128)|90.81%|91.40%|91.18%|\n",
        "|드롭아웃(30%)|91.70%|94.42%|94.15%(200 에폭)|\n",
        "|RMSProp|97.43%|97.62%|97.64%(10 에폭)|\n",
        "|Adam|98.94%|97.89%|97.82(10 에폭)|\n",
        "\n",
        "- 다음 두 실험은 큰 개선점이 없었다.\n",
        "    1. 내부 뉴런의 수를 늘리면 더 복잡한 모델이 생성되고 더 많은 계산량이 필요하지만 개선의 정도가 미미하다.\n",
        "    2. optimizer의 BATCH_SIZE를 변경해도 개선의 정도가 미미하다.\n",
        "\n",
        "## 정규화\n",
        "### 과적합을 피하기 위한 정규화 적용\n",
        "- 좋은 머신러닝 모델 = 훈련 데이터에서 낮은 오류율 <=> 주어진 훈련 데이터에 대해 모델의 손실 함수 최소화 -> min: {손실(훈련 데이터|모델)}\n",
        "- 훈련 데이터에 내재된 모든 관계를 포착하려다 모델의 복잡도 증가\n",
        "    - 부정적인 결과 1. 복잡한 모델 실행에 상당한 시간 소요\n",
        "    - 부정적인 결과 2. 훈련 데이터에서 우수한 성과를 달성할 수 있지만, 검증 데이터에서 나쁜 성과를 보일 수 있다.\n",
        "        - ∵ 모델이 훈련에만 특화된 많은 매개변수를 고려할 순 있지만, 이는 일반적이지 않음\n",
        "        - -> **과적합 overfitting**: 일반화 능력을 잃은 모델\n",
        "        \n",
        "![image](https://user-images.githubusercontent.com/61455647/117966880-64d6dd00-b35f-11eb-9613-096fa71e8b0b.png)\n",
        "\n",
        "- 훈련 과정에서 초기 감소 후 검증 단계에서 손실이 증가한다면, 모델의 복잡성 문제가 발생한 것이다. -> 훈련 데이터 과적합\n",
        "- 과적합 문제를 해결하기 위해 모델의 복잡도를 파악할 수 있어야 한다.\n",
        "    - 모델 = 가중치의 벡터\n",
        "    - 각 가중치는 0이나 0에 매우 가깝지 않으면 출력에 영향을 준다.\n",
        "    - ∴ 모델의 복잡도는 0이 아닌 가중치의 개수로 표현될 수 있다. <=> 손실 함수 측면에서 0이 아닌 가중치의 개수가 최소인 가장 간단한 모델 선택\n",
        "    - 초매개변수 λ >= 0으로 단순 모델의 중요성 조절 -> min: {손실(훈련 데이터|모델)} + λ * 복잡도(모델)\n",
        "- 정규화 방법\n",
        "    - L1 정규화(LASSO): 모델의 복잡도는 가중치 절댓값의 합\n",
        "    - L2 정규화(Ridge): 모델의 복잡도는 가중치 제곱의 합\n",
        "    - Elastic 정규화: 모델의 복잡도는 L1, L2 정규화의 조합\n",
        "- 정규화로 과적합이 분명할 때 신경망의 성능을 향상시킬 수 있다.\n",
        "- TensorFlow는 L1, L2, ElasticNet 정류화를 지원한다.\n",
        "```\n",
        "from tf.keras.regularizers import l2, activity_l2\n",
        "model.add(Dense(64, input_dim=64, W_regularizer=l2(0,01), activity_regularizer=activity_l2(0.01)))\n",
        "```\n",
        "\n",
        "### 배치 정규화(Batch Normalization)의 이해\n",
        "- 경우에 따라 훈련 에폭을 절반으로 줄여 훈련을 가속화할 수 있다.\n",
        "- 기존의 문제점\n",
        "    1. 각 계층은 모든 배치마다 가중치를 지속적으로 다른 분포로 조정한다. -> 모델 훈련 속도 ↓ => 각 배치와 각 에폭에 대해 계층 입력이 좀 더 유사한 분포를 갖도록 하자.\n",
        "    2. 시그모이드 함수는 값이 0에서 상당히 멀어지면 고착되어 가중치가 갱신되지 않는다. => 계층 출력을 0에 가까운 Gaussian 분포 단위로 변환\n",
        "- 해결책\n",
        "    - 활성화 입력 x, 배치 평균 μ, 배치 분산 σ,  작은 수 ε, 선형 변환 y = λx + β\n",
        "    - (x-μ)로 0 주위로 모음 -> (x-μ)/(σ+ε)로 분모가 0이 되는 것을 피함 -> y = λx + β로 훈련 단계에서 정규화 효과 적용\n",
        "    - -> 다른 계층에서도 훈련 과정에서 λ와 β 매개변수 최적화\n",
        "- 활성화 속도가 너무 작아 없어지거나 너무 커지는 것을 방지하는 데에 도움 -> 훈련 속도와 정확도 ↑\n",
        "\n",
        "## 구글 Colab 사용: CPU, GPU, TPU\n",
        "\n",
        "## 감정 분석(IMDb 데이터셋으로 개발된 감정 분석 예시)\n",
        "- IMDb 데이터셋: Internet Movie Database의 50000개의 영화 리뷰 텍스트(긍정/부정) -> 25000건의 훈련과 25000건의 테스트 집합으로 나눈다.\n",
        "- 목표: 텍스트로 이진 판단을 예측할 수 있는 분류기 구축\n",
        "- 데이터셋 설명\n",
        "  - `tf.keras`로 IMDb 로드\n",
        "  - 리뷰의 단어 시퀀스는 정수의 시퀀스로 변환 <=> 각 정수 = 사전의 특정 단어\n",
        "  - 문장을 `max_len` 길이로 채워 길이에 상관 없이 모든 문장을 신경망에 입력\n",
        "  - 각 입력 벡터는 고정된 크기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXEhmK90paTF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500\n",
        "\n",
        "def load_data():\n",
        "  # 데이터 로드\n",
        "  (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "  # 문장을 max_len이 되도록 채워 넣는다.\n",
        "  X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "  X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "  return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqDNXwZfsaEy"
      },
      "source": [
        "- 모델 설명\n",
        "  - `Embedding()` 계층으로 리뷰에 포함된 단어의 희소 공간을 더 조밀한 공간으로 매핑 -> 계산이 용이\n",
        "  - `GlobalMaxPooling1D()` 계층으로 `n_words`의 특징 벡터의 최댓값을 얻음\n",
        "  - 2개의 `Dense()` 계층\n",
        "  - 마지막은 단일 뉴런: 최종 이진 추정을 위해 시그모이드 활성화 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbL58WzosXBi",
        "outputId": "5ce35a7b-73d6-42cb-ba23-6fe9de04712c"
      },
      "source": [
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # 입력: -eEmbedding Layer\n",
        "  # 모델의 입력: 크기의 정수 행렬(batch, input_length), 모델의 출력: 차원(input_length, dim_embedding)\n",
        "  # 입력 중 가장 큰 정수는 n_words보다 작거나 같다.\n",
        "  model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\n",
        "\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  # 각 n_words 특징에서 특징 벡터의 최대를 취함\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "score = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(\"\\nTest score: \", score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.6377"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25000/25000 [==============================] - 24s 962us/sample - loss: 0.6717 - accuracy: 0.6377 - val_loss: 0.6301 - val_accuracy: 0.8304\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.4586 - accuracy: 0.8401 - val_loss: 0.3617 - val_accuracy: 0.8583\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.2806 - accuracy: 0.8868 - val_loss: 0.3059 - val_accuracy: 0.8740\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 24s 945us/sample - loss: 0.2193 - accuracy: 0.9142 - val_loss: 0.2940 - val_accuracy: 0.8764\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.1719 - accuracy: 0.9382 - val_loss: 0.2910 - val_accuracy: 0.8766\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 24s 948us/sample - loss: 0.1346 - accuracy: 0.9541 - val_loss: 0.2934 - val_accuracy: 0.8742\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.1029 - accuracy: 0.9674 - val_loss: 0.3110 - val_accuracy: 0.8690\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 24s 947us/sample - loss: 0.0783 - accuracy: 0.9774 - val_loss: 0.3249 - val_accuracy: 0.8649\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.0584 - accuracy: 0.9846 - val_loss: 0.3388 - val_accuracy: 0.8626\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.0433 - accuracy: 0.9892 - val_loss: 0.3559 - val_accuracy: 0.8603\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 24s 948us/sample - loss: 0.0319 - accuracy: 0.9932 - val_loss: 0.3720 - val_accuracy: 0.8592\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 24s 947us/sample - loss: 0.0255 - accuracy: 0.9945 - val_loss: 0.3883 - val_accuracy: 0.8577\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 24s 951us/sample - loss: 0.0188 - accuracy: 0.9967 - val_loss: 0.4118 - val_accuracy: 0.8541\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 24s 947us/sample - loss: 0.0149 - accuracy: 0.9977 - val_loss: 0.4201 - val_accuracy: 0.8554\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 0.4377 - val_accuracy: 0.8547\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 24s 953us/sample - loss: 0.0099 - accuracy: 0.9986 - val_loss: 0.4575 - val_accuracy: 0.8533\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 24s 949us/sample - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.4592 - val_accuracy: 0.8544\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 24s 952us/sample - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.4770 - val_accuracy: 0.8529\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 24s 955us/sample - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.4894 - val_accuracy: 0.8521\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 24s 956us/sample - loss: 0.0052 - accuracy: 0.9996 - val_loss: 0.5036 - val_accuracy: 0.8526\n",
            "\n",
            "Test score:  0.5036102217435837\n",
            "Test accuracy:  0.8526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTF7VVDssYmr"
      },
      "source": [
        "## 초매개변수 튜닝과 AutoML\n",
        "- **초매개변수 hyperparameter**\n",
        "  - 주어진 신경망에 대해 최적화할 수 있는 여러 매개변수\n",
        "  - ex. 은닉 뉴런의 개수, BATCH_SIZE, 에폭 수, 망 자체의 복잡도에 종속된 매개변수 등\n",
        "  - 신경망 자체의 매개변수(가중치 및 편향 값)와 구별하고자 함\n",
        "- 초매개변수 튜닝(hyperparameter tuning)\n",
        "  - 비용 함수를 최소화하는 초매개변수의 최적 조합을 찾는 과정\n",
        "  - n개의 초매개변수가 이루는 n차원의 공간을 정의, 이 공간에서 비용 함수의 최적 값의 지점 차기\n",
        "  - 방법: 공간에 그리드를 만들어, 각 그리드 정점에 대한 비용 함수 값 확인 => 초매개변수를 버킷으로 나눠 서로 다른 조합을 무차별 접근 방식으로 확인\n",
        "- AutoML: 초매개변수를 자동으로 튜닝하고 최적의 신경망 아키텍처를 자동으로 검색하는 것이 목표인 연구 기법\n",
        "\n",
        "## 출력 예측\n",
        "- 신경망이 훈련되면 예측에 사용될 수 있다.\n",
        "```\n",
        "# 예측하기\n",
        "predictions = model.predict(X)\n",
        "```\n",
        "- 주어진 입력에 대해 출력 계산\n",
        "  - `model.evaluate()`: 손실 값 계산\n",
        "  - `model.predict_class()`: 범주 출력 계산\n",
        "  - `model.predict_proba()`: 부류 확률 계산\n",
        "\n",
        "## 역전파에 대한 실용적 개괄\n",
        "- 다층 퍼셉트론은 **역전파 backpropagation** 프로세스로 훈련 데이터에서 학습\n",
        "- 각 신경망 계층에는 주어진 입력 집합에 대해 출력값을 결정하는 관련 가중치 집합이 있다.\n",
        "- 신경망은 다수의 은닉층을 가질 수 있다.\n",
        "- **순전파 propagate forward**\n",
        "  1. 모든 가중치에 임의의 값 할당\n",
        "  2. 훈련 집합의 각 입력에 대해 신경망 활성화\n",
        "    - 값은 입력 단계에서 은닉 단계를 거쳐 출력 단계로 순방향(forward) 전파\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/118077076-61d6fd80-b3ee-11eb-82e0-5a6779e96c5c.png)\n",
        "\n",
        "- 역전파\n",
        "  - 훈련 집합에서 실제 관측값을 알기 때문에 예측에서 발생한 오차 계산\n",
        "  - GD처럼 적절한 최적화 알고리즘으로 오차를 줄이려는 목적\n",
        "  - 신경망 가중치를 조정하고자 오차를 역으로 전파\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/118077218-a2cf1200-b3ee-11eb-9010-37f4441bf587.png)\n",
        "\n",
        "- 순방향 전파와 역방향 전파 프로세스는 오차가 사전 정의된 임계값 이하로 떨어질 때까지 여러 번 반복된다.\n",
        "  - 특징: 입력\n",
        "  - 레이블: 학습 과정 진행에 사용\n",
        "  - 모델: 손실 함수가 점진적으로 최소화되는 방향으로 갱신\n",
        "  - 신경망은 정확하게 예측된 레이블 수를 증가시키는 방향으로 내부 가중치를 점진적으로 조정\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/61455647/118077333-dca01880-b3ee-11eb-8c32-61ada962135d.png)\n",
        "\n",
        "## 정리\n",
        "\n",
        "## 딥러닝 접근법을 향해\n",
        "- 필기체 숫자 인식 중 99% 정확도에 가까워질수록 추가 개선이 어렵다는 결론을 얻었다.\n",
        "- 적용하지 않은 개선 가능한 점: 이미지의 로컬 공간 구조를 활용하지 않았음=기록된 각 숫자를 나타내는 비트맵을 평면 벡터로 변환해 로컬 공간 구조(인접한 픽셀이 서로 더 가까이 있다)가 사라졌다.\n",
        "- -> CNN(COnvolutional Neural Network)은 특정 유형의 딥러닝 망이 이미지의 로컬 공간 구조를 보존한다+점진적 추상화 레벨을 통한 학습을 이용해 개발"
      ]
    }
  ]
}